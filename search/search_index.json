{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Development Setup","text":"<p>Everything you need to know about getting started with listed technology. Goal of writing this book is to documenting most of the setup steps that we normally come across and have them be least of our problems.</p> <p>This book acts as a reference guide to common commands. You may or may not see the documentation for the latest version, however the book should still enable to get going and start working with the basics.</p> <p>For a more readable format, go to the website</p>","tags":["handbook","developer","index"]},{"location":"#what-to-expect","title":"What to expect","text":"<ul> <li>Installation instructions</li> <li>Cheatsheet</li> <li>Reference to official docs, blogs, etc</li> </ul>","tags":["handbook","developer","index"]},{"location":"#index","title":"Index","text":"<ul> <li>AWS CLI</li> <li>Docker CLI</li> <li>Git CLI</li> <li>Homebrew</li> <li>Java CLI</li> <li>kubectl</li> <li>kustomize</li> <li>PostgreSQL</li> <li>Python CLI</li> <li>Splunk</li> <li>SSH</li> <li>Terminal Bash</li> </ul>","tags":["handbook","developer","index"]},{"location":"#contribution","title":"Contribution","text":"<p>Refer <code>/docs/CONTRIBUTING.md</code> in the repo for making contributions to the template.</p> <ul> <li>Issue Tracker</li> <li>Source Code</li> </ul>","tags":["handbook","developer","index"]},{"location":"#support","title":"Support","text":"<p>If you are having issues, please let us know. We have a mailing list located at: goel4ever@googlegroups.com</p>","tags":["handbook","developer","index"]},{"location":"#license","title":"License","text":"<p>The project is licensed under the BSD license.</p> <p>BSD licenses are a family of permissive free software licenses, imposing minimal restrictions on the use and distribution of covered software. It can be used to distribute open source, freeware and shareware projects as it does not put any restrictions on the redistribution of software.</p>","tags":["handbook","developer","index"]},{"location":"todo/","title":"REVIEW FIRST (ToDos)","text":""},{"location":"todo/#linting","title":"Linting","text":"<ul> <li>https://medium.com/compass-true-north/linting-a-practical-guide-to-introducing-it-to-your-team-28e3605a0dc2</li> </ul>"},{"location":"todo/#contribution","title":"Contribution","text":"<ul> <li>https://choosealicense.com/</li> <li>https://allcontributors.org/docs/en/bot/overview</li> </ul>"},{"location":"todo/#learning","title":"Learning","text":"<ul> <li>Machine learning and AI<ul> <li>100 (Free) AI Courses to Help You Navigate the Future of Work</li> </ul> </li> <li>Blockchain<ul> <li>Certified Blockchain Solutions Architect (CBSA)</li> </ul> </li> </ul>"},{"location":"todo/#tools-to-explore","title":"Tools to explore","text":"<ul> <li>Ghost for self-hosted Ghost users</li> <li>ghost-static-site-generator</li> <li>https://dev.to/bassel/hosting-your-ghost-blog-on-github-pages-for-free-53hl</li> <li>https://cadenkraft.com/creating-a-ghost-blog-with-free-webhosting/</li> <li>GitPython is a python library used to interact with Git repositories.</li> <li>doctopt creates beautiful command-line interfaces using Python</li> </ul>"},{"location":"Browser%20-%20Chrome/","title":"Chrome Browser","text":""},{"location":"Browser%20-%20Chrome/#storage","title":"Storage","text":"<ol> <li>Disk Cache stores resources fetched from the web so that they can be accessed quickly at a latter time if needed.</li> </ol>"},{"location":"Browser%20-%20Chrome/#resources","title":"Resources","text":"<ul> <li>How does Chrome/Chromium eviction algorithm really work? | SuperUser</li> </ul>"},{"location":"Cloud/aws/","title":"AWS CLI","text":"<ol> <li>CLI Configuration</li> <li>IAM</li> <li>S3</li> </ol>"},{"location":"Cloud/aws/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS Account</li> <li>IAM keys</li> </ul>"},{"location":"Cloud/aws/#installation","title":"Installation","text":"<pre><code>$ curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\"\n$ sudo installer -pkg AWSCLIV2.pkg -target /\n\nThe `-o` option specifies the file name that the downloaded package is written to, in the current folder.\n\n# Verify installation\n$ which aws\n$ aws --version\n</code></pre>"},{"location":"Cloud/aws/#running-aws-cli-2-docker-image","title":"Running AWS CLI 2 Docker image","text":"<pre><code>$ docker run --rm -it amazon/aws-cli command\n$ docker run --rm -it amazon/aws-cli --version\n</code></pre>"},{"location":"Cloud/aws/#references","title":"References","text":"<ul> <li>Prerequisites to AWS CLI 2</li> <li>CLI 2 Installation</li> <li>Configuration Basics</li> </ul>"},{"location":"Cloud/aws/cli-configuration/","title":"AWS CLI","text":""},{"location":"Cloud/aws/cli-configuration/#v2-vs-v1","title":"V2 vs V1","text":"<p>The AWS CLI v2 offers several new features including improved installers, new configuration options such as AWS IAM Identity Center (successor to AWS SSO), and various interactive features. </p>"},{"location":"Cloud/aws/cli-configuration/#tools","title":"Tools","text":"<pre><code># Import credentials from the .csv files generated in the AWS Console.\n$ aws configure import --csv file://path/to/creds.csv\n\n# A guided walkthrough of configuring AWS credentials\n$ aws configure wizard\n\n# Configuring and using credentials through AWS SSO\n$ aws configure sso\n\n# Auto complete\n$ complete -C aws_completer aws\n$ aws cloud&lt;TAB&gt;\n$ aws dynamodb update-table --table-name MyTable&lt;TAB&gt;\n\n# Wizards typically combine multiple AWS API calls together in order to create, update, or delete AWS resources\n# Wizards available for the configure, dynamodb, iam, and lambda commands\n$ aws &lt;service-name&gt; wizard &lt;wizard-name&gt;\n$ aws dynamodb wizard new-table\n</code></pre>"},{"location":"Cloud/aws/cli-configuration/#configure-profile","title":"Configure profile","text":"<p>To configure a specific profile in the AWS CLI, you can use the <code>aws configure</code> command and provide the profile name using the <code>--profile</code> flag. Here are the steps:</p> <ol> <li> <p>Open a terminal or command prompt.</p> </li> <li> <p>Run the following command:</p> <p><code>bash aws configure --profile your-profile-name</code></p> <p>Replace <code>your-profile-name</code> with the desired name for your AWS CLI profile.</p> </li> <li> <p>You will be prompted to enter the following information:</p> <ul> <li>AWS Access Key ID: Enter your AWS access key.</li> <li>AWS Secret Access Key: Enter your AWS secret key.</li> <li>Default region name: Enter your default AWS region.</li> <li>Default output format: You can leave this blank or enter your preferred output format (e.g., <code>json</code>).</li> </ul> </li> <li> <p>After entering the required information, the AWS CLI will create a configuration file with the specified profile in your home directory. The default location for this file is <code>~/.aws/credentials</code>. It also stores the other settings you entered in <code>~/.aws/config</code>.</p> </li> </ol> <p>Now, when you run AWS CLI commands, you can specify the profile using the <code>--profile</code> flag, like this:</p> <pre><code>$ aws s3 ls --profile your-profile-name\n</code></pre> <p>This allows you to manage multiple AWS profiles on the same machine, each with its own set of credentials and configurations.</p>"},{"location":"Cloud/aws/cli-configuration/#script","title":"Script","text":"<pre><code>export AWS_ACCOUNT_ID=121212121212\nexport AWS_DEFAULT_REGION=us-east-1\nAWS_DEFAULT_ROLE_NAME=personal-iot\n\n# login to an account if necessary\ns2al () { saml2aws login --skip-prompt --profile=${1} --role=\"arn:aws:iam::${2}:role/${AWS_DEFAULT_ROLE_NAME}\"; }\n\n# inject the active credentials for an account into your env\ns2a () { eval $(saml2aws script --shell=bash --skip-prompt --profile=${1}); }\n# shortcut to remember who you are (and which account you are in)\nawho () { aws sts get-caller-identity; }\n\n# these are the aliases to trigger login (if necessary)\nalias astage=\"s2al stage ${AWS_ACCOUNT_ID}\"\n\n# these are the aliases to trigger account switch\nalias sstage=\"s2a stage\"\n\n# these are the aliases to trigger account login (if necessary) and switch\nalias jstage=\"s2al stage ${AWS_ACCOUNT_ID} &amp;&amp; s2a stage\"\n</code></pre>"},{"location":"Cloud/aws/cli-configuration/#storing-secrets","title":"Storing Secrets","text":""},{"location":"Cloud/aws/dynamodb/","title":"DynamoDB Commands","text":""},{"location":"Cloud/aws/dynamodb/#common-commands","title":"Common commands","text":""},{"location":"Cloud/aws/dynamodb/#list-tables","title":"List tables","text":"<pre><code># List tables\n$ aws dynamodb list-tables --endpoint-url http://localhost:8000\n\n# Using different profile\n$ aws dynamodb list-tables --endpoint-url http://localhost:8000 --profile myprofile\n</code></pre>"},{"location":"Cloud/aws/dynamodb/#create-table","title":"Create table","text":"<pre><code># Create a table\n$ aws dynamodb create-table \\\n    --table-name Music \\\n    --attribute-definitions \\\n        AttributeName=Artist,AttributeType=S \\\n        AttributeName=SongTitle,AttributeType=S \\\n    --key-schema AttributeName=Artist,KeyType=HASH AttributeName=SongTitle,KeyType=RANGE \\\n    --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1 \\\n    --table-class STANDARD\n</code></pre>"},{"location":"Cloud/aws/dynamodb/#query-records","title":"Query Records","text":"<pre><code># Query records\n$ aws dynamodb query --table-name Music --key-conditions file://key-conditions.json\n</code></pre>"},{"location":"Cloud/aws/dynamodb/#addupdate-records","title":"Add/Update Records","text":"<pre><code># Upsert a record\n$ aws dynamodb put-item \\\n    --table-name Music \\\n    --item \\\n        '{\"Artist\": {\"S\": \"No One You Know\"}, \"SongTitle\": {\"S\": \"Call Me Today\"}, \"AlbumTitle\": {\"S\": \"Somewhat Famous\"}}' \\\n    --return-consumed-capacity TOTAL  \n</code></pre>"},{"location":"Cloud/aws/dynamodb/#references","title":"References","text":"<ul> <li>Amazon DynamoDB Introduction | Official guide<ul> <li>Data Modeling | DynamoDB Official guide</li> <li>Best Practices | DynamoDB Official guide</li> </ul> </li> <li>DynamoDB Guide</li> <li>LocalStack | GitHub</li> <li>DynamoDB | LocalStack</li> <li>How to setup DynamoDB locally</li> <li>dynamodb-kotlin-module | GitHub</li> </ul>"},{"location":"Cloud/aws/ec2/","title":"EC2 Commands","text":"<pre><code># List all instances\naws ec2 ls\n\n# List all ec2 instances\naws ec2 describe-instances\naws ec2 describe-instances --region us-west-2\naws ec2 describe-instances --instance-ids i-1234567890abcdef0\naws ec2 describe-instances --filters \"Name=tag:Name,Values=my-instance-name\"\n\n# Stop an instance\naws ec2 stop-instances --instance-ids i-1234567890abcdef0\n\n# Start an instance\naws ec2 start-instances --instance-ids i-1234567890abcdef0\n</code></pre>"},{"location":"Cloud/aws/ec2/#admin-commands","title":"Admin Commands","text":"<pre><code># Launch an instance\naws ec2 run-instances --image-id ami-abc12345 --count 1 --instance-type t2.micro --key-name MyKeyPair --security-group-ids sg-903004f8 --subnet-id subnet-6e7f829e\n\n# Terminate an instance\naws ec2 terminate-instances --instance-ids i-1234567890abcdef0\n</code></pre>"},{"location":"Cloud/aws/elasticache/","title":"AWS Elasticache","text":""},{"location":"Cloud/aws/elasticache/#commands","title":"Commands","text":"<pre><code># List cache clusters\naws elasticache describe-cache-clusters --region us-east-1\n</code></pre>"},{"location":"Cloud/aws/elasticache/#admin-commands","title":"Admin Commands","text":"<pre><code># Create a redis cache cluster\naws elasticache create-cache-cluster --cache-cluster-id my-cluster --engine redis --cache-node-type cache.t2.micro --num-cache-nodes 1 --region us-east-1\n\n# Create a memcached cache cluster\naws elasticache create-cache-cluster --cache-cluster-id my-cluster --engine memcached --cache-node-type cache.t2.micro --num-cache-nodes 1 --region us-east-1\n\n# Modify a cache cluster\naws elasticache modify-cache-cluster --cache-cluster-id my-cluster --num-cache-nodes 3 --apply-immediately --region us-east-1\n\n# Delete a cache cluster\naws elasticache delete-cache-cluster --cache-cluster-id my-cluster --region us-east-1\n</code></pre>"},{"location":"Cloud/aws/elasticache/#data-commands-redis","title":"Data Commands - Redis","text":"<pre><code># Connect to the Redis server\nredis-cli -h my-cluster.abc123.0001.usw2.cache.amazonaws.com -p 6379\n\n# Set a key\nset mykey myvalue\n\n# Get a key\nget mykey\n</code></pre>"},{"location":"Cloud/aws/elasticache/#data-commands-memcached-telnet","title":"Data Commands - Memcached (telnet)","text":"<pre><code># Connect to the Memcached server\ntelnet my-cluster.abc123.0001.usw2.cache.amazonaws.com 11211\n\n# Set a key\nset mykey 0 900 5\nmyval\nSTORED\n\n# Get a key\nget mykey\nVALUE mykey 0 5\nmyval\nEND\n</code></pre>"},{"location":"Cloud/aws/elasticache/#data-commands-memcached-netcat","title":"Data Commands - Memcached (netcat)","text":"<p>An alternative to <code>telnet</code> is <code>netcat</code> (<code>nc</code>). It's a simple Unix utility that reads and writes data across network connections, using TCP or UDP protocol.</p> <pre><code># Connect to the Memcached server\necho -e 'stats\\nquit' | nc my-cluster.abc123.0001.usw2.cache.amazonaws.com 11211\n\n# Set a key\necho -e 'set mykey 0 900 5\\nmyval\\nquit' | nc my-cluster.abc123.0001.usw2.cache.amazonaws.com 11211\n\n# Get a key\necho -e 'get mykey\\nquit' | nc my-cluster.abc123.0001.usw2.cache.amazonaws.com 11211\n</code></pre>"},{"location":"Cloud/aws/elasticache/#references","title":"References","text":"<ul> <li>Amazon ElastiCache | Official docs</li> </ul>"},{"location":"Cloud/aws/iam/","title":"IAM","text":""},{"location":"Cloud/aws/iam/#create-user","title":"Create User","text":"<pre><code># Create group\n$ aws iam create-group --group-name &lt;group_name&gt;\n</code></pre> <pre><code># Attach policies\n$ aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name &lt;group_name&gt;\naws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess --group-name &lt;group_name&gt;\naws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name &lt;group_name&gt;\naws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/IAMFullAccess --group-name &lt;group_name&gt;\naws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess --group-name &lt;group_name&gt;\naws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonSQSFullAccess --group-name &lt;group_name&gt;\naws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEventBridgeFullAccess --group-name &lt;group_name&gt;\n</code></pre> <pre><code># Create User\naws iam create-user --user-name &lt;user_name&gt;\n</code></pre> <pre><code># Add user to the group\naws iam add-user-to-group --user-name &lt;user_name&gt; --group-name &lt;group_name&gt;\n</code></pre> <pre><code># Create access key\naws iam create-access-key --user-name &lt;user_name&gt;\n\n# Note: Record the SecretAccessKey and AccessKeyID in the returned JSON output\n</code></pre> <pre><code># configure the aws client to use your new IAM user\naws configure           # Use your new access and secret key here\naws iam list-users      # you should see a list of all your IAM users here\n\n# Because \"aws configure\" doesn't export these vars for applications to use, we export them now\nexport AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id)\nexport AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key)\n</code></pre>"},{"location":"Cloud/aws/opensearch/","title":"OpenSearch","text":""},{"location":"Cloud/aws/opensearch/#common-commands","title":"Common Commands","text":"<pre><code># Create domain\naws --endpoint-url=http://localhost:4566 opensearch create-domain --domain-name &lt;domain_name&gt;\n# Domain will be available at http://localhost:9200/_&lt;domain_name&gt; in browser\n\n# Describe domain\naws --endpoint-url=http://localhost:4566 opensearch describe-domain --domain-name &lt;domain_name&gt;\n\n# Describe domain and check specific value (status if cluster)\naws --endpoint-url=http://localhost:4566 opensearch describe-domain --domain-name &lt;domain_name&gt; | jq \".DomainStatus.Processing\"\n\n# Check cluster health and verify it's up and running\ncurl http://localhost:9200/_cluster/health | jq .\n\n# Create an index\ncurl -X PUT localhost:9200/&lt;index_name&gt;\ncurl -X PUT localhost:9200/new-domain\n# Index will be available at the same url\n</code></pre>"},{"location":"Cloud/aws/opensearch/#specific-cases","title":"Specific cases","text":""},{"location":"Cloud/aws/opensearch/#remove-read-only-block","title":"Remove read-only block","text":"<p>This command will remove the <code>read_only_allow_delete</code> block from all indices. If you want to remove the block from a specific index, replace <code>_all</code> with the name of the index.</p> <pre><code>curl -X PUT \"localhost:9200/_all/_settings\" -H 'Content-Type: application/json' -d'\n{\n  \"index.blocks.read_only_allow_delete\": null\n}'\n</code></pre>"},{"location":"Cloud/aws/s3/","title":"S3 Commands","text":"<pre><code># List existing buckets\n$ aws s3 ls\n\n# Create the S3 bucket\n$ aws s3 mb s3://mybucket\n\n# List objects in my bucket\n$ aws s3 ls s3://my-bucket\n\n# Delete a file\n$ aws s3 rm s3://my-bucket/my-file.txt\n\n# Download a file\n$ aws s3 cp s3://my-bucket/my-file.txt my-file.txt\n\n# Upload a file\n$ aws s3 cp my-file.txt s3://my-bucket/my-file.txt\n\n# Recursive uploads and downloads of multiple files in a single folder-level\n$ aws s3 cp myfolder s3://mybucket/myfolder --recursive\n\n# Synchronize the contents of a local folder to a copy in an S3 bucket\n$ aws s3 sync myfolder s3://mybucket/myfolder --exclude *.tmp\n</code></pre>"},{"location":"Cloud/aws/s3/#admin-commands","title":"Admin Commands","text":"<pre><code># Create a bucket\naws s3api create-bucket --bucket my-bucket --region us-east-1\n\n# List buckets\naws s3api list-buckets\n\n# Delete a bucket\naws s3api delete-bucket --bucket my-bucket\n</code></pre>"},{"location":"Cloud/aws/s3/#notes","title":"Notes","text":""},{"location":"Cloud/aws/s3/#s3-ls-vs-s3api-list-buckets","title":"s3 ls vs s3api list-buckets","text":"<ol> <li> <p><code>aws s3 ls</code>: This command is part of the high-level (s3) commands provided by the AWS CLI. It lists all the S3 buckets when used without any arguments, and when used with a bucket name, it lists all the objects within that bucket. The output is simpler and more human-readable.</p> </li> <li> <p><code>aws s3api list-buckets</code>: This command is part of the low-level (s3api) commands provided by the AWS CLI. It lists all the S3 buckets in your AWS account, and provides more detailed information about each bucket, such as creation date. The output is in JSON format, which is more machine-readable and can be useful for scripting purposes.</p> </li> </ol>"},{"location":"Cloud/aws/sns/","title":"SNS Commands","text":"<pre><code># Publish to a topic\n$ aws sns publish --topic-arn arn:aws:sns:us-east-1:546419318123:my-topic --message \"Hello!\"\n\n# Subscribe to a topic\n$ aws sns subscribe --topic-arn arn:aws:sns:us-east-1:123456789012:my-topic --protocol email --notification-endpoint example@example.com\n</code></pre>"},{"location":"Cloud/aws/sns/#admin-commands","title":"Admin commands","text":"<pre><code># Create a topic\naws sns create-topic --name my-topic\n\n# List topics\naws sns list-topics\n\n# Delete a topic\naws sns delete-topic --topic-arn arn:aws:sns:us-east-1:123456789012:my-topic\n</code></pre>"},{"location":"Cloud/aws/sqs/","title":"SQS Commands","text":"<pre><code># Send a message\naws sqs send-message --queue-url https://sqs.us-east-1.amazonaws.com/123456789012/my-queue --message-body \"Hello, world!\"\n\n# Receive a message\naws sqs receive-message --queue-url https://sqs.us-east-1.amazonaws.com/123456789012/my-queue\n\n# Delete a message\naws sqs delete-message --queue-url https://sqs.us-east-1.amazonaws.com/123456789012/my-queue --receipt-handle MsgReceiptHandle\n</code></pre>"},{"location":"Cloud/aws/sqs/#admin-commands","title":"Admin Commands","text":"<pre><code># Create a queue\naws sqs create-queue --queue-name my-queue\n\n# List queues\naws sqs list-queues\n\n# Delete a queue\naws sqs delete-queue --queue-url https://sqs.us-east-1.amazonaws.com/123456789012/my-queue\n</code></pre>"},{"location":"Cloud/heroku/","title":"Heroku","text":""},{"location":"Cloud/heroku/#installation","title":"Installation","text":"<pre><code>Option 1\n$ brew install heroku/brew/heroku\n\nOption 2\n$ brew install heroku-toolbelt\n\nLog file location: ~/Library/Caches/heroku/error.log\n</code></pre>"},{"location":"Cloud/heroku/#uninstall","title":"Uninstall","text":"<pre><code>$ rm -rf /usr/local/heroku\n$ rm -rf ~/.local/share/heroku ~/.config/heroku ~/Library/Caches/heroku\nOR\n$ brew uninstall heroku\n$ rm -rf ~/.local/share/heroku ~/.config/heroku ~/Library/Caches/heroku\n</code></pre>"},{"location":"Cloud/heroku/#commands","title":"Commands","text":"<pre><code>$ heroku --version\n$ heroku update\n\n$ heroku plugins        // List any outdated plugins\n</code></pre>"},{"location":"Database/cassandra/","title":"Cassandra","text":""},{"location":"Database/cassandra/#installation","title":"Installation","text":"<pre><code>Requires Java8+ and Python\n$ java -version\n$ python --version\n\nOption 1\n$ pip install cql            # Installs CQL. To use cqlsh, the Cassandra query language shell.\n$ brew install cassandra     # Installs Apache Cassandra\n</code></pre>"},{"location":"Database/cassandra/#verification","title":"Verification","text":"<pre><code>$ cassandra -v               # Prints the installed version\n</code></pre>"},{"location":"Database/cassandra/#managing-service","title":"Managing Service","text":"<pre><code>$ cassandra -f         # Starts Cassandra server using non-daemon process, allowing to see output in terminal\n$ cqlsh                # If the server is up, run this command to open CQL shell\n\nOR\n\n$ launchctl load ~/Library/LaunchAgents/homebrew.mxcl.cassandra.plist        # Start Cassandra\n$ launchctl unload ~/Library/LaunchAgents/homebrew.mxcl.cassandra.plist      # Stop Cassandra\n\nIf any of these commands fail, execute the following once\n$ cp /usr/local/Cellar/cassandra/&lt;version number&gt;/homebrew.mxcl.cassandra.plist ~/Library/LaunchAgents/\n\nFile locations\n==============\nProperties:     /usr/local/etc/cassandra\nLogs:           /usr/local/var/log/cassandra\nData:           /usr/local/var/lib/cassandra/data\n</code></pre>"},{"location":"Database/cassandra/#docs","title":"Docs","text":"<pre><code>http://cassandra.apache.org/doc/latest/\nhttps://docs.datastax.com/en/cql/3.3/cql/cql_using/startCqlLinuxMac.html\n</code></pre>"},{"location":"Database/cassandra/#references","title":"References","text":"<pre><code>https://gist.github.com/hkhamm/a9a2b45dd749e5d3b3ae\n</code></pre>"},{"location":"Database/neo4j/","title":"Neo4j","text":""},{"location":"Database/neo4j/#installation","title":"Installation","text":"<pre><code>Download and install from http://neo4j.com/download/\n\nFYI... to play around, you can start a sandbox online\nhttps://neo4j.com/sandbox-v2/\n</code></pre>"},{"location":"Database/neo4j/#driver-installation","title":"Driver Installation","text":"<pre><code>Maven\n=====\n&lt;dependencies&gt;\n    &lt;dependency&gt;\n        &lt;groupId&gt;org.neo4j.driver&lt;/groupId&gt;\n        &lt;artifactId&gt;neo4j-java-driver&lt;/artifactId&gt;\n        &lt;version&gt;1.5.0-beta03&lt;/version&gt;\n    &lt;/dependency&gt;\n&lt;/dependencies&gt;\n\nNode\n====\n&gt; npm show neo4j-driver@* version        // Display a list of available driver versions\n&gt; npm install neo4j-driver               // Install the latest version or one of the available versions\n\nPython\n======\n&gt; pip install neo4j-driver\n\nhttps://neo4j.com/docs/developer-manual/current/drivers/get-started/\n</code></pre>"},{"location":"Database/neo4j/#docker","title":"Docker","text":"<pre><code>Running Neo4j in a Docker container\n\nBy default the Docker image exposes three ports for remote access:\n\u2022 7474 for HTTP\n\u2022 7473 for HTTPS\n\u2022 7687 for Bolt\n\nIt also exposes two volumes:\n\u2022 /data to allow the database to be persisted outside its container.\n\u2022 /logs to allow access to Neo4j log files.\n\ndocker run \\\n    --publish=7474:7474 --publish=7687:7687 \\\n    --volume=$HOME/neo4j/data:/data \\\n    --volume=$HOME/neo4j/logs:/logs \\\n    neo4j:3.3\n</code></pre>"},{"location":"Database/neo4j/#docs","title":"Docs","text":"<pre><code>https://neo4j.com/docs/api/javascript-driver/1.5/\nhttps://neo4j.com/docs/api/java-driver/1.5-preview/\nhttps://neo4j.com/docs/api/python-driver/1.5/\n\n</code></pre>"},{"location":"Database/neo4j/#references","title":"References","text":"<pre><code>https://neo4j.com/docs/operations-manual/current/configuration/file-locations/\nhttps://neo4j.com/download/other-releases/\n</code></pre>"},{"location":"Database/memcached/","title":"Memcached","text":""},{"location":"Database/memcached/#installation","title":"Installation","text":"<pre><code># Install memcached locally\n$ brew install memcached\n\n## Verify installation\n$ memcached --version\n$ memcached -V\n</code></pre>"},{"location":"Database/memcached/#starting-service","title":"Starting service","text":"<p>Run memcached in a screen session with <code>-vv</code> or <code>-vvv</code> to have it print what it's doing.</p> <pre><code># To start memcached in background\n$ brew services start memcached\n\n# if you don't want/need a background service you can just run:\n$ memcached -l localhost\n</code></pre>"},{"location":"Database/memcached/#commands","title":"Commands","text":"<pre><code># Dump all keys\n$ echo \"stats cachedump 1 0\" | nc localhost 11211\n\n# Flush all keys\necho 'flush_all' | nc localhost 11211\n</code></pre>"},{"location":"Database/memcached/#resources","title":"Resources","text":"<ul> <li>Official Wiki</li> <li>AWS Elasticache</li> </ul>"},{"location":"Database/memcached/tips-faqs/","title":"Tips - FAQs","text":""},{"location":"Database/memcached/tips-faqs/#debugging","title":"Debugging","text":"<p>You \"can\" dump keys via the debug interface <code>stats cachedump</code>, but that will only ever be a partial dump, and is slow.</p>"},{"location":"Database/memcached/tips-faqs/#caching-sessions","title":"Caching Sessions","text":"<p>TBD</p>"},{"location":"Database/memcached/tips-faqs/#get-vs-multiget","title":"Get vs MultiGet","text":"<p>They both have their own use. Fetching a single item from memcached still requires a network roundtrip and a little processing. Use a multi-get when you need more than one key at a time.</p>"},{"location":"Database/memcached/tips-faqs/#key-expiration","title":"Key Expiration","text":"<p>When items expire, total keys do not decrease right away. Expiration in memcached is lazy. In general, an item cannot be known to be expired <code>until</code> something looks at it.</p> <p>You can add billions of items to memcached that all expire at the exact same second, but no additional work is performed during that second by memcached itself.</p>"},{"location":"Database/memcached/tips-faqs/#list-all-keys","title":"List all keys","text":"<p>With memcached, you can't list all keys. There is a debug interface, but that is not an advisable usage.</p> <p>Memcached as a caching service cannot support the ability to safely walk keys without locking out all other operations. Adding indexes, multi-versioning, etc, can make this possible but will lower memory and cpu efficiency.</p>"},{"location":"Database/memcached/tips-faqs/#persistent-storage","title":"Persistent Storage","text":"<p>TBD</p>"},{"location":"Database/memcached/tips-faqs/#references","title":"References","text":"<ul> <li>Programming FAQ wiki</li> <li>Don't cache your sessions in Memcached blog</li> <li>Warm Restart wiki</li> <li>Persistent Memory blog</li> </ul>"},{"location":"Database/mongodb/","title":"MongoDB","text":"<p>MongoDB is a NoSQL database system which stores data in the form of <code>BSON</code> documents. MongoDB uses JSON-like documents with schemata.</p>"},{"location":"Database/mongodb/#index","title":"Index","text":"<p>Refer the links below to get help on following topics.</p> <ul> <li>Installation</li> <li>Cheatsheet</li> </ul>"},{"location":"Database/mongodb/#running-service","title":"Running Service","text":"<pre><code># Run mongod\n$ mongod\n</code></pre>"},{"location":"Database/mongodb/#begin-using-mongodb","title":"Begin using MongoDB","text":"<pre><code>$ mongo --host 127.0.0.1:27017\nWhen you run mongo without any arguments, the mongo shell will attempt to connect to the MongoDB instance running on the\nlocalhost interface on port 27017\n--host command line option to specify the localhost address and port that the mongod listens on\n\nUser's Home should have a file .mongorc.js\nmongo interprets the content of .mongorc.js before displaying the prompt for the first time.\n</code></pre>"},{"location":"Database/mongodb/#references","title":"References","text":"<ul> <li>Performance Difference in Mongoose vs MongoDB Native Driver</li> </ul>"},{"location":"Database/mongodb/terms/","title":"Terms","text":""},{"location":"Database/mongodb/terms/#mongod","title":"mongod","text":"<p><code>mongod</code> is the primary daemon process for the MongoDB system. It handles data requests, manages data access, and performs background management operations.</p>"},{"location":"Database/mongodb/terms/#mongosh","title":"mongosh","text":"<p><code>mongosh</code> is the shell for connecting to remote mongoDB instances. <code>mongosh</code> provides a powerful interface for systems administrators as well as a way for developers to test queries and operations directly with the database. Also provides a fully functional JavaScript environment for use with a MongoDB.</p>"},{"location":"Database/mongodb/terms/#nodejs","title":"node.js","text":"<p>In terms of Node.js, <code>mongodb</code> is the native driver for interacting with a mongodb instance.</p> <p>In terms of Node.js, <code>mongoose</code> is an Object modeling tool (a.k.a. ORM) for MongoDB. mongoose is built on top of the mongodb driver to provide programmers with a way to model their data.</p>"},{"location":"Database/mongodb/terms/#tools","title":"Tools","text":""},{"location":"Database/mongodb/terms/#mongodb-compass","title":"MongoDB Compass","text":"<pre><code>Download and install Compass from the following link or google your way through\nhttps://www.mongodb.com/download-center?filter=enterprise#compass\n</code></pre>"},{"location":"Database/mongodb/terms/#mongodb-atlas","title":"MongoDB Atlas","text":"<pre><code>MongoDB as a service. There is no local setup.\nYou can access the service at the url below after you login.\nhttps://cloud.mongodb.com\n</code></pre>"},{"location":"Database/mongodb/terms/#references","title":"References","text":"<ul> <li>Database tools | MongoDB</li> </ul>"},{"location":"Database/mongodb/1._Installation/docker/","title":"Installing via Docker","text":""},{"location":"Database/mongodb/1._Installation/docker/#download-image","title":"Download image","text":"<pre><code>$ docker pull mongo\n</code></pre>"},{"location":"Database/mongodb/1._Installation/docker/#start-an-instance-via-docker","title":"Start an instance via Docker","text":"<pre><code>$ docker run --name some-mongo -d mongo:tag\n</code></pre>"},{"location":"Database/mongodb/1._Installation/docker/#start-an-instance-via-docker-compose","title":"Start an instance via Docker-Compose","text":"<pre><code># Use root/example as user/password credentials\nversion: '3.1'\n\nservices:\n  mongo:\n    image: mongo\n    restart: always\n    environment:\n      MONGO_INITDB_ROOT_USERNAME: root\n      MONGO_INITDB_ROOT_PASSWORD: example\n\n  mongo-express:\n    image: mongo-express\n    restart: always\n    ports:\n      - 8081:8081\n    environment:\n      ME_CONFIG_MONGODB_ADMINUSERNAME: root\n      ME_CONFIG_MONGODB_ADMINPASSWORD: example\n      ME_CONFIG_MONGODB_URL: mongodb://root:example@mongo:27017/\n</code></pre>"},{"location":"Database/mongodb/1._Installation/docker/#references","title":"References","text":"<ul> <li>Mongo | Docker Hub</li> <li>Docker and MongoDB | MongoDB</li> </ul>"},{"location":"Database/mongodb/1._Installation/homebrew/","title":"Installing via Homebrew (MacOS)","text":""},{"location":"Database/mongodb/1._Installation/homebrew/#install","title":"Install","text":"<p>Download and install from here, or follow directions below</p> <pre><code>$ brew tap mongodb/brew\n$ brew install mongodb-community\n</code></pre> <p>Once installation is finished, you should see something like below:</p> <pre><code>To start mongodb/brew/mongodb-community now and restart at login:\n  brew services start mongodb/brew/mongodb-community\n\nOr, if you don't want/need a background service you can just run:\n  mongod --config /usr/local/etc/mongod.conf\n</code></pre>"},{"location":"Database/mongodb/1._Installation/homebrew/#startstop-service","title":"Start/Stop Service","text":"<pre><code># Start the database\n$ brew services start mongodb-community\nor\n$ mongod\n\n# Stop the database\nbrew services stop mongodb-community\n</code></pre>"},{"location":"Database/mongodb/1._Installation/homebrew/#verify-version","title":"Verify version","text":"<pre><code># Show current mongod version\n$ mongod --version\n\n# Show current mongosh version\n$ mongosh --version\n</code></pre>"},{"location":"Database/mongodb/1._Installation/homebrew/#test-connection","title":"Test connection","text":"<pre><code># To test the connection with the database, run below\n$ mongosh \"mongodb://localhost:27017\"\n\n# Explore all other database tools here:\n# https://www.mongodb.com/docs/database-tools/\n# or\n# https://github.com/mongodb/homebrew-brew\n</code></pre>"},{"location":"Database/mongodb/1._Installation/homebrew/#write-directory","title":"Write Directory","text":""},{"location":"Database/mongodb/1._Installation/homebrew/#configurations","title":"Configurations","text":"<p>MongoDB stores system level configurations in <code>/usr/local/etc/mongod.conf</code> file.</p> <p>You won't need any changes here to get started. However, if you need to make some changes, make sure to restart the service, for it to pick up the changes.</p>"},{"location":"Database/mongodb/1._Installation/homebrew/#default-directory","title":"Default Directory","text":"<p>The default location for <code>dbPath</code> is <code>/usr/local/var/mongodb</code>. To use a custom path, update this path in <code>mongod.conf</code> file.</p>"},{"location":"Database/mongodb/1._Installation/homebrew/#references","title":"References","text":"<ul> <li>MongoDB Shell | MongoDB</li> <li>mongodb/homebrew-brew | GitHub</li> </ul>"},{"location":"Database/mongodb/2._Cheatsheet/","title":"MongoDB Cheatsheet","text":""},{"location":"Database/mongodb/2._Cheatsheet/#mongo-shell","title":"mongo shell","text":""},{"location":"Database/mongodb/2._Cheatsheet/#helpers","title":"Helpers","text":"<pre><code># List available databases\n&gt; show dbs\n\n# Displays the database you are using\n&gt; db\n\n# Switch database. This creates a new DB if it does not exist\n&gt; use &lt;database&gt;\n\n# Show collections\n&gt; show collections\n\n# Displays all the commands available\n&gt; help\n\n# Run JavaScript File\n&gt; load(\"myScript.js\")\n\n# Exit the shell\n&gt; quit()\n</code></pre>"},{"location":"Database/mongodb/2._Cheatsheet/#references","title":"References","text":"<ul> <li>MongoDB Cheat sheet | MongoDB</li> </ul>"},{"location":"Database/mongodb/2._Cheatsheet/crud-operations/","title":"CRUD Operations","text":"<pre><code># Creates collection myCollection and inserts data\n&gt; db.myCollection.insertOne( { x: 1 } );\n\n# List data from collection\n&gt; db.getCollection(\"myCollection\")\n\n# same as db.myCollection.find()\n&gt; db.getCollection(\"myCollection\").find()\n</code></pre>"},{"location":"Database/mongodb/2._Cheatsheet/crud-operations/#references","title":"References","text":"<ul> <li>MongoDB Cheat sheet | MongoDB</li> </ul>"},{"location":"Database/mysql/","title":"MySQL","text":""},{"location":"Database/mysql/#installation","title":"Installation","text":"<pre><code>$ brew install mysql\n</code></pre>"},{"location":"Database/mysql/#verification","title":"Verification","text":"<pre><code>$ mysql.server --help                      // See different commands available for mysql server\n$ mysql.server status                      // Status of MySQL server\n</code></pre>"},{"location":"Database/mysql/#commands","title":"Commands","text":"<pre><code>$ brew services start mysql                // To have launchd start MySQL now and restart at login\n$ brew services stop mysql                 // To have launchd stop MySQL now and restart at login\n$ mysql.server start                       // if you don't want a background service, just start MySQL server\n$ mysql.server stop                        // Stop the MySQL server\n$ mysql -u root                            // connect with the command-line client\n\n$ mysqladmin -u root password 'new-password'        // Set a root password\n$ brew cask install sequel-pro                      // Install a GUI client for MySQL\n</code></pre>"},{"location":"Database/postgresql/","title":"PostgreSQL","text":"<ol> <li>Installation (local)</li> <li>Cheatsheet - DB/Table Metrics</li> <li>Cheatsheet - DB Troubleshooting</li> <li>Concepts</li> </ol>"},{"location":"Database/postgresql/#configure-to-start-automatically","title":"Configure to start automatically","text":"<pre><code>$ mkdir -p ~/Library/LaunchAgents\n$ ln -sfv /usr/local/opt/postgresql/*.plist ~/Library/LaunchAgents\n$ launchctl load ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist\n</code></pre> <p>Note: if not installed via homebrew, it should looks like <code>com.postgresapp.Postgres2LoginHelper.plist</code></p>"},{"location":"Database/postgresql/#references","title":"References","text":"<ol> <li>Postgres - Json functions | Official docs</li> <li>Safe and unsafe operations for high volume PostgreSQL</li> <li>Source Tutorial</li> <li>http://www.postgresqltutorial.com/</li> <li>Postgres Guide - tips on dates</li> <li>Cheatsheet - https://gist.github.com/Kartones/dd3ff5ec5ea238d4c546</li> </ol>"},{"location":"Database/postgresql/core-concepts/","title":"Core Concepts","text":""},{"location":"Database/postgresql/core-concepts/#schema","title":"Schema","text":""},{"location":"Database/postgresql/core-concepts/#table","title":"Table","text":""},{"location":"Database/postgresql/core-concepts/#data-types-and-columns","title":"Data Types and Columns","text":""},{"location":"Database/postgresql/core-concepts/#primary-keys","title":"Primary Keys","text":""},{"location":"Database/postgresql/core-concepts/#relationships-and-joins","title":"Relationships and Joins","text":""},{"location":"Database/postgresql/core-concepts/#constraints","title":"Constraints","text":""},{"location":"Database/postgresql/core-concepts/#indexes","title":"Indexes","text":""},{"location":"Database/postgresql/1._Installation/manage_service/","title":"Manage DB Service","text":"<p>pg_ctl is a utility to initialize, start, stop, or control a PostgreSQL server.</p> <pre><code># Display command options\n$ pg_ctl --help\n</code></pre>"},{"location":"Database/postgresql/1._Installation/postgres_clients/","title":"Client User Interfaces","text":""},{"location":"Database/postgresql/1._Installation/postgres_clients/#references","title":"References","text":"<ul> <li>Postico - Modern PostgreSQL client for Mac</li> <li>Psequel</li> <li>pgAdmin - most popular and feature rich Open Source administration and development platform for PostgreSQL. Designed to run on both client machines and on deployed servers, pgAdmin is capable of handling advanced cases that Postico cannot.</li> </ul>"},{"location":"Database/postgresql/1._Installation/verify/","title":"Verification","text":"<pre><code># Verify if the app is installed already\n$ which postgres\n\n# Display installed Postgres version\n$ postgres --version\n\n# Verify if the app is installed already\n$ which psql\n\n# Open psql command line, if installed correctly\n$ psql -h localhost\n</code></pre>"},{"location":"Database/postgresql/1._Installation/via_docker/","title":"Install via Docker","text":""},{"location":"Database/postgresql/1._Installation/via_docker/#configuration-yaml","title":"Configuration yaml","text":"<pre><code>version: '3.6'\nservices:\n  postgres:\n    image: postgres:14.7\n    restart: always\n    ports:\n    - \"5432:5432\"\n    volumes:\n    - db_data:/var/lib/postgresql/data\n    environment:\n      POSTGRES_PASSWORD: postgrespassword\nvolumes:\n  db_data:\n</code></pre>"},{"location":"Database/postgresql/1._Installation/via_docker/#installation","title":"Installation","text":"<p>Create the configuration yaml and run the following command:</p> <pre><code># Start postgres server\n$ docker-compose up -d\n\n# Go to psql terminal\n$ docker-compose exec &lt;db_container_name&gt; psql postgres &lt;user&gt;\n$ docker-compose exec &lt;db_container_name&gt; psql -U &lt;user&gt;\n</code></pre>"},{"location":"Database/postgresql/1._Installation/via_homebrew/","title":"Installation via Homebrew","text":"<p>The database will be initialized during installation, so there isn\u2019t a need to run initdb</p> <pre><code>$ brew install postgres\n</code></pre> <p>Here is a quick way of knowing if Postgres was installed via brew, brew initializes the database during installation by <code>postgres -D /usr/local/var/postgres</code> this creating the directory specified.</p>"},{"location":"Database/postgresql/1._Installation/via_homebrew/#manage-db-service","title":"Manage DB Service","text":"<pre><code># Starts as a background service\n$ brew services start postgresql\n\n# Stop the service manually\n$ brew services stop postgresql\n\n# Restart the service\n$ brew services restart postgresql\n</code></pre>"},{"location":"Database/postgresql/1._Installation/via_homebrew/#uninstallation","title":"Uninstallation","text":"<pre><code># If installed via homebrew\n$ brew remove postgresql\n</code></pre>"},{"location":"Database/postgresql/1._Installation/via_native_app/","title":"Installation via Native app","text":"<p>Download and install from http://postgresapp.com/ A native macOS app that runs in the menubar without the need of an installer.</p> <p>(Another graphical installer alternative: https://www.bigsql.org/postgresql/installers.jsp)</p> <pre><code># Configure PATH\nsudo mkdir -p /etc/paths.d &amp;&amp;\n    echo /Applications/Postgres.app/Contents/Versions/latest/bin | sudo tee /etc/paths.d/postgresapp\n\n# OR\nPATH=\"/Applications/Postgres.app/Contents/Versions/latest/bin:$PATH\"\n</code></pre> <p>Note: If you are going to use the app, it is recommended to uninstall other PostgreSQL installations due to port conflicts.</p>"},{"location":"Database/postgresql/1._Installation/via_native_app/#uninstallation","title":"Uninstallation","text":"<p>Removing app from Applications folder will be good enough. Just clean the PATH as well.</p>"},{"location":"Database/postgresql/2._concepts/","title":"Concepts","text":"<ul> <li>PostgreSQL can have multiple databases in each instance.</li> <li>Each database can have multiple schemas.</li> <li>Each schema can have multiple tables.</li> </ul>"},{"location":"Database/postgresql/2._concepts/#topics","title":"Topics","text":"<ul> <li>Installation</li> <li>Server + Utilities</li> <li>Login</li> <li>User</li> <li>Database</li> <li>Schema</li> <li>Connection + Connection pool</li> </ul>"},{"location":"Database/postgresql/2._concepts/#resources","title":"Resources","text":"<ul> <li>SQL Indexing and Tuning e-Book</li> <li>AWS re:Invent 2018: Close Loops &amp; Opening Minds: How to Take Control of Systems, Big &amp; Small ARC337 | YouTube</li> </ul>"},{"location":"Database/postgresql/2._concepts/1._server/","title":"Postgres Database Server","text":""},{"location":"Database/postgresql/2._concepts/1._server/#connection-string","title":"Connection String","text":"<p>Connection String is built off of following parameters:</p> <pre><code>POSTGRES_USER = postgres\nPOSTGRES_PASSWORD = postgrespassword\nPOSTGRES_HOST = host.docker.internal\nPOSTGRES_PORT = 5432\nPOSTGRES_DATABASE = postgres\n</code></pre> <p>Note: Host for mac is <code>host.docker.internal</code></p> <p>There are two formats which are widely used:</p> <pre><code># plain keyword/value strings\nhost=localhost port=5432 dbname=mydb connect_timeout=10\n\n# Connection URIs\npostgresql://username:password@host:port/dbname[?paramspec]\npostgresql://postgres:postgrespassword@host.docker.internal:5432/postgres\n</code></pre>"},{"location":"Database/postgresql/2._concepts/1._server/#postgresql-clients","title":"PostgreSQL Clients","text":"<p>Some of the popular client applications for PostgreSQL include</p> <ul> <li><code>pg_dump</code> - Extracting database into a file</li> <li><code>pg_restore</code> - Restoring a database from a file</li> <li><code>create_db</code> - Create a new PostgreSQL database</li> <li><code>create_user</code> - Create a new PostgreSQL user account</li> <li><code>psql</code> - Interactive terminal</li> </ul> <p>A full list of client applications is available here</p>"},{"location":"Database/postgresql/2._concepts/1._server/#command-line-utilities","title":"Command line utilities","text":"<p>Instead of logging into psql, executing SQL queries, and needing to know the details of the query statements, you can use a familiar command line interface to do the same tasks.</p> <pre><code>$ createuser            # creates a user\n$ createdb              # creates a database\n$ dropuser              # deletes a user\n$ dropdb                # deletes a database\n$ postgres              # executes the SQL server itself\n$ pg_dump               # dumps the contents of a single database to a file\n$ pg_dumpall            # dumps all databases to a file\n$ psql                  # Lets you carry administrative functions without needing to know their actual SQL commands\n$ initdb                # initialize a PostgreSQL data directory\n</code></pre>"},{"location":"Database/postgresql/2._concepts/1._server/#psql","title":"psql","text":"<p><code>psql</code> is an interactive terminal, we can use to execute commands, SQL statements and control the database as a whole.</p> <p>This utility is installed with Postgres that lets you carry out administrative functions without needing to know their actual SQL commands. By default, Postgres automatically creates the user <code>postgres</code>. It also creates user for the account you are logged in.</p>"},{"location":"Database/postgresql/2._concepts/2._user/","title":"Postgres Database User","text":""},{"location":"Database/postgresql/2._concepts/2._user/#creating-a-new-user","title":"Creating a new user","text":"<pre><code>&gt; CREATE ROLE admin WITH LOGIN PASSWORD 'admin'\n&gt; ALTER ROLE admin CREATEDB\nOR\n&gt; createuser admin --createdb\n</code></pre>"},{"location":"Database/postgresql/2._concepts/2._user/#references","title":"References","text":"<ul> <li>Create Role | Official docs</li> <li>Alter Role | Official docs</li> </ul>"},{"location":"Database/postgresql/2._concepts/3._database/","title":"Postgres Database","text":""},{"location":"Database/postgresql/2._concepts/3._database/#create-a-new-database","title":"Create a new Database","text":"<pre><code>postgres=#  CREATE DATABASE database_name;\nOR\n&gt; createdb database_name -U admin\n</code></pre>"},{"location":"Database/postgresql/2._concepts/3._database/#database-permissions","title":"Database permissions","text":"<p>Once a database is created, you need to add at least one user who has permission to access the database (aside from the super users, who can access everything).</p> <pre><code>&gt; GRANT ALL PRIVILEGES ON DATABASE database_name TO &lt;user&gt;;\n&gt; GRANT ALL PRIVILEGES ON DATABASE database_name TO admin;\n\n-- Lists all the databases in Postgres\n&gt; \\list\n\n-- Connect to a specific database\n&gt; \\connect &lt;database_name&gt;\n\n-- List the tables in the currently connected database\n&gt; \\dt\n</code></pre>"},{"location":"Database/postgresql/2._concepts/3._database/#rename-a-database","title":"Rename a database","text":"<pre><code>&gt; ALTER DATABASE database_name RENAME TO awesome_application;\n</code></pre>"},{"location":"Database/postgresql/2._concepts/4._schema/","title":"Schema","text":"<p>A <code>schema</code> is a namespace or a named collection of tables, views, functions, constraints, indexes, sequences etc.</p> <p>PostgreSQL supports having multiple schemas in a single database there by letting you namespace different features into different schemas.</p> <p>Fundamentally, schemas let users namespace their various application features, especially third-party stuff to have their own space and not interfere with the primary data source.</p> <p>Especially with role based access, it's easier to restrict access to schemas.</p> <p>By default, the <code>public</code> schema is used in PostgreSQL when you set it up for the first time. Any SQL queries executed will run against the <code>public</code> schema by default unless explicitly mentioned.</p>"},{"location":"Database/postgresql/2._concepts/4._schema/#managing-schema","title":"Managing Schema","text":""},{"location":"Database/postgresql/2._concepts/4._schema/#list-available-schemas","title":"List available Schemas","text":"<pre><code>postgres=# SELECT schema_name FROM information_schema.schemata;\n\npostgres=# SELECT nspname FROM pg_catalog.pg_namespace;\n\npostgres=# \\dn\n</code></pre>"},{"location":"Database/postgresql/2._concepts/4._schema/#creating-schema","title":"Creating Schema","text":"<pre><code>CREATE SCHEMA &lt;schema_name&gt;;\n\npostgres=# CREATE SCHEMA ecommerce;\n</code></pre>"},{"location":"Database/postgresql/2._concepts/4._schema/#dropping-schema","title":"Dropping Schema","text":"<pre><code>DROP SCHEMA &lt;schema_name&gt;;\n\n-- to cascade delete all referenced objects\nDROP SCHEMA &lt;schema_name&gt; CASCADE;\n</code></pre>"},{"location":"Database/postgresql/2._concepts/4._schema/#information-schema","title":"Information Schema","text":"<p>The <code>information_schema</code> consists of a set of views that contain information about the objects defined in the current database. This schema automatically exists in all databases. The information schema views do not, however, contain information about PostgreSQL-specific features; to inquire about those you need to query the system catalogs or other PostgreSQL-specific views.</p>"},{"location":"Database/postgresql/2._concepts/4._schema/#references","title":"References","text":"<ul> <li>Information Schema | Official documentation</li> </ul>"},{"location":"Database/postgresql/3._tips/disk-usage/","title":"Disk Usage Metrics","text":""},{"location":"Database/postgresql/3._tips/disk-usage/#database-size","title":"Database Size","text":"<pre><code>-- Print size of the database in use\nSELECT pg_size_pretty(pg_database_size('some_database_name'));\n</code></pre>"},{"location":"Database/postgresql/3._tips/disk-usage/#table-size","title":"Table Size","text":"<pre><code>-- Measure table size\nSELECT pg_size_pretty(pg_relation_size('table_name'));\n\n-- Measure index size\nSELECT pg_size_pretty(pg_relation_size('table_index_name'));\n\n-- Measure table size, along with indexes\nSELECT pg_size_pretty(pg_total_relation_size('table_name'));\n\n-- Print total column size, average size and percentage occupancy in a table\nSELECT\n    sum(pg_column_size(column_name)) AS total_size,\n    avg(pg_column_size(column_name)) AS average_size,\n    sum(pg_column_size(column_name)) * 100.0 / pg_relation_size('table_name') AS percentage\nFROM table_name;\n\n-- (Pretty) Print total column size, average size and percentage occupancy in a table\nSELECT\n    pg_size_pretty(sum(pg_column_size(column_name))) AS total_size,\n    pg_size_pretty(avg(pg_column_size(column_name))) AS average_size,\n    sum(pg_column_size(column_name)) * 100.0 / pg_relation_size('table_name') AS percentage\nFROM table_name;\n\n-- TODO: Yet to fix this query\nSELECT\npercentile_disc(0.25) within group (order by table_name) p25,\npercentile_disc(0.75) within group (order by table_name) p75,\npercentile_disc(0.90) within group (order by table_name) p90,\npercentile_disc(0.95) within group (order by table_name) p95,\npercentile_disc(0.99) within group (order by table_name) p99,\npercentile_disc(0.999) within group (order by table_name) p999,\npercentile_disc(0.9999) within group (order by table_name) p9999,\nmax(table_name) max\n    FROM (\n        SELECT col FROM (\n            SELECT t.pk, t.count(column_name) AS col\n                FROM table_name t\n            GROUP BY 1\n        ) a\n    ) b;\n</code></pre>"},{"location":"Database/postgresql/3._tips/performance/","title":"Performance Considerations","text":"<p>Postgres itself actually tracks access patterns of your data and will on its own keep frequently accessed data in cache.</p> <p>Generally you want your database to have a cache hit rate of about <code>99%</code>. If you find yourself with a ratio significantly lower than 99% then you likely want to consider increasing the cache available to your database. You can find your cache hit rate with:</p> <pre><code>SELECT\n       sum(heap_blks_read) as heap_read, \n       sum(heap_blks_hit) as heap_hit, \n       (sum(heap_blks_hit) - sum(heap_blks_read)) / sum(heap_blks_hit) as ratio\nFROM\n     pg_statio_user_tables;\n</code></pre>"},{"location":"Database/postgresql/3._tips/performance/#indexes","title":"Indexes","text":"<p>The other primary piece for improving performance is <code>indexes</code>. While accessing data from cache is faster than disk, even data within memory can be slow if Postgres must parse through hundreds of thousands of rows to identify if they meet a certain condition. To generate a list of your tables in your database with the largest ones first and the percentage of time which they use an index you can run:</p> <pre><code>SELECT\n      relname, 100 * idx_scan / (seq_scan + idx_scan) percent_of_times_index_used, \n      n_live_tup rows_in_table\nFROM\n     pg_stat_user_tables\nWHERE\n     seq_scan + idx_scan &gt; 0\nORDER BY\n     n_live_tup DESC;\n</code></pre> <p>While there is no perfect answer, if you\u2019re not somewhere around 99% on any table over 10,000 rows you may want to consider adding an index. When examining where to add an index you should look at what kind of queries you\u2019re running. Generally you\u2019ll want to add indexes where you\u2019re looking up by some other id or on values that you\u2019re commonly filtering on such as created_at fields.</p> <p>Note: If you\u2019re adding an index on a production database use <code>CREATE INDEX CONCURRENTLY</code> to have it build your index in the background and not hold a lock on your table. The limitation to creating indexes concurrently is they can typically take 2-3 times longer to create and can\u2019t be run within a transaction. Though for any large production site these trade-offs are worth the trade-off in experience to your end users.</p>"},{"location":"Database/postgresql/3._tips/performance/#index-cache-hit-rate","title":"Index Cache Hit Rate","text":"<pre><code>SELECT\n   sum(idx_blks_read) as idx_read, \n   sum(idx_blks_hit) as idx_hit, \n   (sum(idx_blks_hit) - sum(idx_blks_read)) / sum(idx_blks_hit) as ratio\nFROM\n     pg_statio_user_indexes;\n</code></pre>"},{"location":"Database/postgresql/3._tips/performance/#references","title":"References","text":"<ul> <li>Cache | Postgres Guide</li> <li>Postgres Performance | Heroku</li> </ul>"},{"location":"Database/postgresql/3._tips/query-date-and-time/","title":"Using date and time","text":""},{"location":"Database/postgresql/3._tips/query-date-and-time/#data-types","title":"Data types","text":"<ul> <li>Date - Date only (2012-04-25)</li> <li>Time - Time only (13:00:00.00)</li> <li>Timestamp - Date and Time (2012-04-25 13:00:00.00)</li> <li>Time with Timezone - Time only (13:00:00.00 EST)</li> <li>Timestamp with Timezone (2012-04-25 13:00:00.00 EST)</li> <li>Interval - A span of time (4 days)</li> </ul> <p>Note: All timezone-aware dates and times are saved internally in UTC. PostgreSQL stores the timestamp in UTC value.</p>"},{"location":"Database/postgresql/3._tips/query-date-and-time/#print-datetime","title":"Print date/time","text":"<pre><code>-- Prints current timestamp with timezone\nSELECT now();\nSELECT current_timestamp;\n\n-- Prints current timestamp without timezone\nSELECT NOW()::timestamp;\n\n-- Prints current date\nSELECT NOW()::date;\n\n-- Prints current time\nSELECT NOW()::time;\n\n-- Prints current timestamp in selected timezone\nSELECT now() AT TIME ZONE 'EST';\nSELECT now() AT TIME ZONE 'UTC';\n</code></pre>"},{"location":"Database/postgresql/3._tips/query-date-and-time/#tips","title":"Tips","text":""},{"location":"Database/postgresql/3._tips/query-date-and-time/#truncating-timestamps","title":"Truncating timestamps","text":"<pre><code>SELECT count(*), date_trunc('day', created_at)\n    FROM users\n    GROUP BY 2\n    ORDER BY 2 DESC;\n</code></pre>"},{"location":"Database/postgresql/3._tips/query-date-and-time/#intervals","title":"Intervals","text":"<pre><code>SELECT count(*)\n    FROM users\n    WHERE created_at &gt;= (now() - '1 day'::INTERVAL);\n\nSELECT count(*)\n    FROM users\n    WHERE created_at &gt;= (now() - interval '1 month');\n\n-- Extracting data from interval\nSELECT\n    EXTRACT (\n        MINUTE FROM INTERVAL '7 hours 33 minutes'\n    );\n\n-- Converting interval to string\nSELECT\n    to_char(\n        INTERVAL '12h 35m 24s',\n        'HH24:MI:SS'\n    );\n\nSELECT\n    to_char(\n        INTERVAL '20h 42m 48s',\n        'HH12:MI:SS'\n    );\n</code></pre>"},{"location":"Database/postgresql/3._tips/query-date-and-time/#references","title":"References","text":"<ul> <li>https://www.postgresguide.com/tips/dates/</li> <li>https://databasefaqs.com/postgresql-now-function</li> <li>Date input format | Official doc</li> <li>Time input format | Official doc</li> <li>Timezone input format | Official doc</li> </ul>"},{"location":"Database/postgresql/3._tips/query-json/","title":"Query Jsonb","text":"<p>JSONB is an on disk binary representatin of JSON, this means it\u2019s more efficiently stored and indexable.</p> <pre><code>-- Extracting an attribute\nSELECT preferences-&gt;'key_name' FROM user_preferences;\n\n-- Extracting an attribute as text*\nSELECT preferences-&gt;&gt;'key_name' FROM user_preferences;\n\n-- Some key holds some value\nSELECT * FROM user_preferences\n    WHERE preferences-&gt;'key_name' ? 'value';\n\n-- Extracting First element of an array in json field\nSELECT preferences::json-&gt;0 FROM user_preferences;\n\nSELECT preferences::json-&gt;'key_name' FROM user_preferences;\nSELECT * FROM user_preferences\n    WHERE preferences-&gt;&gt;'key_name' = 'value';\n</code></pre>"},{"location":"Database/postgresql/3._tips/query-metadata/","title":"Query Metadata","text":""},{"location":"Database/postgresql/3._tips/query-metadata/#indexes","title":"Indexes","text":"<pre><code>-- Show table indexes\nSELECT * FROM pg_indexes\n    WHERE tablename='table_name' AND schemaname='schema_name';\n\n-- Get all indexes from all tables of a schema\nSELECT\n   t.relname AS table_name,\n   i.relname AS index_name,\n   a.attname AS column_name\n    FROM\n       pg_class t,\n       pg_class i,\n       pg_index ix,\n       pg_attribute a,\n       pg_namespace n\n    WHERE\n       t.oid = ix.indrelid\n       AND i.oid = ix.indexrelid\n       AND a.attrelid = t.oid\n       AND a.attnum = ANY(ix.indkey)\n       AND t.relnamespace = n.oid\n--         AND n.nspname = 'some_name'\n    ORDER BY\n       t.relname,\n       i.relname;\n</code></pre>"},{"location":"Database/postgresql/3._tips/query-metadata/#view-elements","title":"View Elements","text":"<pre><code>-- Show table indexes\nSELECT * FROM pg_indexes WHERE tablename='table_name' AND schemaname='schema_name';\n\n-- List all procedures\nSELECT * FROM pg_proc;\n\n-- List view (including the definition)\nSELECT * FROM pg_views;\n\n-- List extensions enabled\nSELECT * FROM pg_extension;\n\n-- List available extensions\nSELECT * FROM pg_available_extension_versions;\n</code></pre>"},{"location":"Database/postgresql/3._tips/query-metadata/#configuration","title":"Configuration","text":"<pre><code>-- Show current user's statement timeout\nshow statement_timeout;\n</code></pre>"},{"location":"Database/postgresql/3._tips/troubleshooting/","title":"Troubleshooting","text":""},{"location":"Database/postgresql/3._tips/troubleshooting/#queries","title":"Queries","text":"<pre><code>-- Get all queries from all DBs\nSELECT * FROM pg_stat_activity;\n-- TODO: Get all queries from all DBs, waiting for data\n-- SELECT * FROM pg_stat_activity WHERE waiting='t';\n\n-- Currently running queries with process pid:\nSELECT\n  pg_stat_get_backend_pid(s.backendid) AS procpid,\n  pg_stat_get_backend_activity(s.backendid) AS current_query\nFROM (SELECT pg_stat_get_backend_idset() AS backendid) AS s;\n</code></pre>"},{"location":"Database/postgresql/3._tips/troubleshooting/#connections","title":"Connections","text":"<pre><code>-- Get Connections by Database\nSELECT datname, numbackends FROM pg_stat_database;\n\n-- Kill all Connections:\nSELECT pg_terminate_backend(pg_stat_activity.pid)\n    FROM pg_stat_activity\n    WHERE datname = current_database() AND pid &lt;&gt; pg_backend_pid();\n</code></pre>"},{"location":"Database/postgresql/3._tips/troubleshooting/#permissions","title":"Permissions","text":"<pre><code>-- Check permissions in a table:\nSELECT grantee, privilege_type\n    FROM information_schema.role_table_grants\n    WHERE table_name='name-of-the-table';\n</code></pre>"},{"location":"Database/postgresql/3._tips/using-psql/","title":"Using psql","text":"<p>Psql is the interactive terminal for working with Postgres.</p>"},{"location":"Database/postgresql/3._tips/using-psql/#commands","title":"Commands","text":""},{"location":"Database/postgresql/3._tips/using-psql/#login-to-postgres-server","title":"Login to postgres server","text":"<pre><code># Login to psql command line\n$ psql -h &lt;host&gt; -d &lt;database&gt; -U &lt;user&gt; -p &lt;port&gt;\n$ psql -h 127.0.0.1 -d postgres -U postgres -p 5432\n\n# Login to psql command line, with defaults other than username\n$ psql postgres -U &lt;username&gt;\n\n# Login using connection string\n$ psql postgresql://username:password@host:port/dbname\n</code></pre> <p>Running the parameterized command prompts you to enter the password. If the connection is successful, you should see something like this:</p> <pre><code>psql (14.7)\nType \"help\" for help.\n\npostgres=#\n</code></pre> <p>Connecting in SSL mode</p> <pre><code>$ psql \"sslmode=require host=&lt;host&gt; dbname=&lt;database&gt; user=&lt;user&gt;\"\n</code></pre> <p>If the command runs successfully, you should see a similar output in your terminal:</p> <pre><code>psql (14.7)\nSSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)\nType \"help\" for help.\n\npostgres=#\n</code></pre> <p>It's important to mention that the Postgres server should support SSL to use the SSL mode.</p>"},{"location":"Database/postgresql/3._tips/using-psql/#context","title":"Context","text":"<pre><code>-- Connect to another database\n# \\c dbname\n# \\c dbname username\n\n-- Quit from postgres shell\n# \\q\n\n-- Text editor inside psql\n# \\e\n\n-- Displays a list of users installed by running a query internally\n# \\du\n</code></pre>"},{"location":"Database/postgresql/3._tips/using-psql/#table","title":"Table","text":"<pre><code>-- List tables in database\n# \\d\n# \\dt\n\n-- List all tables in database along with size\n# \\d+\n\n-- Describe a table\n# \\d &lt;table_name&gt;\n# \\d users\n\n-- Describe a table with constraints\n#\\d+ users\n</code></pre>"},{"location":"Database/postgresql/3._tips/using-psql/#schema","title":"Schema","text":"<pre><code>-- List all databases\n# \\l\n# \\list\n\n-- List all databases with additional information\n# \\l+\n\n-- List all schemas\n# \\dn\n\n-- List all schemas with permissions\n# \\dn+\n\n-- List all functions with additional information\n#\\df+\n\n-- Lists all functions that contain to_array in its name\n\\df *to_array*\n</code></pre>"},{"location":"Database/postgresql/3._tips/using-psql/#configurations","title":"Configurations","text":"<pre><code>-- Timing is on.\n# \\timing\n\n-- Setting password. It'll ask you to enter the password\n# \\password &lt;role&gt;\n# \\password postgres\n</code></pre>"},{"location":"Database/postgresql/3._tips/using-psql/#references","title":"References","text":"<ul> <li>Psql | Postgres Guide</li> <li>Psql | PostgreSQL Client Applications</li> <li>psql command line tutorial and cheat sheet | GitHub tomcam</li> <li>17 Practical psql Commands That You Don\u2019t Want To Miss</li> </ul>"},{"location":"Database/postgresql/administration/","title":"Postgres Administration [TODO]","text":""},{"location":"Database/postgresql/administration/#database-hosting","title":"Database Hosting","text":"<ul> <li>https://aws.amazon.com/free/database/</li> <li>https://heliohost.org/</li> <li>https://supabase.com/</li> <li>https://fly.io/blog/free-postgres/</li> <li>https://hasura.io/learn/database/postgresql/installation/5-postgresql-hosting-providers/</li> </ul>"},{"location":"Database/postgresql/administration/#references","title":"References","text":"<ul> <li>Concepts<ul> <li>https://docs.qgis.org/2.18/en/docs/training_manual/database_concepts/index.html</li> <li>https://www.postgresqltutorial.com/</li> <li>https://arctype.com/blog/postgresql-features-list/</li> <li>http://www.foo.be/docs-free/aw_pgsql_book.pdf</li> <li>https://www.codecademy.com/learn/fscp-advanced-postgresql/modules/fscp-postgres-database-performance/cheatsheet</li> <li>https://www.educba.com/postgresql-features/</li> </ul> </li> <li>Bloating<ul> <li>https://www.cybertec-postgresql.com/en/index-bloat-reduced-in-postgresql-v14/</li> <li>https://medium.com/compass-true-north/dealing-with-significant-postgres-database-bloat-what-are-your-options-a6c1814a03a5</li> <li>https://www.highgo.ca/2021/03/20/how-to-check-and-resolve-bloat-in-postgresql/</li> <li>https://www.highgo.ca/tag/postgresql/</li> </ul> </li> <li>Connection Management<ul> <li>https://pgpool.net/mediawiki/index.php/Main_Page</li> <li>https://scalegrid.io/blog/postgresql-connection-pooling-part-4-pgbouncer-vs-pgpool/</li> <li>https://tommasini-giovanni.medium.com/resilient-postgresql-cluster-pgbouncer-pgpool-ii-and-repmgr-88830de6e8ea</li> </ul> </li> <li>https://www.cybertec-postgresql.com/en/pgbouncer-types-of-postgresql-connection-pooling/</li> </ul>"},{"location":"Database/postgresql/code/code-nodejs/","title":"Code Node.js","text":"<p>Assuming installation is already done, breaking down tasks into smaller sections</p> <ul> <li>Setup local DB service up and running</li> <li>Setup DB connection in the code</li> <li>Ability to import data</li> <li>Ability to create/update/delete</li> </ul>"},{"location":"Database/postgresql/code/code-nodejs/#setup-local-db-service","title":"Setup local DB service","text":"<p>Make sure the service is up and running, however the way it was installed on the machine</p> <pre><code># To create a database for your new application\n$ initdb &lt;db_directory&gt; -E utf8\n$ initdb /usr/local/var/postgres -E utf8\n</code></pre> <p>Ideally we shouldn't need this step, since creating DB should be part of automation.</p>"},{"location":"Database/postgresql/code/code-nodejs/#nodejs","title":"Node.js","text":""},{"location":"Database/postgresql/code/code-nodejs/#packages-required","title":"Packages required","text":"<pre><code>pg            # Postgres Client for Node\npg-format     # Allows us to safely make dynamic SQL queries\nexpress       # Create a quick and basic server. Doesn't have to via Express though\n</code></pre>"},{"location":"Database/postgresql/code/code-nodejs/#environment-variables","title":"Environment Variables","text":"<p>Refer this</p> <pre><code>PGHOST='localhost'\nPGUSER=process.env.USER\nPGDATABASE=process.env.USER\nPGPASSWORD=null\nPGPORT=5432\n</code></pre>"},{"location":"Database/postgresql/code/code-nodejs/#code","title":"Code","text":"<pre><code>const { Pool, Client } = require('pg')\n\n# pools will use environment variables for connection information\nconst pool = new Pool()\n\npool.query('SELECT NOW()', (err, res) =&gt; {\n  console.log(err, res)\n  pool.end()\n})\n\n# you can also use async/await\nconst res = await pool.query('SELECT NOW()')\nawait pool.end()\n\n# clients will also use environment variables for connection information\nconst client = new Client()\nawait client.connect()\n\nconst res = await client.query('SELECT NOW()')\nawait client.end()\n</code></pre>"},{"location":"Database/postgresql/code/code-nodejs/#resources","title":"Resources","text":"<ul> <li>https://www.packtpub.com/books/content/how-setup-postgresql-nodejs</li> <li>https://node-postgres.com/</li> </ul>"},{"location":"Database/redis/","title":"Redis","text":""},{"location":"Database/redis/#installation","title":"Installation","text":"<pre><code>$ brew install redis\n</code></pre>"},{"location":"Database/redis/#commands-server","title":"Commands - Server","text":"<pre><code>$ redis-server\n\n# Alternatively, start in daemon mode using\n$ redis-server --daemonize yes\n</code></pre>"},{"location":"Database/redis/#commands-client","title":"Commands - Client","text":"<pre><code>$ redis-cli ping\n</code></pre>"},{"location":"Database/redis/#redis-shell-commands","title":"Redis Shell Commands","text":"<pre><code>&gt; set a 1\n&gt; get a\n&gt; keys *\n&gt; quit\n</code></pre>"},{"location":"Infrastructure/ansible/","title":"Ansible","text":"<p>Ansible is an open source, command-line IT automation software application written in Python. It can configure systems, deploy software, and orchestrate advanced workflows to support application deployment, system updates, and more.</p>"},{"location":"Infrastructure/ansible/#ansible-builder","title":"Ansible Builder","text":"<p>Ansible Builder is a tool that aids in the creation of Ansible Execution Environments. The builder creates consistent and reproducible automation execution environments.</p>"},{"location":"Infrastructure/ansible/#ansible-semaphore","title":"Ansible Semaphore","text":"<p>Ansible Semaphore is beautiful web interface for running Ansible playbooks. If your project has grown and deploying from the terminal is no longer for you then Ansible Semaphore is what you need.</p> <ul> <li>YT This web UI for Ansible is so damn useful! | ChristianLempa</li> </ul>"},{"location":"Infrastructure/ansible/#event-driven-ansible","title":"Event-driven Ansible","text":"<p>Event-Driven Ansible provides the event-handling capability needed to automate time-consuming tasks and respond to changing conditions in any IT domain. </p> <p>Event-Driven Ansible can process events containing discrete intelligence about conditions in the IT environment, determine the appropriate response to the event, then execute automated actions to address or remediate the event.</p>"},{"location":"Infrastructure/ansible/#references","title":"References","text":"<ul> <li>How Ansible works | Ansible</li> <li>Introduction to Ansible Builder | Official docs</li> <li>Introduction to Ansible Builder | Ansible Blog</li> <li>Event-Driven Ansible | RedHat</li> </ul>"},{"location":"Infrastructure/docker/","title":"Docker","text":"<ol> <li>Installation</li> <li>Cheatsheet Docker</li> <li>Cheatsheet Docker Compose</li> <li>Docker Compose Configuration</li> <li>Docker Networking</li> <li>Docker Images</li> <li>Docker Containers</li> <li>Persistent Storage</li> <li>Resource Clean up</li> <li>Example Nginx with Volume</li> <li>Example Nodejs Web Server w/ Proxy</li> <li>Example Build Custom Docker Image</li> <li>BuildKit</li> </ol>"},{"location":"Infrastructure/docker/#good-to-know","title":"Good to know","text":"<ol> <li>The Docker for Mac application does not use docker-machine to provision that VM; but rather creates and manages it directly.</li> <li>Docker exposes the docker API on a socket in <code>/var/run/docker.sock</code>. Since this is the default location where docker will look if no environment variables are set, you can start using docker and docker-compose without setting any environment variables.</li> <li>With Docker for Mac, you get only one VM, and you don\u2019t manage it. It is managed by the Docker for Mac application, which includes auto-update to update the client and server versions of Docker.</li> <li>If you need several VMs and want to manage the version of the Docker client or server you are using, you can continue to use docker-machine, on the same machine.</li> </ol>"},{"location":"Infrastructure/docker/#to-be-explored","title":"To be explored","text":"<ul> <li>pid / uts / ipc settings</li> <li>Networking</li> <li>Policies</li> <li>Debugging, Logging</li> <li>Security</li> <li>Constraints on resources</li> <li>Parameters &amp; Environment Variables</li> </ul>"},{"location":"Infrastructure/docker/#resources","title":"Resources","text":"<ol> <li>Official Docs</li> <li>Play with Docker Labs</li> <li>Dockerfile reference</li> <li>Docker <code>run</code> reference</li> <li>Docker Network settings</li> <li>Docker Restart policies</li> <li>Dockerfile parameters</li> <li>Use the docker command line reference</li> <li>egghead.io Docker Fundamentals</li> <li>Docker cheatsheet - GitHub wsargent</li> <li>Docker Hub mongo reference</li> </ol>"},{"location":"Infrastructure/docker/buildkit/","title":"BuildKit","text":"<p>BuildKit is a toolkit for converting source code to build artifacts in an efficient, expressive and repeatable manner. Simply put, it is what turns a Dockerfile into a Docker image. And it doesn\u2019t just build Docker images; it can build OCI images and several other output formats.</p> <p>BuildKit is a proposal to separate out docker build experience into a separate project, allowing different users to collaborate on the underlying technology and reuse and customize it in different ways. One of the main design goals of BuildKit is to separate frontend and backend concerns during a build process</p> <p>Note: BuildKit has been integrated to docker build since Docker 18.09</p>"},{"location":"Infrastructure/docker/buildkit/#installation-setup","title":"Installation &amp; Setup","text":"<pre><code>$ brew install buildkit\n</code></pre> <p>You need to run buildkitd as the root user on the host.</p> <pre><code>$ sudo buildkitd\n\n# Verify installation\n$ buildctl -h\n</code></pre>"},{"location":"Infrastructure/docker/buildkit/#building-a-dockerfile-with-buildctl","title":"Building a Dockerfile with buildctl","text":"<pre><code>buildctl build \\\n    --frontend=dockerfile.v0 \\\n    --local context=. \\\n    --local dockerfile=.\n# or\nbuildctl build \\\n    --frontend=dockerfile.v0 \\\n    --local context=. \\\n    --local dockerfile=. \\\n    --opt target=foo \\\n    --opt build-arg:foo=bar\n</code></pre>"},{"location":"Infrastructure/docker/buildkit/#hlb","title":"HLB","text":"<p>HLB is a high-level build language for BuildKit.</p> <p>Describe your build in containerized units of work, and BuildKit will build your target as efficiently as possible.</p> <p>With BuildKit, we can substitute the dockerfile syntax for hlb and replace the docker image format for a pure tar file output. That is just one of the possible combinations BuildKit, with its pluggable backends and frontends, unlocks.</p>"},{"location":"Infrastructure/docker/buildkit/#references","title":"References","text":"<ul> <li>What is BuildKit? blog</li> <li>Creator's blog</li> </ul>"},{"location":"Infrastructure/docker/cheatsheet-dc-configuration/","title":"Docker Compose Configuration Cheatsheet","text":""},{"location":"Infrastructure/docker/cheatsheet-dc-configuration/#health-check","title":"Health Check","text":"<p>The <code>healthcheck</code> directive allows you to specify a command that Docker will run to check the health of your container. If the command returns a non-zero code, Docker will consider the container to be unhealthy.</p> <ul> <li>Docker will run the <code>test</code> command to check the health of your container. In this case, we used the <code>cqlsh</code> command to describe the <code>keyspaces</code> in Cassandra.</li> <li>The <code>interval</code> setting specifies how often Docker should run the healthcheck command.</li> <li>The <code>timeout</code> setting specifies how long Docker should wait for the healthcheck command to complete.</li> <li>The <code>retries</code> setting specifies how many times Docker should run the healthcheck command before giving up and killing the container.</li> </ul> <p>Example:</p> <pre><code>healthcheck:\n  test: [ \"CMD\", \"cqlsh\", \"-e\", \"describe keyspaces\" ]\n  interval: 5s\n  timeout: 5s\n  retries: 60\n</code></pre>"},{"location":"Infrastructure/docker/cheatsheet-dc-configuration/#restart-policy","title":"Restart Policy","text":"<p>This will ensure that your service automatically restarts if it crashes or if the Docker daemon restarts.</p> <pre><code>restart: unless-stopped\nrestart: always\n</code></pre>"},{"location":"Infrastructure/docker/cheatsheet-dc-configuration/#logging","title":"Logging","text":"<p>Docker Compose supports logging configuration which can be useful for debugging and monitoring. You can specify a driver (json-file, syslog, etc.) and options depending on your needs.</p> <pre><code>logging:\n  driver: \"json-file\"\n  options:\n    max-size: \"200k\"\n    max-file: \"10\"\n</code></pre> <p>The logging configuration is set to use the json-file driver and will rotate logs when they reach 200k in size, keeping a maximum of 10 log files.</p>"},{"location":"Infrastructure/docker/cheatsheet-dc-configuration/#resource-limits","title":"Resource Limits","text":"<p>Limit resources like CPU and memory. It's not strictly required, but it's better to have some limits.</p> <pre><code>mem_limit: \"${MEM_LIMIT}\"\ncpus: \"${CPU_LIMIT}\"\n</code></pre>"},{"location":"Infrastructure/docker/cheatsheet-dc-configuration/#networking","title":"Networking","text":"<p><code>Bridge</code> driver is the default network driver for Docker, and it's used when you don't specify a driver. It provides a private network internal to the host so containers on this network can communicate.</p> <pre><code>networks:\n  my_network:\n    driver: bridge\n</code></pre> <p>By default, all containers have networking enabled and they can make any outgoing connections. The operator can completely disable networking with <code>docker run --network none</code> which disables all incoming and outgoing networking. In cases like this, you would perform I/O through files or <code>STDIN</code> and <code>STDOUT</code> only.</p>"},{"location":"Infrastructure/docker/cheatsheet-dc-configuration/#update-strategy","title":"Update Strategy","text":"<p>For services that need to be updated with zero downtime, specify an update strategy.</p> <pre><code>deploy:\n  mode: replicated\n  replicas: 1\n  update_config:\n    parallelism: 1\n    delay: 10s\n</code></pre> <p>The update configuration ensures that updates are carried out one at a time with a delay of 10s between updates.</p>"},{"location":"Infrastructure/docker/cheatsheet-docker-compose/","title":"Docker Compose cheat sheet","text":"<pre><code>$ docker-compose up -d\n$ docker-compose down --remove-orphans\n$ docker-compose down --remove-orphans -v\n\n$ docker-compose logs --tail=\"all\" &lt;container_name&gt;\n$ docker-compose logs -f &lt;container_name&gt;\n\n$ docker-compose exec &lt;db_container_name&gt; psql postgres &lt;user&gt;\n$ docker-compose exec -it &lt;container_name&gt; bash\n\n$ docker prune &lt;something&gt;\n</code></pre>"},{"location":"Infrastructure/docker/cheatsheet-docker/","title":"Docker Cheatsheet","text":"","tags":["docker","cheatsheet"]},{"location":"Infrastructure/docker/cheatsheet-docker/#info","title":"Info","text":"<pre><code># Docker version information\n$ docker --version\n\n# Displays client version and Server version\n$ docker version\n\n# More detailed info\n$ docker info\n\n# Docker Manual\n$ docker --help\n\n# Docker Manual for specific command\n$ docker &lt;command&gt; --help\n</code></pre>","tags":["docker","cheatsheet"]},{"location":"Infrastructure/docker/cheatsheet-docker/#docker-resources","title":"Docker Resources","text":"<pre><code># List all images locally stored with Docker engine\n$ docker images\n\n# List all images locally stored with Docker engine\n$ docker image ls\n\n# List all images (including hidden)\n$ docker image ls -a\n\n# Delete an image from local image store\n$ docker rmi alpine:3.4\n</code></pre>","tags":["docker","cheatsheet"]},{"location":"Infrastructure/docker/cheatsheet-docker/#build","title":"Build","text":"<pre><code># Build an image from Dockerfile in the current directory with a tag\n$ docker build -t myapp:1.0\n\n</code></pre>","tags":["docker","cheatsheet"]},{"location":"Infrastructure/docker/cheatsheet-docker/#build-options","title":"Build Options","text":"<pre><code># Show the output of the run commands that were not loaded from the cache\n# Alternatively use -&gt; export BUILDKIT_PROGRESS=plain\n--progress plain\n\n# Rerun steps that have been cached\n--no-cache\n</code></pre>","tags":["docker","cheatsheet"]},{"location":"Infrastructure/docker/cheatsheet-docker/#ship","title":"Ship","text":"<pre><code># Pull an image from a registry\n$ docker pull alpine:3.4\n\n# Retag a local image with a new image name and tag\n$ docker tag alpine:3.4 myrepo/myalpine:3.4\n\n# Log into a registry (Docker Hub by default)\n$ docker login my.registry.com:8000\n\n# Push an image to a registry\n$ docker push myrepo/myalpine:3.4\n</code></pre>","tags":["docker","cheatsheet"]},{"location":"Infrastructure/docker/cheatsheet-docker/#run","title":"Run","text":"<pre><code># Creates a new container from the image and start it\n$ docker run &lt;image&gt;\n\n# Start a Dockerized web server\n$ docker run -d -p 80:80 --name webserver nginx\n\n# Start an existing container\n$ docker start web\n\n# Stop a running container through SIGTERM\n$ docker stop web\n\n# Stop a running container through SIGKILL\n$ docker kill web\n\n# Kill running containers\n$ docker kill $(docker ps -q)\n\n# List the networks\n$ docker network ls\n\n# Create an overlay network\n$ docker network create --subnet 10.1.0.0/24 --gateway 10.1.0.1 -d overlay mynet\n\n# List the running containers\n$ docker ps\n\n# Delete all running and stopped containers\n$ docker rm -f $(docker ps -aq)\n\n# Create a new bash process inside the container and connect it to the terminal\n$ docker exec -it web bash\n\n# Print the last 100 lines of a container's logs\n$ docker logs --tail 100 web\n</code></pre>","tags":["docker","cheatsheet"]},{"location":"Infrastructure/docker/cheatsheet-docker/#prune-cleanup","title":"Prune / Cleanup","text":"<pre><code># single command that will clean up any resources that are dangling\n$ docker system prune\n    \u2014 images, containers, volumes, and networks\n\n# remove any stopped containers &amp; all unused images (not just dangling images)\n$ docker system prune -a\n</code></pre>","tags":["docker","cheatsheet"]},{"location":"Infrastructure/docker/cheatsheet-docker/#container-commands","title":"Container Commands","text":"<pre><code>DOCKER_HIDE_LEGACY_COMMANDS=true docker --help\n\ndocker image ls\n\n# List all the running containers\ndocker container ls\ndocker container ls -a\n\n# Run container in an interactive mode\ndocker container run -it openjdk\n\n# Run container in a detached mode\ndocker container run -d jboss/wildfly\ndocker container run -d --name web jboss/wildfly\n\n# # This is to override the default command and run your own\ndocker container run -it --name web jboss/wildfly bash\n\n# To let the container pick a random port\ndocker container run -it --name web -P jboss/wildfly\n\n# To let the container pick the port passed in\ndocker container run -it --name web -p 8080:8080 jboss/wildfly\n\n# Mount local directory on container\ndocker container run -it --name web -p 8080:8080\n    -v `pwd`/webapp.war:/opt/jboss/wildfly/standalone/deployments/webapp.war jboss/wildfly\n\ndocker container logs &lt;container_name&gt;\ndocker container logs unruffled_easley\n# Tailing the logs\ndocker container logs unruffled_easley -f\n\n# Stop the container\ndocker container stop &lt;container-id&gt;\ndocker container stop bb655989064b\n# Remove the container\ndocker container rm bb655989064b\n# Stop and remove the container in a single command\ndocker container rm -f bb655989064b\n</code></pre> <p>You need to be able to expose ports and attach volumes, so that it can deploy <code>.war</code> files to it.</p>","tags":["docker","cheatsheet"]},{"location":"Infrastructure/docker/cheatsheet-docker/#network-settings","title":"Network Settings","text":"<pre><code># Set custom dns servers for the container\n--dns=[]\n\n# Connect a container to a network\n--network=\"bridge\"\n    'bridge': create a network stack on the default Docker bridge\n    'none': no networking\n    'container:&lt;name|id&gt;': reuse another containers network stack\n    'host': use the Docker host network stack\n    '&lt;network-name&gt;|&lt;network-id&gt;': connect to a user-defined network\n\n# Add network-scoped alias for the container\n--network-alias=[]\n\n# Add a line to /etc/hosts (host:IP)\n--add-host=\"\"\n\n# Sets the container's Ethernet device's MAC address\n--mac-address=\"\"\n\n# Sets the container's Ethernet device's IPv4 address\n--ip=\"\"\n\n# Sets the container's Ethernet device's IPv6 address\n--ip6=\"\"\n\n# Sets one or more container's Ethernet device's link local IPv4/IPv6 addresses\n--link-local-ip=[]\n</code></pre>","tags":["docker","cheatsheet"]},{"location":"Infrastructure/docker/cheatsheet-docker/#reference","title":"Reference","text":"<ul> <li>Official docker site</li> <li>Official docker command reference</li> <li>Docker cheatsheet | GitHub : wsargent</li> </ul>","tags":["docker","cheatsheet"]},{"location":"Infrastructure/docker/cheatsheet-troubleshooting/","title":"Troubleshooting","text":"<pre><code># stats for all running containers\n$ docker stats\n\n# output a JSON object that describes the configuration of the container\n$ docker inspect $CONTAINER\n\n# Get hash for the image\ndocker inspect --format='{{index .RepoDigests 0}}' $IMAGE\n</code></pre>"},{"location":"Infrastructure/docker/docker-networking/","title":"Docker Networking","text":"<p>To do input/output with a detached container use network connections or shared volumes.</p> <p>By default, all containers have networking enabled and they can make any outgoing connections. The operator can completely disable networking with <code>docker run --network none</code> which disables all incoming and outgoing networking. In cases like this, you would perform I/O through files or <code>STDIN</code> and <code>STDOUT</code> only.</p>"},{"location":"Infrastructure/docker/docker-networking/#network","title":"Network","text":""},{"location":"Infrastructure/docker/docker-networking/#benefits-of-network","title":"Benefits of Network","text":"<p>There are several benefits to creating a network for containers:</p> <ul> <li> <p>Isolation: Containers on the same network can communicate with one another directly, without the overhead introduced by routing traffic through the host. While containers in different networks cannot, unless specifically configured to do so. This can be useful for isolating applications or services from each other for <code>security</code> or <code>performance</code> reasons.</p> </li> <li> <p>Service Discovery: Docker provides automatic service discovery on user-defined networks. This means that a container can use DNS to discover and refer to other containers by their service name in the same network.</p> </li> <li> <p>Custom IP Addressing: User-defined networks also allow you to use custom IP address ranges and subnets. This can be useful if you need to <code>avoid IP address conflicts</code> with other networks or systems. You can also use networks to enable <code>IPv6</code> for your containers.</p> </li> <li> <p>Scalability: When you scale a service without a defined network, Docker will not automatically handle the networking for the additional instances of the service. You would need to manually configure the networking to ensure that the instances can communicate with each other and with other services. You'll need to manually configure networking for the scaled services instead.     <code>sh     $ docker network connect my_network my_web_app_1     $ docker network connect my_network my_web_app_2</code></p> </li> <li>Load Balancing: Docker automatically load balances requests to all instances of a scaled services, using round-robin strategy. You can use networks to distribute traffic across multiple containers.</li> <li>Network Policies: You use networks to enforce network policies for your containers.</li> <li>Service Mesh: You can use networks to create a service mesh for your containers.</li> <li>Observability: You can use networks to monitor and troubleshoot your containers.</li> </ul>"},{"location":"Infrastructure/docker/docker-networking/#driver","title":"Driver","text":"<p><code>Bridge</code> driver is the default network driver for Docker, and it's used when you don't specify a driver. It provides a private network internal to the host so containers on this network can communicate.</p> <p><code>Overlay</code>: used when you need to enable network communication between Docker containers running on different hosts or if you need to create a swarm service spanning multiple nodes in a Docker Swarm. It creates a distributed network among multiple Docker daemon hosts. This network sits on top of (overlays) the host-specific networks, allowing containers connected to it to communicate securely, even across different Docker daemon hosts. This can be very useful in cases where you are running a distributed application across multiple hosts.</p> <p><code>Macvlan</code>: used when you want to assign a MAC address to a container, making it appear as a physical device on your network. The Docker daemon routes traffic to containers by their MAC addresses. Using the macvlan driver is sometimes the best choice when dealing with legacy applications that expect to be directly connected to the physical network, rather than routed through the Docker host\u2019s network stack. Macvlan is ideal for cases where network stack isolation is important, as it removes the need for a bridge between the Docker host and the container by assigning a MAC address to each container's virtual network interface.</p>"},{"location":"Infrastructure/docker/installation/","title":"Installation","text":"<pre><code># Option 1\n$ brew cask install docker\n$ brew cask reinstall docker\n\n# Option 2\n# Download OS specific version and install\nhttps://docker.com/getdocker\n# OS-specific instructions. For production\nhttps://docs.docker.com/engine/installation\n\n# Installation Location: /usr/local/bin\n</code></pre>"},{"location":"Infrastructure/docker/installation/#verify-installation","title":"Verify Installation","text":"<pre><code># Display installed version number\n$ docker --version\n\n# Display more details version information with client and server\n$ docker version\n\n# Display system-wide information\n$ docker info\n\n# Docker Compose version\n$ docker-compose --version\n</code></pre>"},{"location":"Infrastructure/docker/recipes/building-custom-docker-image/","title":"Building Custom Docker Image","text":""},{"location":"Infrastructure/docker/recipes/building-custom-docker-image/#build-image-for-tengine","title":"Build image for Tengine","text":"<p>Tengine doesn't have a readily available docker image</p>"},{"location":"Infrastructure/docker/recipes/building-custom-docker-image/#download-tengine","title":"Download Tengine","text":"<pre><code>https://tengine.taobao.org/download.html\n\n# or Direct download link\nhttps://tengine.taobao.org/download/tengine-2.3.3.tar.gz\n</code></pre>"},{"location":"Infrastructure/docker/recipes/building-custom-docker-image/#base-debian-container","title":"Base Debian Container","text":"<pre><code># Run debian container and get to the bash prompt\n$ docker run -it debian bash\n</code></pre> <p>Curl is not available by default. Install curl</p> <pre><code>/# apt-get update\n/# apt-get install -y curl\n</code></pre> <p>Download tengine file</p> <pre><code>/# curl &lt;file_url&gt; &gt; &lt;download_path&gt;\n/# curl https://tengine.taobao.org/download/tengine-2.3.3.tar.gz &gt; /opt/tengine-2.3.3.tar.gz\n</code></pre> <p>Open zip file</p> <pre><code>/# cd /opt\n/opt# tar xzf tengine-2.3.3.tar.gz\n/opt# cd tengine-2.3.3\n</code></pre> <p>Before installing tengine, it'll require C to be installed It'll also require PCRE and OpenSSL libraries</p> <pre><code>/# apt-get install -y gcc\n/# apt-get install -y libpcre3-dev\n/# apt-get install -y libssl-dev\n\n/# apt-get install -y make\n</code></pre> <p>Install tengine</p> <pre><code>/opt/tengine-2.3.3# ./configure\n/opt/tengine-2.3.3# make\n/opt/tengine-2.3.3# make install\n\n# by default, it will be installed to /usr/local/nginx\n</code></pre> <p>Start the server</p> <pre><code># Check if the server is running\n/# ps aux\n\n# If not, go to the installation location, or run the executable below\n/# /usr/local/nginx/sbin/nginx\n\n# Check if the server is running\n/# ps aux\n\n# You should see a master and worker process\n</code></pre>"},{"location":"Infrastructure/docker/recipes/building-custom-docker-image/#building-dockerfile","title":"Building Dockerfile","text":"<p>Based on the steps performed above</p> <pre><code>FROM debian\nRUN apt-get udpate &amp;&amp; apt-get install -y \\\n    curl \\\n    gcc \\\n    libpcre3-dev \\\n    libssl-dev \\\n    make\n\nRUN curl https://tengine.taobao.org/download/tengine-2.3.3.tar.gz &gt; /opt/tengine-2.3.3.tar.gz\n\nWORKDIR /opt\n\nRUN tar xzf tengine-2.3.3.tar.gz\n\nWORKDIR /opt/tengine-2.3.3\n\nRUN ./configure\nRUN make\nRUN make install\n\n# forward requests and error logs to docker log collector\n# Tengine log paths are different than standard nginx paths\n# RUN ln -sf /dev/stdout /var/log/nginx/access.log \\\n#     &amp;&amp; ln -sf /dev/stderr /var/log/nginx/error.log\nRUN ln -sf /dev/stdout /usr/local/nginx/logs/access.log \\\n    &amp;&amp; ln -sf /dev/stderr /usr/local/nginx/logs/error.log\n\nEXPOSE 80 443\n\n# Found from official nginx image on docker hub\nCMD [\"/usr/local/nginx/sbin/nginx\", \"-g\", \"daemon off;\"]\n</code></pre>"},{"location":"Infrastructure/docker/recipes/building-custom-docker-image/#building-tengine-image-container","title":"Building Tengine Image &amp; Container","text":"<pre><code>$ docker build -t custom/tengine:2.3.3 .\n$ docker images\n$ docker run -p 8000:80 custom/tengine:2.3.3\n</code></pre>"},{"location":"Infrastructure/docker/recipes/nginx-with-volume/","title":"Nginx with Persistent Volume","text":""},{"location":"Infrastructure/docker/recipes/nginx-with-volume/#spinning-up-nginx-without-volume","title":"Spinning up nginx without volume","text":"<pre><code>$ docker run -p 8080:80 --name web --rm nginx\n\nGo to http://localhost:8080 to view the default page.\nOnce you exit the container, it'll be deleted because of --rm flag\n</code></pre>"},{"location":"Infrastructure/docker/recipes/nginx-with-volume/#modify-the-default-file","title":"Modify the default file","text":"<pre><code>$ vi index.html\n\n# web because that's the container name\n$ docker cp index.html web:/usr/share/nginx/html\n</code></pre>"},{"location":"Infrastructure/docker/recipes/nginx-with-volume/#persisting-your-changes","title":"Persisting your changes","text":"<p>From the last example, if we stop and start the container again, our <code>index.html</code> changes will be lost since they are not persistent. This can be solved by attaching a volume to it. Run the below command instead:</p> <pre><code>$ docker run -p 8080:80 --name web --rm -v web:/usr/share/nginx/html nginx\n\n# Here the name web after option -v is the name of the volume.\n# Path following colon(:) is the directory we want to be persistent.\n\n# Now copy the index.html\n</code></pre>"},{"location":"Infrastructure/docker/recipes/nginx-with-volume/#persisting-from-host-machine","title":"Persisting from Host Machine","text":"<p>Very similar to the last option. Instead of specifying the name of the volume, we specify the directory on host.</p> <pre><code>$ docker run -p 8080:80 --name web --rm -v $PWD:/usr/share/nginx/html nginx\n\n# Notice that the volume name \"web\" is replaced by whatever PWD (current directory) is.\n# Also note that the container does not need to be restarted for updates to show.\n</code></pre>"},{"location":"Infrastructure/docker/recipes/nodejs-web-server-with-proxy/","title":"Node.js Web Server with Docker","text":""},{"location":"Infrastructure/docker/recipes/nodejs-web-server-with-proxy/#nodejs-app","title":"Node.js App","text":"<p>Prerequisite: have a <code>index.js</code> ready listening on a port ready.</p>"},{"location":"Infrastructure/docker/recipes/nodejs-web-server-with-proxy/#dockerfile","title":"Dockerfile","text":"<p><code>Dockerfile</code> is just a text file that provides docker engine with instructions on how to build the image.</p> <pre><code>FROM mhart/alpine-node\nCOPY index.js .\nEXPOSE 8000\nCMD node index.js\n</code></pre>"},{"location":"Infrastructure/docker/recipes/nodejs-web-server-with-proxy/#building-the-image","title":"Building the image","text":"<pre><code>$ docker build -t myserver .\n\n# -t specifies the name of the docker image\n</code></pre>"},{"location":"Infrastructure/docker/recipes/nodejs-web-server-with-proxy/#running-the-container","title":"Running the container","text":"<pre><code>$ docker run -p 8001:8000 myserver\n\n# Verify the server is running + port forwarding is setup correctly through firewal\n$ curl localhost:8001\n</code></pre>"},{"location":"Infrastructure/docker/recipes/nodejs-web-server-with-proxy/#nginx-proxy-container","title":"Nginx Proxy Container","text":""},{"location":"Infrastructure/docker/recipes/nodejs-web-server-with-proxy/#running-nginx-container","title":"Running Nginx Container","text":"<pre><code># By default nginx runs on port 80\n$ docker run --rm -p 6000:80 nginx\n\n# Verify the server is running + port forwarding is setup correctly through firewal\n$ curl localhost:6000\n</code></pre>"},{"location":"Infrastructure/docker/recipes/nodejs-web-server-with-proxy/#setting-up-nginx-as-proxy","title":"Setting up Nginx as Proxy","text":"<p>The intent is to update nginx configurations to proxy web requests into our node.js app</p> <p>default.conf</p> <pre><code># This file will replace default nginx configuration file.\n# We have bare-minimum directives in the file.\n# Nginx will use default if a directive is not defined.\n\nserver {\n    location / {\n        proxy_set_header HOST $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_pass http://app:8000;\n    }\n}\n</code></pre> <p>Dockerfile for Nginx image</p> <pre><code>FROM nginx\ncopy default.conf /etc/nginx/conf.d/\n</code></pre> <p>Building the image</p> <pre><code>$ docker built -t foo/nginx .\n</code></pre> <p>Run the container</p> <pre><code>$ docker run -p 6000:80 --link node-app:app --name nginx-proxy foo/nginx\n\nUsing link may not be the best way now. TBD.\n</code></pre> <p>Verify</p> <pre><code>curl http://localhost:6000\n\nThis should direct to node.js server response instead of default nginx index html response\n</code></pre>"},{"location":"Infrastructure/docker/tips/handling-cleanup/","title":"Docker Clean up","text":"<pre><code># Remove all containers that are not currently running\n$ docker container prune\n\n# Remove all images that are not attached to any container\n$ docker image prune\n\n# Remove all images that are not attached to any container\n$ docker image prune\n\n# Remove all stopped containers, volumes, networks and dangling images\n$ docker system prune\n\n# Remove all stopped containers, volumes, networks and unused images\n$ docker system prune -a\n</code></pre>"},{"location":"Infrastructure/docker/tips/handling-docker-containers/","title":"Handling Docker Containers","text":""},{"location":"Infrastructure/docker/tips/handling-docker-containers/#docker-run","title":"Docker run","text":"<p>Run a docker container (creates a container)</p> <pre><code>$ docker run &lt;image_name&gt;\n$ docker run mongo\n\n$ docker run -d &lt;image_name&gt;        # Run as a daemon in background\n\n# Note: If the image has not been downloaded already locally, it will be downloaded. If the image exists locally, it'll be pulled from downloaded images cache\n</code></pre> <p>Assign a container name</p> <pre><code>$ docker run -d --name some_name &lt;image_name&gt;\n\n# Note: docker assigns random names to each container. Use the above command to give custom names to containers\n\n# Rename a container\n$ docker rename old_name custom_name\n</code></pre> <p>Assign a port to a container</p> <pre><code>$ docker run -d -p 8080:8080 &lt;image_name&gt;\n\n# Note: docker assigns random names to each container. Use the above command to give custom names to containers\n</code></pre> <p>Automatically remove container on exit</p> <pre><code>$ docker run --rm &lt;image_name&gt;\n$ docker run -p 8080:80 --name web --rm nginx\n\n# Note: docker assigns random names to each container. Use the above command to give custom names to containers\n</code></pre>"},{"location":"Infrastructure/docker/tips/handling-docker-containers/#other-container-commands","title":"Other Container commands","text":"<p>Start a existing docker container</p> <pre><code>$ docker start &lt;container_id&gt;\n$ docker start &lt;container_name&gt;\n\n$ docker start -d &lt;image_name&gt;      # Start as a daemon in background\n\n# Note: Only starts a stopped/exited docker container\n</code></pre> <p>Stop a running containers</p> <pre><code>$ docker stop &lt;container_id&gt;\n$ docker stop &lt;container_name&gt;\n</code></pre> <p>List current running containers</p> <pre><code>$ docker ps\n\n# List including stopped/exited status\n$ docker ps -a\n</code></pre> <p>Remove a container</p> <pre><code>$ docker rm &lt;container_id&gt;\n$ docker rm &lt;container_name&gt;\n\n# Note: containers need to be stopped in order for them to be removed\n</code></pre>"},{"location":"Infrastructure/docker/tips/handling-docker-images/","title":"Handling Docker Images","text":"<p>Downloading a docker image</p> <pre><code>$ docker pull &lt;image_name&gt;\n$ docker pull mongo\n$ docker pull mongo:6.0.2\n$ docker pull mongo:latest\n\n# Note: If no tag is provided, docker pull the image with latest tag\n</code></pre> <p>List docker images downloaded to our machine</p> <pre><code>$ docker images\n</code></pre> <p>Remove a docker image</p> <pre><code>$ docker rmi &lt;image_name&gt;\n$ docker rmi mongo\n$ docker rmi mongo:6.0.2\n\n$ docker rmi &lt;image_id&gt;\n$ docker rmi d05826c43c40\n\n# Remove multiple images (use image name or image id)\n$ docker rmi mongo postgres redis\n</code></pre> <p>Downloading a docker image</p> <pre><code></code></pre>"},{"location":"Infrastructure/docker/tips/handling-docker-images/#references","title":"References","text":"<ol> <li>Docker Hub</li> </ol>"},{"location":"Infrastructure/docker/tips/handling-persistent-storage/","title":"Persistent Storage","text":"<p>Container based file systems in docker are by default ephemeral. Volumes allow us to mount persistent disks to one or many containers.</p>"},{"location":"Infrastructure/docker/tips/handling-persistent-storage/#nginx-example","title":"Nginx example","text":"<pre><code>$ docker run -d -p 8080:8080 -v volume_name:path_in_container &lt;image_name&gt;\n$ docker run -p 8080:80 --name web --rm -v web:/usr/share/nginx/html nginx\n</code></pre> <p>To persist the volume on the host machine</p> <pre><code>$ docker run -d -p 8080:8080 -v host_path:path_in_container &lt;image_name&gt;\n$ docker run -p 8080:80 --name web --rm -v $PWD:/usr/share/nginx/html nginx\n</code></pre>"},{"location":"Infrastructure/docker/tips/handling-persistent-storage/#mounting-manually","title":"Mounting manually","text":"<p>Creating a volume</p> <pre><code>$ docker volume create --name &lt;volume_name&gt;\n$ docker volume create --name webdata\n\n# Note: This volume can be later referenced by any container. Name helps finding it easily.\n</code></pre> <p>Attaching volume</p> <pre><code># Run a daemon nginx container with webdata mounted\n$ docker run -d --name web -v webdata:/usr/share/nginx/html -p 8000:80 nginx\n\n# Check contents of default html\n$ curl localhost:8000\n</code></pre> <p>Modify contents via docker</p> <pre><code># By directly entering the container\n$ docker exec web bash -c 'echo \"foo\" &gt; /usr/share/nginx/html/index.html'\n\n# Check contents of modified html\n$ curl localhost:8000\n</code></pre> <p>Verify data persistence</p> <pre><code># Stop and remove the web docker container\n# Run a new container web1 using the same command\n\n# Check contents of modified html\n$ curl localhost:8000\n\n# The changes made earlier should persist\n</code></pre> <p>Mounting the same volume to another container</p> <pre><code># Run a new container web2 using the same command, except the port to be 8001\n\n# Check contents of modified html\n$ curl localhost:8001\n\n# The changes made with container web1 should reflect from container web2 as well.\n</code></pre>"},{"location":"Infrastructure/docker/tips/handling-persistent-storage/#inspecting-the-volume","title":"Inspecting the volume","text":"<pre><code>$ docker inspect -f '{{ .Mounts }}' &lt;container_name&gt;\n$ docker inspect -f '{{ .Mounts }}' web1\n\n# -f flag is used to specify a format to filter results\n\n# To find more information about the volume\n$ docker volume inspect &lt;volume_name&gt;\n$ docker volume inspect webdata\n</code></pre>"},{"location":"Infrastructure/docker/tips/handling-persistent-storage/#other-volume-operations","title":"Other volume operations","text":"<pre><code># List all volumes\n$ docker volume ls\n\n# To remove a volume, all containers (where it is mounted on) must be stopped and removed\n$ docker volume rm &lt;volume_name&gt;\n$ docker volume rm webdata\n</code></pre>"},{"location":"Infrastructure/kubectl/","title":"kubectl","text":""},{"location":"Infrastructure/kubectl/#index","title":"Index","text":"<ol> <li>installation</li> <li>kubeconfig configuration</li> <li>kubeconfig commands</li> <li>kubeconfig Plugins/Extensions</li> </ol>"},{"location":"Infrastructure/kubectl/#topics","title":"Topics","text":"<ul> <li>namespaces</li> <li>containers</li> <li>pods</li> <li>secrets</li> <li>config maps</li> <li>services</li> <li>deployments</li> <li>all resources</li> <li>job</li> <li>cronjob</li> </ul>"},{"location":"Infrastructure/kubectl/#references","title":"References","text":"<ul> <li>Official Cheatsheet</li> <li>Configure Default Memory Requests and Limits</li> <li>Namespaces</li> </ul>"},{"location":"Infrastructure/kubectl/commands/","title":"kubectl Commands","text":""},{"location":"Infrastructure/kubectl/commands/#defaults","title":"Defaults","text":"<pre><code>$ kubectl config get-contexts\n$ kubectl config use-context my-cluster-name\n\n# permanently save the namespace for all subsequent kubectl commands in that context\n$ kubectl config set-context --current --namespace=ggckad-s2\n</code></pre>"},{"location":"Infrastructure/kubectl/commands/#creating-objects","title":"Creating Objects","text":"<p><code>kubectl apply</code> creates and updates resources in a cluster.</p> <pre><code># create resource(s)\n$ kubectl apply -f ./my-manifest.yaml\n\n# create resource(s) in all manifest files in dir\n$ kubectl apply -f ./dir\n\n# create resource(s) from url\n$ kubectl apply -f https://git.io/vPieo\n\n# start a single instance of nginx\n$ kubectl create deployment nginx --image=nginx\n\n# create a Job which prints \"Hello World\"\n$ kubectl create job hello --image=busybox:1.28 -- echo \"Hello World\"\n\n# create a CronJob that prints \"Hello World\" every minute\n$ kubectl create cronjob hello --image=busybox:1.28   --schedule=\"*/1 * * * *\" -- echo \"Hello World\"\n</code></pre>"},{"location":"Infrastructure/kubectl/commands/#viewing-finding-resources","title":"Viewing / Finding Resources","text":"<pre><code># get the documentation for pod manifests\n$ kubectl explain pods\n</code></pre> <p>Get commands with basic output</p> <pre><code>$ kubectl get services                          # List all services in the namespace\n$ kubectl get deployment my-dep                 # List a particular deployment\n\n$ kubectl get pods                              # List all pods in the namespace\n$ kubectl get pod my-pod -o yaml                # Get a pod's YAML\n$ kubectl get pods --all-namespaces             # List all pods in all namespaces\n$ kubectl get pods -o wide                      # List all pods in the current namespace, with more details\n</code></pre> <p>Describe commands with verbose output</p> <pre><code>$ kubectl describe nodes my-node\n$ kubectl describe pods my-pod\n</code></pre> <p>General use commands</p> <pre><code># Check which nodes are ready\nJSONPATH='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}' \\\n &amp;&amp; kubectl get nodes -o jsonpath=\"$JSONPATH\" | grep \"Ready=True\"\n\n# Output decoded secrets without external tools\nkubectl get secret my-secret -o go-template='{{range $k,$v := .data}}{{\"### \"}}{{$k}}{{\"\\n\"}}{{$v|base64decode}}{{\"\\n\\n\"}}{{end}}'\n\n# List Events sorted by timestamp\nkubectl get events --sort-by=.metadata.creationTimestamp\n\n# Compares the current state of the cluster against the state that the cluster would be in if the manifest was applied.\nkubectl diff -f ./my-manifest.yaml\n</code></pre>"},{"location":"Infrastructure/kubectl/commands/#updating-resources","title":"Updating Resources","text":""},{"location":"Infrastructure/kubectl/commands/#patching-resources","title":"Patching Resources","text":""},{"location":"Infrastructure/kubectl/commands/#editing-resources","title":"Editing Resources","text":""},{"location":"Infrastructure/kubectl/commands/#scaling-resources","title":"Scaling Resources","text":""},{"location":"Infrastructure/kubectl/commands/#deleting-resources","title":"Deleting Resources","text":""},{"location":"Infrastructure/kubectl/commands/#interacting-with-running-pods","title":"Interacting with running Pods","text":"<pre><code>kubectl logs my-pod                                 # dump pod logs (stdout)\nkubectl logs -f my-pod                              # stream pod logs (stdout)\nkubectl logs -f my-pod -c my-container              # stream pod container logs (stdout, multi-container case)\n\nkubectl run -i --tty busybox --image=busybox:1.28 -- sh  # Run pod as interactive shell\nkubectl run nginx --image=nginx -n mynamespace      # Start a single instance of nginx pod in the namespace of mynamespace\nkubectl run nginx --image=nginx --dry-run=client -o yaml &gt; pod.yaml\n                                                    # Generate spec for running pod nginx and write it into a file called pod.yaml\n\nkubectl port-forward my-pod 5000:6000               # Listen on port 5000 on the local machine and forward to port 6000 on my-pod\nkubectl exec my-pod -- ls /                         # Run command in existing pod (1 container case)\nkubectl exec --stdin --tty my-pod -- /bin/sh        # Interactive shell access to a running pod (1 container case)\nkubectl exec my-pod -c my-container -- ls /         # Run command in existing pod (multi-container case)\nkubectl top pod POD_NAME --containers               # Show metrics for a given pod and its containers\nkubectl top pod POD_NAME --sort-by=cpu              # Show metrics for a given pod and sort it by 'cpu' or 'memory'\n</code></pre>"},{"location":"Infrastructure/kubectl/commands/#resources","title":"Resources","text":"<ul> <li>Official docs</li> <li>Official cheatsheet</li> <li>kubectl docs</li> </ul>"},{"location":"Infrastructure/kubectl/configuration/","title":"Configuring kubeconfig","text":"<p>In order for kubectl to find and access a Kubernetes cluster, it needs a kubeconfig file.</p> <p>Check that kubectl is properly configured by getting the cluster state:</p> <pre><code># cluster information in the current context\n$ kubectl cluster-info\n$ kubectl config current-context\n\n# URL response means kubectl is correctly configured to access your cluster.\n</code></pre> <p>If you see the connection refused to the cluster, then either kubectl is not configured correctly or docker is not up.</p> <p>To check whether it is configured properly, use:</p> <pre><code>$ kubectl cluster-info dump\n</code></pre>"},{"location":"Infrastructure/kubectl/configuration/#basic-structure","title":"Basic Structure","text":"<p>A configuration file describes clusters, users, and contexts. By default, kubectl configuration is located at <code>~/.kube/config</code>.</p> <pre><code>apiVersion: v1\nkind: Config\npreferences: {}\n\nclusters:\n- cluster:\n  name: development\n- cluster:\n  name: scratch\n\nusers:\n- name: developer\n- name: experimenter\n\ncontexts:\n- context:\n  name: dev-frontend\n- context:\n  name: dev-storage\n- context:\n  name: exp-scratch\n</code></pre>"},{"location":"Infrastructure/kubectl/configuration/#define-clusters-users-and-contexts","title":"Define Clusters, Users, and Contexts","text":"<p>Add cluster details</p> <pre><code>$ kubectl config --kubeconfig=config-demo set-cluster development --server=https://1.2.3.4 --certificate-authority=fake-ca-file\n\n$ kubectl config --kubeconfig=config-demo set-cluster scratch --server=https://5.6.7.8 --insecure-skip-tls-verify\n\n</code></pre> <p>Add user details</p> <pre><code>$ kubectl config --kubeconfig=config-demo set-credentials developer --client-certificate=fake-cert-file --client-key=fake-key-seefile\n\n$ kubectl config --kubeconfig=config-demo set-credentials experimenter --username=exp --password=some-password\n</code></pre> <p>Add context details</p> <pre><code>$ kubectl config --kubeconfig=config-demo set-context dev-frontend --cluster=development --namespace=frontend --user=developer\n\n$ kubectl config --kubeconfig=config-demo set-context dev-storage --cluster=development --namespace=storage --user=developer\n\n$ kubectl config --kubeconfig=config-demo set-context exp-scratch --cluster=scratch --namespace=default --user=experimenter\n</code></pre> <p>Add Credentials</p> <pre><code># add a new user to your kubeconf that supports basic auth\n$ kubectl config set-credentials kubeuser/foo.kubernetes.com --username=kubeuser --password=kubepassword\n</code></pre>"},{"location":"Infrastructure/kubectl/configuration/#view-configuration","title":"View Configuration","text":"<pre><code>$ kubectl config view\n$ kubectl config get-clusters\n$ kubectl config get-contexts\n$ kubectl config get-users\n\nChange current context\n$ kubectl config use-context dev-frontend\n\nView configs only for the current context\n$ kubectl config view --minify\n</code></pre>"},{"location":"Infrastructure/kubectl/configuration/#remove-configuration","title":"Remove Configuration","text":"<pre><code>$ kubectl --kubeconfig=config-demo config unset clusters.&lt;name&gt;\n$ kubectl --kubeconfig=config-demo config unset contexts.&lt;name&gt;\n$ kubectl --kubeconfig=config-demo config unset users.&lt;name&gt;\n</code></pre>"},{"location":"Infrastructure/kubectl/configuration/#references","title":"References","text":"<ul> <li>Configure access multiple clusters</li> </ul>"},{"location":"Infrastructure/kubectl/installation-helm/","title":"k8s Installation","text":""},{"location":"Infrastructure/kubectl/installation-helm/#installation","title":"Installation","text":"<pre><code>brew install helm\n</code></pre>"},{"location":"Infrastructure/kubectl/installation-helm/#install-plugin","title":"Install plugin","text":"<pre><code># Install unittest plugin\nhelm plugin install https://github.com/quintush/helm-unittest\n\n# List installed plugins\nhelm plugin list\n</code></pre>"},{"location":"Infrastructure/kubectl/installation-helm/#working-with-helm-template","title":"Working with Helm template","text":"<pre><code># Render helm template locally\nhelm template &lt;project&gt;\n\n# Create a zip file of the helm chart\nhelm package\n\n# Dry run\nhelm install --generate-name --dry-run --debug --values ./test-values.yaml &lt;zip_file_name&gt;.tgz\n</code></pre>"},{"location":"Infrastructure/kubectl/installation-helm/#unit-test","title":"Unit test","text":"<pre><code># Run unit tests\nhelm unittest &lt;project&gt;\n\n# Override value during test run\n# TODO: This doesn't work yet\nhelm unittest &lt;project&gt;&gt; --set helmTests.enabled=true\n\n# Run debug mode\nhelm unittest &lt;project&gt; --debug\n</code></pre>"},{"location":"Infrastructure/kubectl/installation-helm/#install-linter","title":"Install linter","text":"<pre><code># Install linter\nbrew install yamllint\n\n# Verify version\nyamllint --version\n\n# Lint a file\nyamllint &lt;project&gt;/templates/tests/&lt;test_file&gt;.yaml\n</code></pre>"},{"location":"Infrastructure/kubectl/installation-helm/#references","title":"References","text":"<ul> <li>https://labs.play-with-k8s.com/</li> <li>https://www.katacoda.com/courses/kubernetes/playground</li> <li>https://www.katacoda.com/</li> </ul>"},{"location":"Infrastructure/kubectl/installation-k8s/","title":"k8s Installation","text":""},{"location":"Infrastructure/kubectl/installation-k8s/#installation","title":"Installation","text":"<pre><code># Install k8s cli\n$ brew install kubernetes-cli\n\n# Verification\n$ kubectl version\n</code></pre>"},{"location":"Infrastructure/kubectl/installation-k8s/#minikube","title":"Minikube","text":"<pre><code>$ brew cask install minikube\n$ minikube version\n$ minikube start\n</code></pre>"},{"location":"Infrastructure/kubectl/installation-k8s/#references","title":"References","text":"<ul> <li>https://labs.play-with-k8s.com/</li> <li>https://www.katacoda.com/courses/kubernetes/playground</li> <li>https://www.katacoda.com/</li> </ul>"},{"location":"Infrastructure/kubectl/installation-kubectl/","title":"Install kubectl","text":"<pre><code>$ brew install kubectl\n</code></pre> <p>Or follow this document</p> <p>Verify installation</p> <pre><code>$ kubectl version\n$ kubectl version --client\n$ kubectl version --client --output=yaml\n</code></pre>"},{"location":"Infrastructure/kubectl/plugins-extensions/","title":"Plugins and Extensions","text":""},{"location":"Infrastructure/kubectl/plugins-extensions/#tools","title":"Tools","text":"<ul> <li>Cluster</li> <li><code>kind</code> -- a tool for running local Kubernetes clusters using Docker container \u201cnodes\u201d. kind was primarily designed for testing Kubernetes itself, but may be used for local development or CI.</li> <li><code>minikube</code> -- a tool that lets you run Kubernetes locally. minikube runs a single-node Kubernetes cluster on your personal computer so that you can try out Kubernetes, or for daily development work.</li> <li><code>kubeadm</code> -- a tool to create and manage Kubernetes clusters. It performs the actions necessary to get a minimum viable, secure cluster up and running in a user friendly way.</li> </ul>"},{"location":"Infrastructure/kubectl/plugins-extensions/#plugins","title":"Plugins","text":"<ul> <li>AutoCompletion -- kubectl provides autocompletion support for Bash, Zsh, Fish, and PowerShell which can save you a lot of typing. (refer this for instructions)</li> <li><code>kubectl-convert</code> -- allows you to convert manifests between different API versions. This can be particularly helpful to migrate manifests to a non-deprecated api version with newer Kubernetes release.</li> </ul>"},{"location":"Infrastructure/kubectl/plugins-extensions/#components","title":"Components","text":"<ul> <li><code>kubelet</code> -- the component that runs on all of the machines in your cluster and does things like starting pods and containers.</li> </ul>"},{"location":"Infrastructure/kubectl/plugins-extensions/#references","title":"References","text":"<ul> <li>kind</li> <li>minikube</li> <li>kubeadm</li> </ul>"},{"location":"Infrastructure/kustomize/","title":"kustomize","text":"<p>Kustomize introduces a template-free way to customize application configuration that simplifies the use of off-the-shelf applications. Now, built into <code>kubectl</code> as <code>apply -k</code>.</p>"},{"location":"Infrastructure/kustomize/#installation","title":"Installation","text":"<pre><code>$ brew install kustomize\n</code></pre>"},{"location":"Infrastructure/kustomize/#common-commands-using-standalone","title":"Common Commands (using standalone)","text":"<pre><code># To view Resources found in a directory containing a kustomization file:\n$ kustomize build &lt;kustomization_directory&gt;\n$ kustomize build &lt;kustomization_directory&gt; --enable-help\n</code></pre>"},{"location":"Infrastructure/kustomize/#common-commands-using-with-kubectl","title":"Common Commands (using with kubectl)","text":"<pre><code># To view Resources found in a directory containing a kustomization file:\n$ kubectl kustomize &lt;kustomization_directory&gt;\n</code></pre> <pre><code># To apply those Resources\n$ kubectl apply -k &lt;kustomization_directory&gt;\n</code></pre>"},{"location":"Infrastructure/kustomize/#references","title":"References","text":"<ul> <li>Official site</li> <li>Command reference</li> <li>Using with kubectl</li> </ul>"},{"location":"Infrastructure/terraform/","title":"Terraform","text":"<p>Terraform code is written in the <code>HashiCorp Configuration Language</code> (HCL) in files with the extension <code>.tf</code>. It is a declarative language, so your goal is to describe the infrastructure you want, and Terraform will figure out how to create it.</p>"},{"location":"Infrastructure/terraform/#usage","title":"Usage","text":""},{"location":"Infrastructure/terraform/#installation","title":"Installation","text":"<pre><code># Install Terraform\n$ brew tap hashicorp/tap\n$ brew install hashicorp/tap/terraform\n\n# Verify Installation\n$ terraform -help\n$ terraform -version\n</code></pre>"},{"location":"Infrastructure/terraform/#version-manager","title":"Version Manager","text":"<p><code>tfenv</code>, a Terraform version manager, to install a specific version of Terraform. </p> <pre><code># Install tfenv\n$ brew install tfenv\n\n# Install Terraform 1.4.6 using tfenv:\n$ tfenv install 1.4.6\n\n# Set Terraform 1.4.6 as the global version:\n$ tfenv use 1.4.6\n</code></pre>"},{"location":"Infrastructure/terraform/#plugin","title":"Plugin","text":"<pre><code># Enable tab completion\n$ terraform -install-autocomplete\n\n# This will update `.bashrc` or `.zshrc` depending on the shell you are in.\n# Once the autocomplete support is installed, you will need to restart your shell.\n</code></pre>"},{"location":"Infrastructure/terraform/#initialize","title":"Initialize","text":"<pre><code># Initialize the project, which downloads a plugin called a provider that lets Terraform interact with Docker.\n$ terraform init\n</code></pre> <p>Provision the <code>NGINX</code> server container with apply. When Terraform asks you to confirm type <code>yes</code> and press <code>ENTER</code>.</p> <pre><code>$ terraform apply\n</code></pre> <p>Verify the existence of the NGINX container by visiting localhost:8000</p> <pre><code># To stop the container\n$ terraform destroy\n</code></pre>"},{"location":"Infrastructure/terraform/#connect-with-aws","title":"Connect with AWS","text":"<p>For Terraform to be able to make changes in your AWS account, you will need to set the AWS credentials for the IAM user you created earlier as the environment variables <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>.</p> <pre><code>$ export AWS_ACCESS_KEY_ID=(your access key id)\n$ export AWS_SECRET_ACCESS_KEY=(your secret access key)\n</code></pre> <p>Note that these environment variables apply only to the current shell.</p> <p>In addition to environment variables, Terraform supports the same authentication mechanisms as all AWS CLI and SDK tools. Therefore, it\u2019ll also be able to use credentials in <code>$HOME/.aws/credentials</code>, which are automatically generated if you run <code>aws configure</code>, or IAM roles, which you can add to almost any resource in AWS.</p>"},{"location":"Infrastructure/terraform/#getting-started","title":"Getting started","text":"<p>Create <code>main.tf</code> in a folder with basic configuration. Go into the folder where you created main.tf and run the <code>terraform init</code> command, which is needed only once, but is also idempotent. This tells Terraform to scan the code, figure out which providers you\u2019re using, and download the code for them. By default, the provider code will be downloaded into a <code>.terraform</code> folder, which is Terraform\u2019s scratch directory (you may want to add it to <code>.gitignore</code>). Terraform will also record information about the provider code it downloaded into a <code>.terraform.lock.hcl</code> file.</p> <p>Next, run <code>terraform plan</code> command to see what Terraform will do before actually making any changes. This is a great way to sanity-check your code before unleashing it onto the world.</p> <p>Run <code>terraform apply</code> to actually create the Instance.</p> <p>Files to commit: 1. All <code>*.tf</code> files 2. <code>.terraform.lock.hcl</code> file</p> <p>Files to ignore 1. <code>.terraform/</code> - temp sratch directory 2. <code>*.tfstate</code> - files for managing state 3. <code>*.tfstate.backup</code></p>"},{"location":"Infrastructure/terraform/#references","title":"References","text":"<ul> <li>Terraform Official documentation</li> <li>Terraform home page</li> <li>AWS<ul> <li>A Comprehensive Guide to Authenticating to AWS on the Command Line | Medium blog</li> </ul> </li> <li>GitHub Codespaces<ul> <li>Run Terraform within GitHub Codespaces</li> <li>Set up a dev container for Terraform in GitHub Codespaces</li> </ul> </li> </ul>"},{"location":"Infrastructure/terraform/about/","title":"Terraform","text":"<p>Terraform is an open source tool created by HashiCorp written in the Go programming language, that allows you to define your infrastructure as code using a simple, declarative language and to deploy and manage that infrastructure across a variety of public cloud providers using a few commands.</p>"},{"location":"Infrastructure/terraform/about/#iaac","title":"IaaC","text":"<p>There are four core values in the DevOps movement: culture, automation, measurement, and sharing.</p> <p>There are five broad categories of IaC tools:</p> <ul> <li>Ad hoc scripts - whatever task you were doing manually, break it down into discrete steps, use your favorite scripting language (e.g., <code>Bash</code>, <code>Ruby</code>, <code>Python</code>) to define each of those steps in code, and execute that script on your server.</li> <li>Configuration management tools - <code>Chef</code>, <code>Puppet</code>, and <code>Ansible</code> are all configuration management tools, which means that they are designed to install and manage software on existing servers. The code looks similar to the Bash script, but using a tool like Ansible offers a number of advantages.</li> <li>Server templating tools - An alternative to configuration management, tools such as <code>Docker</code>, <code>Packer</code>, and <code>Vagrant</code>. The idea behind server templating tools is to create an image of a server that captures a fully self-contained \"snapshot\", and launching a bunch of servers and configuring them by running the same code.<ul> <li>A virtual machine (VM) emulates an entire computer system, including the hardware. You run a hypervisor, such as VMware, VirtualBox, or Parallels, to virtualize (i.e., simulate) the underlying CPU, memory, hard drive, and networking. You can define VM images as code using tools such as Packer and Vagrant.</li> <li>A container emulates the user space of an OS.2 You run a container engine, such as Docker, CoreOS rkt, or cri-o, to create isolated processes, memory, mount points, and networking. You can define container images as code using tools such as Docker and CoreOS rkt.</li> </ul> </li> <li>Orchestration tools - To manage VMs and containers. Handling these tasks is the realm of orchestration tools such as <code>Kubernetes</code>, <code>Marathon/Mesos</code>, Amazon Elastic Container Service (<code>Amazon ECS</code>), <code>Docker Swarm</code>, and <code>Nomad</code>.</li> <li>Provisioning tools - provisioning tools such as <code>Terraform</code>, <code>CloudFormation</code>, <code>OpenStack Heat</code>, and <code>Pulumi</code> are responsible for creating the servers themselves.</li> </ul>"},{"location":"Infrastructure/terraform/about/#simplest-script","title":"Simplest script","text":"<p>This snippet instructs Terraform to make API calls to AWS to deploy a server.</p> <pre><code>provider \"aws\" {\n  region = \"us-east-2\"\n}\n\nresource \"aws_instance\" \"example\" {\n  ami           = \"ami-0fb653ca2d3203ac1\"\n  instance_type = \"t2.micro\"\n}\n</code></pre> <pre><code># To deploy it, just run the follwing\n$ terraform init\n\n# To preview what changes would terraform make\n$ terraform plan\n\n# To deploy the infrastructure\n$ terraform apply\n</code></pre>"},{"location":"Infrastructure/terraform/about/#questions","title":"Questions","text":""},{"location":"Infrastructure/terraform/about/#transparent-portability","title":"Transparent Portability","text":"<p>Whether Terraform supports transparent portability between different cloud providers?</p> <p>The reality is that you can\u2019t deploy \u201cexactly the same infrastructure\u201d in a different cloud provider because the cloud providers don\u2019t offer the same types of infrastructure!</p> <p>Terraform\u2019s approach is to allow you to write code that is specific to each provider, taking advantage of that provider\u2019s unique functionality, but to use the same language, toolset, and IaC practices under the hood for all providers.</p>"},{"location":"Infrastructure/terraform/about/#terraform-trade-offs-to-consider","title":"Terraform Trade-offs to consider","text":"<p>Configuration management versus provisioning: Once you have an <code>image</code> created from a Dockerfile or Packer template, all that\u2019s left to do is <code>provision the infrastructure</code> for running those images. And when it comes to provisioning, a provisioning tool is going to be your best choice.</p> <p>Mutable infrastructure versus immutable infrastructure: If you\u2019re not using server templating tools, a popular combination is to use Terraform to provision your servers and Ansible to configure each one. This is because, Configuration management tools such as Chef, Puppet, and Ansible typically default to a <code>mutable infrastructure paradigm</code>. Over time, as you apply more and more updates, each server builds up a unique history of changes. As a result, each server becomes slightly different than all the others, leading to subtle configuration bugs that are difficult to diagnose and reproduce. If you\u2019re using a provisioning tool such as Terraform to deploy machine images created by Docker or Packer, most <code>changes</code> are actually deployments of a completely new server.</p> <p>Procedural language versus declarative language: Chef and Ansible encourage a procedural style in which you write code that specifies, step by step, how to achieve some desired end state. Terraform, CloudFormation, Puppet, OpenStack Heat, and Pulumi all encourage a more declarative style in which you write code that specifies your desired end state, and the IaC tool itself is responsible for figuring out how to achieve that state. Example, say you create 10 servers with each Ansible and Terraform. Imagine traffic has gone up, and you want to increase the number of servers to 15. With Ansible, the procedural code you wrote earlier is no longer useful; if you just updated the number of servers to 15 and reran that code, it would deploy 15 new servers, giving you 25 total! So instead, you need <code>to be aware</code> of what is already deployed and write a totally new procedural script to add the five new servers.</p> <p>General-purpose language versus domain-specific language: Chef and Pulumi allow you to use a <code>general-purpose programming language</code> (GPL) to manage infrastructure as code: Chef supports Ruby; Pulumi supports a wide variety of GPLs, including JavaScript, TypeScript, Python, Go, C#, Java, and others. Terraform, Puppet, Ansible, CloudFormation, and OpenStack Heat each use a <code>domain-specific language</code> (DSL) to manage infrastructure as code: Terraform uses HCL; Puppet uses Puppet Language; Ansible, CloudFormation, and OpenStack Heat use YAML (CloudFormation also supports JSON). DSLs are designed for use in one specific domain, whereas GPLs can be used across a broad range of domains. DSLs are easier to learn, clearer and more concise, more uniform. While, if you know the programming language well, GSLs can be advantageous as well.</p> <p>Master versus masterless: By default, Chef and Puppet require that you run a master server for storing the state of your infrastructure and distributing updates. Ansible, CloudFormation, Heat, Terraform, and Pulumi are all masterless by default. For example, Terraform communicates with cloud providers using the cloud provider\u2019s APIs, so in some sense, the API servers are master servers, except that they don\u2019t require any extra infrastructure or any extra authentication mechanisms (i.e., just use your API keys). Ansible works by connecting directly to each server over SSH, so again, you don\u2019t need to run any extra infrastructure or manage extra authentication mechanisms (i.e., just use your SSH keys).</p> <p>Agent versus agentless: Chef and Puppet require you to install agent software on each server that you want to configure. The agent typically runs in the background on each server and is responsible for installing the latest configuration management updates. All of these extra moving parts introduce a large number of new failure modes into your infrastructure. Ansible, CloudFormation, Heat, Terraform, and Pulumi do not require you to install any extra agents. You just issue commands, and the cloud provider\u2019s agents execute them for you on all of your servers. With Ansible, your servers need to run the SSH daemon, which is common to run on most servers anyway.</p> <p>Paid versus free offering: CloudFormation and OpenStack Heat are completely free. Terraform, Chef, Puppet, Ansible, and Pulumi are all available in free versions and paid versions. The free versions of Terraform, Chef, Puppet, and Ansible can all be used successfully for production use cases. A key part of managing infrastructure as code is managing state, and Pulumi, by default, uses Pulumi Service as the backend for state storage. You can switch to other supported backends for state storage, such as Amazon S3, Azure Blob Storage, or Google Cloud Storage, but the Pulumi backend documentation explains that only Pulumi Service supports transactional checkpointing (for fault tolerance and recovery), concurrent state locking (to prevent corrupting your infrastructure state in a team environment), and encrypted state in transit and at rest. So if you\u2019re going to use Pulumi, you more or less have to pay for Pulumi Service.</p> <p>Use of multiple tools together:</p> <p>You use Terraform to deploy all the underlying infrastructure, including the network topology, data stores, load balancers, and servers. You then use Ansible to deploy your apps on top of those servers. There are many ways to get Ansible and Terraform to work together (e.g., Terraform adds special tags to your servers, and Ansible uses those tags to find the servers and configure them). The major downside is that using Ansible typically means that you\u2019re writing a lot of procedural code, with mutable servers, so as your codebase, infrastructure, and team grow, maintenance can become more difficult.</p> <p>Ideally you want was an open source, cloud-agnostic provisioning tool with a large community, a mature codebase, and support for immutable infrastructure, a declarative language, a masterless and agentless architecture, and an optional paid service. Terraform, although not perfect, comes the closest to meeting all of our criteria.</p>"},{"location":"Infrastructure/terraform/about/#references","title":"References","text":"<ul> <li>Terraform: Up and Running, 3rd Edition, Yevgeniy Brikman | O'Reilly<ul> <li>Sample github code for the book</li> </ul> </li> <li>Terraform Modules | Gruntwork</li> </ul>"},{"location":"Infrastructure/terraform/components/","title":"Terraform Components","text":""},{"location":"Infrastructure/terraform/components/#provider","title":"Provider","text":"<p>Providers are platforms to create infrastructure on. E.g., AWS, GCP, Azure, etc.</p> <pre><code>provider \"aws\" {\n  region = \"us-east-2\"\n  profile = \"profile_name\"\n}\n</code></pre> <p>If you're using multiple AWS profiles, specify the profile name.</p>"},{"location":"Infrastructure/terraform/components/#resource","title":"Resource","text":"<p>Resources are the services you create within an infrastructure.</p> <pre><code>resource \"&lt;PROVIDER&gt;_&lt;TYPE&gt;\" \"&lt;NAME&gt;\" {\n  [CONFIG ...]\n}\n\nresource \"aws_instance\" \"example\" {\n  ami           = \"ami-0fb653ca2d3203ac1\"\n  instance_type = \"t2.micro\"\n\n  tags = {\n    Name = \"terraform-example\"\n  }\n}\n</code></pre> <p>Each resource supports different arguments. Some are required, others are optional. Look up the terraform documentation for all valid arguments and their values.</p> <p>It is a good practice to add tags to each resource.</p> <p>By default, AWS does not allow any incoming or outgoing traffic from an EC2 Instance. To allow the ingress, you need to create a security group.</p> <pre><code>resource \"aws_security_group\" \"instance\" {\n  name = \"terraform-example-instance\"\n\n  ingress {\n    from_port   = 8080\n    to_port     = 8080\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name = \"terraform-sg-example\"\n  }\n}\n</code></pre> <p><code>CIDR</code> blocks are a concise way to specify IP address ranges. For example, a CIDR block of 10.0.0.0/24 represents all IP addresses between 10.0.0.0 and 10.0.0.255. The CIDR block 0.0.0.0/0 is an IP address range that includes all possible IP addresses, so this security group allows incoming requests on port 8080 from any IP.</p> <p>Simply creating a security group isn\u2019t enough; you need to tell the EC2 Instance to actually use it by passing the ID of the security group into the <code>vpc_security\u200b_group_ids</code> argument of the <code>aws_instance</code> resource.</p> <p>An <code>expression</code> in Terraform is anything that returns a value. One particularly useful type of expression is a <code>reference</code>, which allows you to access values from other parts of your code. To access the ID of the security group resource, you are going to need to use a resource attribute reference, which uses the following syntax: <code>&lt;PROVIDER&gt;_&lt;TYPE&gt;.&lt;NAME&gt;.&lt;ATTRIBUTE&gt;</code>. An attribute could be one of the exported ones by the resource, like <code>id</code>.</p>"},{"location":"Infrastructure/terraform/open-tofu/","title":"OpenTofu","text":"<p>OpenTofu is a fork of Terraform that is open-source, community-driven, and managed by the Linux Foundation.</p>"},{"location":"Infrastructure/terraform/open-tofu/#migrating-from-terraform","title":"Migrating from Terraform","text":"<p>OpenTofu 1.6 is compatible with Terraform 1.6. Follow the migration guide in resources to get started.</p>"},{"location":"Infrastructure/terraform/open-tofu/#getting-started","title":"Getting started","text":"<pre><code># Installation\nbrew install opentofu\n\n# Verify version\ntofu --version\n</code></pre> <pre><code># Initialization\ntofu init\n\n# Inspect the plan\ntofu plan\n\n# Apply changes\ntofu apply\n</code></pre>"},{"location":"Infrastructure/terraform/open-tofu/#resources","title":"Resources","text":"<ul> <li>Official documentation</li> <li>Terraform to OpenTofu migration</li> </ul>"},{"location":"Infrastructure/terraform/state-management/","title":"State Management","text":"<p>Terraform uses a state file to keep track of the resources it has created. The state file is a JSON-formatted file that stores information about the infrastructure's current state, including the resources created, their configurations, and dependencies.</p> <p>Key points about Terraform's state management:</p> <ol> <li> <p>State File: Terraform maintains a local or remote state file that serves as a record of the deployed infrastructure's configuration. This file is crucial for tracking changes and understanding the current state.</p> </li> <li> <p>Local State: By default, Terraform stores the state file locally in the same directory as the configuration files. However, this is not recommended for team collaboration or production use due to potential synchronization issues.</p> </li> <li> <p>Remote State: In a collaborative or production environment, it's common to store the state file remotely, using backend systems like Amazon S3, Azure Storage, or HashiCorp Consul. Remote state allows multiple team members to work on the same infrastructure while maintaining consistency.</p> </li> <li> <p>Locking Mechanism: Terraform uses a locking mechanism to prevent multiple users or processes from modifying the state file simultaneously. This helps avoid conflicts and ensures data integrity.</p> </li> <li> <p>Checksums: Terraform calculates checksums for resource configurations within the state file. During subsequent executions, it compares these checksums to detect any changes in resource configurations.</p> </li> <li> <p>Refresh Command: Before making changes, Terraform recommends running the <code>terraform refresh</code> command, which updates the state file with the latest information about the deployed resources.</p> </li> </ol> <p>By leveraging the state file, Terraform can plan and apply changes effectively, ensuring that the infrastructure remains in the desired state as defined in the configuration files.</p>"},{"location":"Infrastructure/terraform/state-management/#avoid-committing-state-files","title":"Avoid committing state files","text":"<p>Avoiding the commitment of Terraform state files is a best practice for several reasons:</p> <ol> <li> <p>Security: State files may contain sensitive information, such as API keys, access credentials, and other secrets. Committing these files to version control systems exposes sensitive data, posing a security risk.</p> </li> <li> <p>Collaboration: In a collaborative environment, multiple team members might be working on the same infrastructure. Committing state files could lead to conflicts, especially when concurrent changes are made, resulting in potential data corruption or loss.</p> </li> <li> <p>File Size: Terraform state files can become large as the infrastructure grows. Committing large binary files to version control systems increases repository size, impacting performance and making it harder to manage.</p> </li> <li> <p>Continuous Integration/Continuous Deployment (CI/CD): CI/CD systems may handle Terraform deployments, and committing state files could interfere with the workflow. CI/CD processes often rely on remote state storage for consistency.</p> </li> <li> <p>Portability: Committing state files can limit the portability of the infrastructure. By storing state remotely, different team members or environments can easily access and manage the infrastructure.</p> </li> <li> <p>State Locking: Remote state storage provides a locking mechanism to prevent concurrent modifications, ensuring data integrity. Committing state files locally does not provide this level of protection.</p> </li> </ol> <p>To address these concerns, it's recommended to use a remote backend for storing Terraform state, such as Amazon S3, Azure Storage, or HashiCorp Consul. This approach enhances security, facilitates collaboration, and improves the overall maintainability of Terraform configurations.</p>"},{"location":"Infrastructure/terraform/state-management/#storing-state-remotely","title":"Storing state remotely","text":"<p>Using a remote backend for storing Terraform state involves configuring Terraform to store the state information in a centralized and shared location, such as cloud storage. Here's a general guide:</p> <ol> <li> <p>Choose a Backend:    Decide on a remote backend. Popular options include Amazon S3, Azure Storage, Google Cloud Storage, or HashiCorp Consul.</p> </li> <li> <p>Update Terraform Configuration:    Modify your Terraform configuration to include the backend configuration. For example, using Amazon S3 as a backend:</p> </li> </ol> <p><code>hcl    terraform {      backend \"s3\" {        bucket         = \"your-s3-bucket-name\"        key            = \"path/to/your/statefile.tfstate\"        region         = \"your-aws-region\"        encrypt        = true        dynamodb_table = \"optional-dynamodb-table-name-for-locking\"      }    }</code></p> <p>Adjust the configuration according to your chosen backend.</p> <ol> <li>Initialize Terraform:    After updating the configuration, run the following command in the terminal:</li> </ol> <p><code>bash    terraform init</code></p> <p>This initializes Terraform and configures it to use the specified backend.</p> <ol> <li>Apply Changes:    Apply changes to your infrastructure as usual:</li> </ol> <p><code>bash    terraform apply</code></p> <ol> <li>State in Remote Backend:    The state file is now stored in the chosen remote backend. Other team members or environments can use the same backend configuration to collaborate.</li> </ol> <p>This approach ensures that the Terraform state is stored centrally, reducing the risk of conflicts and enabling better collaboration. Additionally, using a backend like S3 often provides features like versioning and encryption for added security. Adjust the backend configuration based on your specific cloud provider and requirements.</p>"},{"location":"IoT/raspberry-pi/","title":"Raspberry Pi","text":"<p>I prefer working on the pi remotely from my laptop, where I have everything set up. To do that, the device needs to be configured first.</p>"},{"location":"IoT/raspberry-pi/#index","title":"Index","text":"<ul> <li>First time setting up the device</li> <li>Accessing the pi remotely</li> </ul>"},{"location":"IoT/raspberry-pi/#references","title":"References","text":"<ul> <li>Raspberry Pi</li> <li>SSH/OpenSSH/Keys</li> <li>SSH/OpenSSH/Configuring</li> <li>digitalocean How To Configure SSH Key-Based Authentication on a Linux Server</li> </ul>"},{"location":"IoT/raspberry-pi/01-setting-up-device/","title":"Setting up the device","text":""},{"location":"IoT/raspberry-pi/01-setting-up-device/#install-operating-system","title":"Install Operating System","text":"<p>Skip this step if we have an SD card already with one of the Pi OS installed.</p>"},{"location":"IoT/raspberry-pi/01-setting-up-device/#downloading-os-image","title":"Downloading OS image","text":"<p>Download a desired image from the official website</p>"},{"location":"IoT/raspberry-pi/01-setting-up-device/#burn-image-on-sd-card","title":"Burn Image on SD card","text":"<p>You can use the official Raspberry Pi Imager (preferred). Or BalenaEtcher is an excellent image writer for SD cards. These tool extracts the available files and makes the card bootable.</p>"},{"location":"IoT/raspberry-pi/01-setting-up-device/#update-credentials","title":"Update Credentials","text":"<p>It is always a good idea to set personalized credentials on devices before letting them go online.</p>"},{"location":"IoT/raspberry-pi/01-setting-up-device/#default-credentials","title":"Default Credentials","text":"<ul> <li>default user name is <code>pi</code></li> <li>default password is <code>raspberrypi</code></li> <li>default hostname is <code>raspberrypi.local</code></li> </ul>"},{"location":"IoT/raspberry-pi/01-setting-up-device/#changing-password","title":"Changing password","text":"<p>Type <code>passwd</code>, press <code>Enter</code> and follow the instructions.</p>"},{"location":"IoT/raspberry-pi/01-setting-up-device/#changing-password-for-root-user","title":"Changing password for root user","text":"<ol> <li>Type <code>sudo su</code> and enter</li> <li>Type <code>passwd root</code>, press enter</li> <li>Type your new password and hit enter</li> </ol>"},{"location":"IoT/raspberry-pi/01-setting-up-device/#add-new-user","title":"Add new user","text":"<ol> <li>Type <code>adduser</code> and enter</li> <li>Type <code>sudo adduser &lt;user_name&gt;</code> and enter. Example: <code>sudo adduser mypiuser</code></li> <li>Follow the remaining instructions</li> </ol>"},{"location":"IoT/raspberry-pi/01-setting-up-device/#delete-a-user","title":"Delete a user","text":"<ol> <li>Type <code>userdel</code> and enter</li> <li>Type <code>sudo userdel -r &lt;user_name&gt;</code> and enter. Example: <code>sudo userdel -r pi</code></li> </ol>"},{"location":"IoT/raspberry-pi/01-setting-up-device/#enable-ssh","title":"Enable SSH","text":"<p>Secure Socket Shell (ssh) disabled by default, for security reasons. To enable ssh, place a blank text file with name <code>ssh</code> (no file extension) in the root directory. Root directory depends on the user. For user <code>pi</code>, it would be <code>/home/pi/</code>.</p>"},{"location":"IoT/raspberry-pi/01-setting-up-device/#set-up-wifi","title":"Set up WiFi","text":"<p>This step configures WiFi for the Pi, so the device automatically stays connected to internet every time it is on.</p> <p>Create <code>/etc/wpa_supplicant/wpa_supplicant.conf</code> file, with settings below. Make sure to replace network credentials with desired ones.</p> <pre><code>country=US\nctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev\nupdate_config=1\n\nnetwork={\n    ssid=\"NETWORK-NAME\"\n    psk=\"NETWORK-PASSWORD\"\n    priority=1\n    id_str=\"Family\"\n}\n\n# If you have more than one network to connect to\nnetwork={\n    ......\n}\n</code></pre> <p>If the wifi network is hidden, add <code>scan_ssid=1</code> to the configuration file.</p> <p>Verify the <code>wlan0</code> network setting using <code>iwconfig</code>, <code>ifconfig</code> or <code>iwlist wlan0 scan</code> commands.</p>"},{"location":"IoT/raspberry-pi/01-setting-up-device/#update-device","title":"Update Device","text":"<p>To get the latest version of the operation system and dependencies, use the following command:</p> <pre><code>$ sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre>"},{"location":"IoT/raspberry-pi/02-accessing-remotely/","title":"Accessing the device remotely","text":"<p>Assumption: Both devices are on the same network</p>"},{"location":"IoT/raspberry-pi/02-accessing-remotely/#ssh","title":"SSH","text":""},{"location":"IoT/raspberry-pi/02-accessing-remotely/#ssh-using-credentials","title":"SSH using credentials","text":"<pre><code># Command\n$ ssh user@ip_address\n\n# Example\n$ ssh pi@192.168.0.1\n</code></pre>"},{"location":"IoT/raspberry-pi/02-accessing-remotely/#ssh-using-key-based-authentication","title":"SSH using key-based authentication","text":"<p>To create your public and private SSH keys on the command-line:</p> <pre><code>mkdir ~/.ssh\nchmod 700 ~/.ssh\nssh-keygen -t rsa\n</code></pre> <p>Your public key is now available at <code>~/.ssh/id_rsa.pub</code>.</p>"},{"location":"IoT/raspberry-pi/02-accessing-remotely/#about-passphrase","title":"About passphrase","text":"<p>Your SSH key passphrase is only used to protect your private key from thieves. It's never transmitted over the Internet, and the strength of your key has nothing to do with the strength of your passphrase.</p>"},{"location":"IoT/raspberry-pi/02-accessing-remotely/#transfer-client-key-to-host","title":"Transfer client key to host","text":"<pre><code>ssh-copy-id &lt;username&gt;@&lt;host&gt;\n</code></pre>"},{"location":"IoT/raspberry-pi/02-accessing-remotely/#verifying-access","title":"Verifying access","text":"<pre><code>ssh &lt;username&gt;@&lt;host&gt;\n</code></pre>"},{"location":"IoT/raspberry-pi/02-accessing-remotely/#disable-password-authentication","title":"Disable password authentication","text":"<p>Look for <code>sshd_config</code> file in the pi, and change the following line.</p> <pre><code>#PasswordAuthentication yes\n\nto\n\nPasswordAuthentication no\n</code></pre>"},{"location":"IoT/raspberry-pi/02-accessing-remotely/#installing-some-dependencies","title":"Installing some dependencies","text":"<pre><code>$ sudo apt install -y &lt;package_1&gt;  &lt;package_2&gt; . . .\n</code></pre>"},{"location":"Local/r-cli/","title":"R","text":""},{"location":"Local/r-cli/#installation","title":"Installation","text":"<pre><code>$ brew tap homebrew/science\n$ brew install R\n\nYou will probably also want to install a few of Hadley Wickham\u2019s packages\n$ R                                  // Launches R\n    &gt; install.packages(\"ggplot2\")    // Offers a powerful graphics language for creating elegant and complex plots\n    &gt; install.packages(\"dplyr\")      // Makes describing the steps and program execution, fast and easy\n    &gt; install.packages(\"tidyr\")      // Makes it easy to tidy your data\n\nWhen you exit the R prompt, make sure to save the workspace image if you want to avoid downloading the same\npackages again in future.\n</code></pre>"},{"location":"Local/r-cli/#install-rstudio","title":"Install RStudio","text":"<pre><code>RStudio is relatively lightweight and runs without using too much system resources\n\nDownload and install from:\nhttps://www.rstudio.com/products/RStudio/#Desktop\n</code></pre>"},{"location":"Local/r-cli/#install-mactex","title":"Install MacTex","text":"<pre><code>To export Rmarkdown documents as PDFs, to share analyses with colleagues, RStudio requires MacTeX. The installer for\nthe full version of MacTeX is giant (~3GB), but worth it if you plan to share your work as a PDF. Unless you\u2019re\nexperienced with MacTeX, go with the full MacTeX installation rather than BasicTeX.\n\nDownload and install from:\nhttps://tug.org/mactex/\n</code></pre>"},{"location":"Local/virtualbox-cli/","title":"VirtualBox","text":""},{"location":"Local/virtualbox-cli/#installation","title":"Installation","text":"<pre><code>Option 1:\nDownload and install from\nhttps://www.virtualbox.org/wiki/Downloads\n\nOption 2:\n$ brew cask install virtualbox            // Install a fresh version\n$ brew cask reinstall virtualbox          // Update existing version\n</code></pre>"},{"location":"Local/virtualbox-cli/#verification","title":"Verification","text":"<pre><code>$ vboxmanage --version\n$ vboxmanage list runningvms\n</code></pre>"},{"location":"Local/xcode-cli/","title":"XCode","text":"<p>It is no longer necessary to download the full Xcode IDE from the Mac App Store. You can save youreself some disk space and just run the command below to only install Xcode Command Line tools.</p> <p><code>XCode Command line</code> is bare minimum software required for using Homebrew or MacPorts.</p>"},{"location":"Local/xcode-cli/#installation","title":"Installation","text":"<pre><code>$ xcode-select --install\n</code></pre>"},{"location":"Local/xcode-cli/#verification","title":"Verification","text":"<pre><code>$ xcode-select -p\nShould output /Applications/Xcode.app/Contents/Developer\n</code></pre>"},{"location":"Local/IDE/atom/","title":"Atom","text":"<ul> <li>As they say, its a hackable-text-editor. Built in HTML, JS, CSS and Node.js running over Electron</li> <li>Open source IDE with wonderful support for packages</li> <li>Refer: https://atom.io/packages</li> <li>Looks promising for full-stack work</li> </ul>"},{"location":"Local/IDE/atom/#plugins-configurations","title":"Plugins &amp; Configurations","text":"<pre><code>Atom has a package manager for you to be able to install plugins:\n$ apm\n</code></pre>"},{"location":"Local/IDE/intellij/","title":"IntelliJ Idea","text":"<p>So far best IDE for Java</p>"},{"location":"Local/IDE/intellij/#keymap","title":"KeyMap","text":"<pre><code>FYI... you can import a custom KeyMap as well, to keep consistent across IDEs/Languages\n\nPredefined KeyMap Card\nhttps://resources.jetbrains.com/storage/products/intellij-idea/docs/IntelliJIDEA_ReferenceCard.pdf\nAlso available under Help | Keymap Reference\n\nCmd + Ctrl + G     // Select all Occurrences\nCmd + Shift + L    // Reformat code\nCmd + Shift + A    // Find an action within IDE\nCmd + Shift + O    // Find a file\nCmd + O            // Find a Class\nCmd + E            // View recent\n</code></pre>"},{"location":"Local/IDE/intellij/#plugins-configurations","title":"Plugins &amp; Configurations","text":"<pre><code>Key Promoter X\nA plugin that shows the corresponding keyboard shortcut whenever a command is executed using the mouse and suggests to\ncreate a shortcut for commands that are executed frequently.\nhttps://plugins.jetbrains.com/plugin/9792-key-promoter-x\n\nFind Plugin\n===========\nMarkdown\nAutoComplete\n\nhttps://plugins.jetbrains.com/search?correctionAllowed=true&amp;pr=idea_ce&amp;search=&amp;orderBy=&amp;should_have_source=\n</code></pre>"},{"location":"Local/IDE/intellij/#checkout","title":"Checkout","text":"<pre><code>https://plugins.jetbrains.com/plugin/4486-sql-code-assistant\nhttps://plugins.jetbrains.com/plugin/228-sql-query-plugin\nhttps://plugins.jetbrains.com/plugin/9696-java-stream-debugger\n</code></pre>"},{"location":"Local/IDE/jetbrains-webstorm/","title":"WebStorm","text":"<p>Baby project of IntelliJ, a cut-out of few features for JavaScript development</p>"},{"location":"Local/IDE/jetbrains-webstorm/#features","title":"Features","text":"<ul> <li>Does everything very well related to JavaScript, TypeScript, Node.js</li> <li>Excellent Git support</li> </ul>"},{"location":"Local/IDE/visual-studio-code/","title":"Visual Studio Code","text":"<p>A free, open source, constantly updating, multi-platform and full-stack IDE.</p> <pre><code>Supported Languages that I care for:\nHTML, CSS, SASS, XML, JavaScript, TypeScript\nPython, R\nC++, Java, Go, F#, SQL\nDockerfile, Markdown\n</code></pre>"},{"location":"Local/IDE/visual-studio-code/#plugins-index-personal","title":"Plugins Index - Personal","text":"<p>Remember using Emmet built-in capabilities in the IDE</p>"},{"location":"Local/IDE/visual-studio-code/#personal-choice-code-management","title":"Personal Choice - Code Management","text":"<ul> <li>Settings Sync - Synchronize Settings, Snippets, Themes, File Icons, Launch, Keybindings, Workspaces and Extensions Across Multiple Machines Using GitHub Gist.</li> <li>Project Manager - Easily switch between projects</li> </ul>"},{"location":"Local/IDE/visual-studio-code/#personal-choice-visual-extensions","title":"Personal Choice - Visual Extensions","text":"<ul> <li>Material Theme - Most popular IDE UI theme so far</li> <li>Material Icon Theme - Easy to identify files within the project</li> <li>Prettier - Code Formatter - helps format your JavaScript/TypeScript/JSON/YAML/CSS/HTML/Markdown and Angular/Vue/GraphQL, colors keywords to make your code easily readable. beautify is another similar extensions. Remember to set \"Format on save\" setting after pressing \"Cmd + comma\".</li> <li>Bracket Pair Colorizer 2 - Makes it easier to identify opening and closing brackets</li> <li>TODO Highlight - highlight TODOs, FIXMEs, and any keywords, annotations</li> <li> </li> <li> <p>Auto Rename Tag - useful for HTML to rename start and ending tag on change without having to remember</p> </li> <li> <p>Rest Client - like postman, calls API from the IDE</p> </li> <li>CSS Peek - Allows you to peek CSS definition from HTML file</li> <li>HTML CSS Support - Missing CSS support for HTML documents</li> <li>SVG Viewer - Lets you see an actual image for svg format file</li> <li>Live Server - creates a development local server for your static and dynamic pages. Live Reload reloads the page as soon as you save your work.</li> <li>Live Sass Compiler - compiles your scss files into css files just as swiftly and as real-time as node-sass</li> <li>Paste JSON as Code - Copy JSON, paste as Go, TypeScript, C#, C++ and more.</li> </ul>"},{"location":"Local/IDE/visual-studio-code/#personal-choice-functional-extensions-ui","title":"Personal Choice - Functional Extensions - UI","text":""},{"location":"Local/IDE/visual-studio-code/#personal-choice-functional-extensions-frontend","title":"Personal Choice - Functional Extensions - Frontend","text":"<ul> <li>Debugger for Chrome - Debug Javascript code in chrome browser</li> <li>ES7 React/Redux/GraphQL/React-Native snippets - provides you Javascript and React/Redux snippets in ES7 with Babel plugin features.</li> <li>Import Cost - Display import/require package size (Javascript/node) in the editor</li> </ul>"},{"location":"Local/IDE/visual-studio-code/#personal-choice-backend","title":"Personal Choice - Backend","text":"<ul> <li>Maven for Java - Manage Maven projects, execute goals, generate project from archetype</li> <li>Java Dependency Viewer - Manage Java Dependencies in VSCode</li> <li>Java Extension Pack - collection of popular extensions that can help write, test and debug Java applications</li> </ul>"},{"location":"Local/IDE/visual-studio-code/#personal-choice-iot","title":"Personal Choice - IoT","text":"<ul> <li>PlatformIO IDE - IDE for embedded development</li> </ul>"},{"location":"Local/IDE/visual-studio-code/#plugins-index","title":"Plugins Index","text":""},{"location":"Local/IDE/visual-studio-code/#must-have","title":"Must-Have","text":"<ul> <li>ESLint - linting for your Javascript and jsx.</li> <li>Beautify - Beautify javascript, JSON, CSS, Sass, and HTML</li> <li>VS Live Share - enables you to collaboratively edit and debug with others in real time, regardless what programming languages you're using</li> <li>VS Live Share Extension Pack - includes integrated audio and text chat</li> <li>Code Runner - Run code snippet or code file for multiple languages (pretty much everything)</li> <li>YAML - YAML Language Support by Red Hat, with built-in Kubernetes and Kedge syntax support</li> <li>Markdown Preview Enhanced - Markdown Preview Enhanced ported to vscode</li> </ul>"},{"location":"Local/IDE/visual-studio-code/#need-to-have","title":"Need-to-Have","text":"<ul> <li>Git Project Manager - Allows you to change easily between git projects.</li> <li>Todo Tree - displays them in a tree view in the explorer pane. Clicking a TODO within the tree will open the file and put the cursor on the line containing the TODO.</li> <li>markdownlint - Linter for Markdown. Some constructs don't work well in all parsers and should be avoided.</li> <li>Git History - View git log, file history, compare branches or commits</li> <li>GitLens - Visualize code authorship at a glance via Git blame annotations and code lens, seamlessly navigate and explore Git repositories, gain valuable insights via powerful comparison commands</li> <li>Docker - easy to build, manage and deploy containerized applications. Automatic file creation, Syntax highlighting, hover tips, IntelliSense, Linting, etc</li> <li>Terraform - Syntax highlighting, linting, formatting, and validation for Hashicorp's Terraform</li> <li>Python - Supports versions 2.7, 3.4, including features such as linting, debugging, IntelliSense, code navigation, code formatting, refactoring, unit tests, snippets, and more</li> <li>Jupyter - Data Science with Jupyter on Visual Studio Code</li> <li>C/C++ - IntelliSense, debugging, and code browsing</li> <li>Arduino - easy to develop, build, deploy and debug your Arduino sketches</li> </ul>"},{"location":"Local/IDE/visual-studio-code/#good-to-have","title":"Good-to-Have","text":"<ul> <li>Auto Import - Automatically finds, parses and provides code actions and code completion for all available imports. Works with Typescript and TSX</li> <li>npm Intellisense - plugin that autocompletes npm modules in import statements</li> <li>NPM - helps in to manage the package.json file in every imaginable way. It also indicates discrepancies in version control of packages. It also provides for quick run of npm commands with easy shortcuts.</li> <li>Path Autocomplete - Provides path completion for visual studio code</li> <li>Highlight Matching Tag - Highlights matching closing or opening tag</li> <li>CSS Peek - Allow peeking to css ID and class strings as definitions from html files to respective CSS. Allows peek and goto definition.</li> <li>SVG Viewer - SVG Viewer for Visual Studio Code.</li> <li>Javascript Code Snippets - provides javascript code snippets in ES6 syntax while using javascript, Typescript, Javascript React, Typescript React files</li> <li>JavaScript (ES6) code snippets - Code snippets for JavaScript in ES6 syntax</li> <li>React Native Tools - Code-hinting, debugging and integrated commands for React Native</li> <li>REST Client - allows you to send HTTP request and view the response</li> <li>Go - Rich Go language support for Visual Studio Code</li> <li>Java Test Runner - Run and debug JUnit or TestNG test cases</li> <li>Debugger for Java - A lightweight Java debugger</li> <li>XML Tools - XML Formatting, XQuery, and XPath Tools</li> <li>Anaconda Extension Pack - Includes Python and YAML support</li> <li>Indent Rainbow - colorizes indentation in code to make readability easier</li> </ul>"},{"location":"Local/IDE/visual-studio-code/#yet-to-explore","title":"Yet to explore","text":"<ul> <li>IntelliSense for CSS, SCSS class names in HTML, Slim and SCSS</li> <li>IntelliSense for CSS class names in HTML - CSS class name completion for the HTML class attribute based on the definitions found in your workspace.</li> <li>Power Tools</li> <li>Git Graph</li> <li>Atom Keymap</li> <li>Azure Pipelines</li> <li>EditorConfig for VS Code</li> <li>SQL Server (mssql) - Develop Microsoft SQL Server, Azure SQL Database and SQL Data Warehouse everywhere</li> <li>Debugger for Chrome</li> <li>LaTeX Workshop - Boost LaTeX typesetting efficiency with preview, compile, autocomplete, colorize, and more</li> </ul>"},{"location":"Local/IDE/visual-studio-code/#downsides","title":"Downsides","text":"<ul> <li>No support noticed for GCP yet.</li> </ul>"},{"location":"Local/IDE/visual-studio-code/#additional-notes","title":"Additional Notes","text":"<pre><code>https://code.visualstudio.com/docs/nodejs/nodejs-tutorial\n</code></pre>"},{"location":"Local/SSH/","title":"SSH for Remote Server Authentication","text":"<ol> <li>Generate a new key</li> <li>Config file</li> <li>Copy SSH key to Remote server</li> <li>SSH into a machine</li> <li>Verify Fingerprints</li> <li>Copy files over SSH</li> <li>Audit SSH logs</li> <li>Port forwarding with SSH tunnel</li> <li>Lock down incoming SSH connections</li> </ol> <p>Note:</p> <ol> <li>Never share your private key with anyone. That's your identification</li> <li>Feel free to share your public key with anyone or any server.</li> <li>Last part of the content of public key is a comment to document which public key it is. Feel free to change it anytime. Comment is helpful in keeping track of servers using different keys.</li> </ol>"},{"location":"Local/SSH/#ssh-escape-sequences-and-codes","title":"SSH escape sequences and codes","text":"<p>Little known feature. These hidden sequences allow you to unstick a frozen terminal window, keep a remote SSH session open in the background, and more</p> <pre><code>Type ~(tilda) + .(period) to exit from a frozen terminal window\n\nTo keep an ssh connection open but not occupy the terminal window.\nThis will suspend ssh connection in the background\nType ~(tilda) + Ctrl z\n\nTo reconnect back to the suspended connection\n$ fg\n</code></pre>"},{"location":"Local/SSH/#questions","title":"Questions","text":"<pre><code>1. How is userid/pwd different from ssh to the server as above?\n</code></pre>"},{"location":"Local/SSH/#resources","title":"Resources","text":"<ol> <li>egghead.io SSH for Remote Server Authentication</li> <li></li> </ol>"},{"location":"Local/SSH/copy_files_over_ssh/","title":"Securely copy files remotely over SSH","text":"<pre><code>Copy file to host machine\n$ scp &lt;file&gt; &lt;user&gt;@&lt;host&gt;:&lt;location&gt;\n$ scp foo.txt root@104.105.103.102:~/\n\nCopy folder to host machine (recursive copy)\n$ scp &lt;folder&gt; &lt;user&gt;@&lt;host&gt;:&lt;location&gt;\n$ scp -r some-folder root@104.105.103.102:~/\n\nNote: You can use -i and -P flags here as well\nAlert: the port flag in this case is UPPERCASE\n\nCopy file from host machine\n$ scp &lt;user&gt;@&lt;host&gt;:&lt;file_path&gt; &lt;destination_folder&gt;\n$ scp root@104.105.103.102:~/foo.txt ./foo.txt\n</code></pre>"},{"location":"Local/SSH/copy_ssh_key_to_remote_server/","title":"Copy an SSH key to a remote server","text":"<pre><code>$ ssh-copy-id root@104.105.103.102\nYou'll be asked for password most likely and then you're all set.\n\nTest if your key was copied to the server\n$ ssh root@104.105.103.102\n\nOnce you ssh into the server, look at your key by:\nroot@instance:~# vi ~/.ssh/authorized_keys\n\nThis file contains a list of public ssh keys, which have been granted access for authentication.\nThe key you copied from your local machine should be in here.\nTo allow another user to authenticate into the server, just add their public SSH key into this file.\n</code></pre>"},{"location":"Local/SSH/lock_down_incoming_ssh_connections/","title":"Modify server configuration to lock down incoming SSH connections","text":"<pre><code>root@instance:~# vi /etc/ssh/sshd_config\n\nDisable the ability to login as a root user by changing the following value\nPermitRootLogin no\n\nDisable the ability to login as a root user by using password. And able to login using SSH keys\nPermitRootLogin prohibit-password\n\nCompletely disable password authentication on the server by changing the following value\nPasswordAuthentication no\n\nOnly allow specific users to be able to SSH into the server\nAllowUsers foo bar@1.2.3.4 *@1.2.3.4\nThis allows specific users, user from a specific IP address or all users coming in from specific IP address\n\nNow restart the SSH service\nroot@instance:~# service ssh restart\n</code></pre>"},{"location":"Local/SSH/port_forwarding_with_ssh_tunnel/","title":"Configure local and remote port forwarding with an SSH tunnel","text":"<p>Let's say you are on a network connection which is blocking access to a specific website. You can easily get around this with an SSH tunnel and local port forwarding. This allows you to bypass intranet firewalls and setup persistent connections between your local host machine and a remote server.</p> <pre><code>$ ssh -L &lt;local&gt;:&lt;host&gt;:&lt;remote&gt; &lt;user&gt;@&lt;machine&gt;\n$ ssh -L 8000:yahoo.com:80 root@104.105.103.102\n$ ssh -f -N -L 8000:yahoo.com:80 root@104.105.103.102\n\nlocal   - local port you wish to use as a tunnel\nhost    - website or host you wish to access\nremote  - port you wish to connect to of the host\nmachine - ip of the machine acting as a tunnel\n-f flag - tells the command to run in background\n-N flag - does not open a shell window\n\nNote: This works when one or few hosts are blocked, not everything\n\nYou can look up the process if its actually running\n$ ps aux | grep ssh\n\nKill the tunnel by\n$ kill &lt;process_id&gt;\n</code></pre> <p>Forward remote ports. This will forward remote connections back to our local host machine. Useful when the user wants to access their local host machine.</p> <pre><code>Note: Remote port forwarding is disabled on SSH service by default \n\nEnable Remote port forwarding on host\nroot@instance:~# vi /etc/ssh/sshd_config\n\nIn the file, set the GatewayPorts to yes\nGatewayPorts yes\n\nNow restart the SSH service\nroot@instance:~# service ssh restart\n\nBack to your local machine terminal\n$ ssh -R &lt;remote&gt;:&lt;listerner_ip&gt;:&lt;local&gt; &lt;connection_string&gt;\n$ ssh -R 8000:localhost:3000 root@104.105.103.102\n\nremote - port you wish to listen to on the remote host\nlocal  - port that the requests should be forwarded to\n</code></pre>"},{"location":"Local/SSH/ssh_audit_logs/","title":"SSH Monitoring / Audit","text":"<p>Monitoring and Auditing SSH connection attempts</p> <pre><code>root@instance:~# vi /var/log/auth.log\n\nDisplay most recent login attempts from all users\nroot@instance:~# lastlog\n\nDisplay most recent login attempts from a specific users\nroot@instance:~# lastlog -u &lt;username&gt;\n\nLookup bash history file to see if there was an unauthorized access and what all was done\nroot@instance:~# vi ~/.bash_history\n</code></pre>"},{"location":"Local/SSH/ssh_config_file/","title":"SSH config file","text":"<p>Simplifies SSH connection by storing the intricate details surrounding the SSH connection. Fields are optional not required.</p> <pre><code># Create a config file\n$ vi ~/.ssh/config\n</code></pre> <p>Content of config file</p> <pre><code>Host 104.105.103.102\n\nOR\n\nHost prod-instance\n    HostName 104.105.103.102\n    User root\n    IdentityFile ~/.ssh/id_rsa\n    Port 22\n</code></pre> <p>SSH into the server configured in config file</p> <pre><code>$ ssh prod-instance\n\n# So much easier!!\n</code></pre>"},{"location":"Local/SSH/ssh_into_a_machine/","title":"SSH into a machine","text":"<p>Use this command to ssh into a remote/local machine.</p> <pre><code># SSH into a server with a user name\n$ ssh root@104.105.103.102\n$ ssh root@host.com\n</code></pre> <p>Note: If you try to SSH into a machine by using SSH command without <code>username</code>, the command assumes the name same as the host machine name, which is not desired in a lot of cases.</p>"},{"location":"Local/SSH/ssh_into_a_machine/#port-access","title":"Port access","text":"<p>Sometimes for security reasons, some server block connections to the default port 22 and bind the ssh server to a different port.</p> <pre><code># SSH into a server using a different port 2022\n$ ssh -p 2022 root@104.105.103.102\n</code></pre>"},{"location":"Local/SSH/ssh_into_a_machine/#using-identity","title":"Using identity","text":"<p>Using different identity instead of default one</p> <pre><code># SSH into a server using a different identity and not the default id_rsa file\n$ ssh -i ~/.ssh/another_key root@104.105.103.102\n</code></pre>"},{"location":"Local/SSH/ssh_into_a_machine/#executing-a-command-remotely","title":"Executing a command remotely","text":"<pre><code># SSH, Run a command and Exit\n$ ssh root@104.105.103.102 &lt;command&gt;\n$ ssh root@104.105.103.102 hostname\n</code></pre>"},{"location":"Local/SSH/ssh_keygen/","title":"Generate new SSH public/private key pair","text":"<pre><code># Most common way\n$ ssh-keygen\n</code></pre> <p>Key pair will be generated in the location: <code>~/.ssh</code>.</p> <p>File names:</p> <ul> <li>Private key - id_rsa</li> <li>Public key  - id_rsa.pub</li> </ul>"},{"location":"Local/SSH/ssh_verify_fingerprints/","title":"Check and Verify SSH fingerprints","text":"<p>Once you SSH into a remote server, server fingerprint and key are added to <code>~/.ssh/known_hosts</code> file. This file contains a list of remote servers you've connected to in the past. File ensures that you are connected to the correct server and not a fake impersonator.</p> <p>If you try connecting to a remote host with an IP previously assigned to another machine, you'll get a host mismatch error. This is because the remote host fingerprint does not match to your <code>known_hosts</code> file.</p> <pre><code># Find the remote host fingerprint on server\n$ ssh root@104.105.103.102\nroot@instance:~# ssh-keygen -l -f /etc/ssh/ssh_host_ecdsa_key.pub\n\n-l - tells the keygen that we want the fingerprint\n-f - tell the keygen where to find the public key\n\n# Remove all the keys related to a remote host machine\n$ ssh-keygen -R 104.105.103.102\n\n# Find the remote host fingerprint on local machine\n$ ssh root@104.105.103.102\n\n# With first time connections, the server shows its fingerprint. Match this to the one you found on the server\n</code></pre>"},{"location":"Local/git-cli/","title":"Git","text":"<ol> <li>Installation</li> <li>Configuration</li> <li>Cheatsheet</li> </ol>"},{"location":"Local/git-cli/#common-commands","title":"Common Commands","text":"<pre><code># remove all untracked files\n$ git clean -fxd\n</code></pre>"},{"location":"Local/git-cli/#references","title":"References","text":"<ul> <li>Official documentation</li> <li>Official reference manual</li> <li>Official online book Pro Git (free)</li> <li>GitHub cheatsheet</li> <li>Git Hooks</li> </ul>"},{"location":"Local/git-cli/cheatsheet/","title":"Git Cheatsheet","text":""},{"location":"Local/git-cli/cheatsheet/#git-basics","title":"Git Basics","text":"<pre><code># Displays version of Git installed\n$ git --version\n\n# Initialize the current folder or directory as a git repository\n$ git init\n$ git init &lt;directory&gt;\n\n# Clone an existing remote repo or local repo onto local machine\n$ git clone &lt;repo-url/path&gt;\n\n# List which files are staged, unstaged and untracked\n$ git status\n\n# Stage all changes in directory for the next commit\n# Will only add new files and modified files to the index to be committed\n$ git add &lt;directory&gt;\n# Stage all changes in the repository\n$ git add -A\n\n# Commit the staged snapshot with &lt;message&gt; as commit message\n$ git commit -m \"&lt;message&gt;\"\n\n# Update last commit message\n$ git commit --amend\n\n# Display the entire commit history using default format.\n$ git log\n\n# Show unstaged changes between your index and working directory\n$ git diff\n# Show difference between working directory and last commit\n$ git diff HEAD\n# Show difference between staged changes and last commit\n$ git diff --cached\n</code></pre>"},{"location":"Local/git-cli/cheatsheet/#undoing-changes","title":"Undoing Changes","text":"<pre><code># Create new commit that undoes all of the changes made in &lt;commit&gt;, then apply\n$ git revert &lt;commit&gt;\n\n# Discard changes in working directory or the staging area\n$ git restore &lt;file&gt;\n# Restore myfile.txt in staging area and working directory to the state of last commit (HEAD)\n$ git restore --source=HEAD --staged --worktree myfile.txt\n\n# Reset staging area to match most recent commit, but leave code unchanged\n$ git reset\n# Remove &lt;file&gt; from staging area\n$ git reset &lt;file&gt;\n# Reset staging area and working directory\n$ git reset --hard\n# Move current branch tip backward to &lt;commit&gt;, reset staging area but NOT local code\n$ git reset &lt;commit&gt;\n# Move current branch tip backward to &lt;commit&gt;, reset staging area as well as local code\n$ git reset --hard &lt;commit&gt;\n# Move HEAD (and the branch pointer) one commit back, discarding the last commit and all changes in the staging area and the working directory\n$ git reset --hard HEAD~1\n\n# Remove untracked files from your working directory\n$ git clean\n# Perform a \"dry run\" of the git clean\n# Shows what files would be removed from the working directory, but it won't actually remove them.\n$ git clean -n\n</code></pre>"},{"location":"Local/git-cli/cheatsheet/#git-branches","title":"Git Branches","text":"<pre><code># List all of the branches in your repo\n$ git branch\n\n# Checkout an existing branch named &lt;branch&gt;\n$ git checkout &lt;branch&gt;\n\n# Create and checkout a new branch named &lt;branch&gt;\n$ git checkout -b &lt;branch&gt;\n\n# Merge &lt;branch&gt; into current branch\n$ git merge &lt;branch&gt;\n</code></pre>"},{"location":"Local/git-cli/cheatsheet/#remote-repositories","title":"Remote Repositories","text":"<pre><code># Create a new connection to remote repo. &lt;name&gt; is shortcut for &lt;url&gt;\n$ git remote add &lt;name&gt; &lt;url&gt;\n\n# Update the remote repo url to &lt;url&gt; with shortcut &lt;name&gt;\n$ git remote set-url &lt;name&gt; &lt;url&gt;\n\n# Fetches a specific &lt;branch&gt; from the remote repo.\n$ git fetch &lt;remote&gt; &lt;branch&gt;\n\n# Fetch the specified remote's copy of current branch and merge into local copy\n$ git pull &lt;remote&gt;\n\n# Push the &lt;branch&gt; to &lt;remote&gt;, along with necessary commits and objects\n$ git push &lt;remote&gt; &lt;branch&gt;\n#Creates named branch in remote if doesn't exist\n</code></pre>"},{"location":"Local/git-cli/common-tasks/","title":"Common Tasks in Git","text":""},{"location":"Local/git-cli/common-tasks/#change-commit-message","title":"Change commit message","text":"<pre><code># Change last commit message\n$ git commit --amend\n</code></pre>"},{"location":"Local/git-cli/common-tasks/#handling-branches","title":"Handling Branches","text":"<pre><code># Fetch all remote branches\n$ git fetch origin\n\n# List the branches available for checkout\ngit branch -a\n\n# Pull changes from a remote branch\n# git automatically sets the local branch to track the remote branch\ngit checkout -b my-branch origin/my-branch\n</code></pre>"},{"location":"Local/git-cli/common-tasks/#creating-branch-locally","title":"Creating branch locally","text":"<pre><code># create a new branch\ngit branch new-branch\n\n# change environment to the new branch\ngit checkout new-branch\n\n# create a change\ntouch new-file.js\n\n# commit the change\ngit add .\ngit commit -m \"add new file\"\n\n# push to a new branch\ngit push --set-upstream origin new-branch\n</code></pre>"},{"location":"Local/git-cli/configuration-global/","title":"Git Configuration (Global)","text":"<p>When you initialize a git repository, or work on an existing repository, by default there is no need for additional configuration. All configurations on this page are optional.</p>"},{"location":"Local/git-cli/configuration-global/#core-configurations","title":"Core configurations","text":"<p>Global configurations are the ones that apply to all repositories.</p> <p>Note: You can always use any of the global configuration <code>locally</code> by removing <code>--global</code> flag from the command.</p> <pre><code># Set the user name associated with all local commits\ngit config --global user.name \"First Last\"\n\n# Set the user email associated with all local commits\ngit config --global user.email \"username@gmail.com\"\n\n# View all of the global configurations set\ngit config --list\n\n# To manually edit global configuration, edit the config file directly\nnano ~/.gitconfig\n</code></pre>"},{"location":"Local/git-cli/configuration-global/#alias","title":"alias","text":"<p>This configuration allows you to create shortcuts for Git commands. For example, you can create an alias <code>co</code> for the <code>checkout</code> command.</p> <pre><code>git config --global alias.co checkout\n</code></pre>"},{"location":"Local/git-cli/configuration-global/#pullrebase","title":"pull.rebase","text":"<p>This configuration determines whether git pull performs a <code>rebase</code> (true) instead of a <code>merge</code> (false) by default.</p> <p>The default merge means that when you pull changes from a remote repository, Git will create a <code>new merge commit</code> in your local repository that combines your changes with the changes from the remote repository.</p> <p>However, this can lead to a <code>cluttered commit history</code>, especially if you're working on a feature branch and frequently pulling in changes from the main branch. Every time you pull, a new merge commit is created, which can make it harder to understand the history of your feature branch.</p> <pre><code>git config --global pull.rebase true\n</code></pre> <p>Setting <code>pull.rebase</code> to <code>true</code> changes the default behavior of <code>git pull</code> to perform a rebase operation instead of a merge. This means that when you pull changes from a remote repository, Git will replay your changes on top of the changes from the remote repository, instead of creating a new merge commit. This can result in a cleaner and more linear commit history.</p> <p>If there are <code>uncommitted</code> local changes, and there is <code>no conflict</code>, Git will perform the rebase operation successfully. Your local changes will still be present after the pull operation.</p> <p>If there are <code>uncommitted</code> local changes, and there is <code>a conflict</code>, Git will not be able to perform the rebase and will give you an error message. You will need to either commit or <code>stash</code> your changes before you can pull the remote changes.</p> <p><code>Untracked</code> files are not affected by git pull, regardless of the pull.rebase setting. They remain untracked unless you add them to the staging area.</p>"},{"location":"Local/git-cli/configuration-global/#pullff","title":"pull.ff","text":"<p>Determines how git pull operates when there are <code>no conflicts</code> between the local and remote branches. The <code>pull.ff</code> configuration can be set to <code>true</code>, <code>false</code>, or <code>only</code>.</p> <pre><code>git config --global pull.ff only\n</code></pre> <p>When <code>pull.ff</code> is set to <code>only</code>, Git will only allow fast-forward merges when performing a <code>git pull</code>. A <code>fast-forward merge</code> is possible when the tip of the branch you are trying to merge into is a direct ancestor of the branch you want to merge. In other words, the branches have not diverged.</p> <p>If a fast-forward merge is not possible (i.e., the branches have diverged), Git will not perform the merge and will give you an error message. This can be useful if you want to ensure that your commit history is linear and avoid merge commits.</p>"},{"location":"Local/git-cli/configuration-global/#pushdefault","title":"push.default","text":"<p>This configuration determines the default behavior of <code>git push</code>. The <code>simple</code> mode, which is the <code>default</code> in recent versions of Git, only pushes the current branch to its corresponding upstream branch.</p> <p>Changing the default push behavior can be useful in different scenarios. For example, if you're working on a feature branch and you want to ensure that you only push changes to that branch, you might set <code>push.default</code> to <code>current</code>. This way, when you run <code>git push</code> without specifying a branch, Git will only push the current branch.</p> <pre><code>git config --global push.default simple\n</code></pre> <p>Other options include:</p> <ul> <li>nothing: This option will not push anything unless you specify the branch you want to push.  </li> <li>current: This option pushes the current branch to its upstream branch.  </li> <li>upstream: This option pushes the current branch to its upstream branch, but it also requires the upstream branch to be set.  </li> <li>simple (default in recent versions of Git): This option is similar to upstream, but it also verifies that the upstream branch has the same name as the local one.  </li> <li>matching (default in older versions of Git): This option pushes all branches that have the same name in both local and remote repositories.</li> </ul>"},{"location":"Local/git-cli/configuration-global/#colorui","title":"color.ui","text":"<p>This configuration enables color output in the terminal for Git commands.</p> <pre><code>git config --global color.ui auto\n</code></pre>"},{"location":"Local/git-cli/configuration-global/#credentialhelper","title":"credential.helper","text":"<p>This configuration allows Git to remember your credentials for a certain period of time so you don't have to enter them every time you perform a remote operation.</p> <pre><code>git config --global credential.helper cache\n</code></pre>"},{"location":"Local/git-cli/configuration-global/#core-configurations_1","title":"Core configurations","text":""},{"location":"Local/git-cli/configuration-global/#coreeditor","title":"core.editor","text":"<p>The <code>core.editor</code> configuration in Git is used to specify the default text editor that Git will use when it needs to open a file for you to edit. This typically happens in several situations:</p> <ul> <li>Commit Messages: When you run git commit without the -m option, Git opens the default text editor for you to write a commit message.  </li> <li>Interactive Rebase: During an interactive rebase (git rebase -i), Git opens the default text editor for you to choose which commits to modify, squash, reword, etc.  </li> <li>Merge Conflicts: When a merge conflict occurs, Git opens the default text editor for you to resolve the conflict.  </li> </ul> <pre><code>git config --global core.editor \"vim\"\ngit config --global core.editor \"code\"\n</code></pre> <p>By setting this configuration, you can choose an editor that you are comfortable with. If you don't set this configuration, Git will use the system's default editor, which is usually Vim or Nano on Unix-based systems, and may not be the editor you are most comfortable with.</p>"},{"location":"Local/git-cli/configuration-global/#difftool","title":"diff.tool","text":"<p>Configure the tool to be used to view diffs.</p> <pre><code>git config --global diff.tool \"your_diff_tool\"\ngit config --global diff.tool \"vim\"\n</code></pre>"},{"location":"Local/git-cli/configuration-global/#mergetool","title":"merge.tool","text":"<p>Configure the tool to be used for merge conflicts.</p> <pre><code>git config --global merge.tool \"your_merge_tool\"\ngit config --global merge.tool \"vim\"\n</code></pre>"},{"location":"Local/git-cli/configuration-global/#pullrebase-vs-pullff","title":"pull.rebase vs pull.ff","text":"<p>The <code>pull.rebase</code> and <code>pull.ff</code> control different aspects of the git operation.</p> <ol> <li> <p><code>pull.rebase</code>: When set to <code>true</code>, this configuration changes the behavior of <code>git pull</code> to perform a rebase instead of a merge. This means that when you pull changes from a remote repository, Git will replay your local commits on top of the changes from the remote repository, instead of creating a new merge commit. This can result in a cleaner and more linear commit history.</p> </li> <li> <p><code>pull.ff</code>: When set to <code>only</code>, this configuration changes the behavior of <code>git pull</code> to only allow fast-forward merges. A fast-forward merge is possible when the tip of the branch you are trying to merge into is a direct ancestor of the branch you want to merge. If a fast-forward merge is not possible (i.e., the branches have diverged), Git will not perform the merge and will give you an error message.</p> </li> </ol> <p><code>pull.rebase true</code> and <code>pull.ff only</code> both aim to maintain a clean and linear commit history, but they do so in different ways. <code>pull.rebase true</code> does this by avoiding merge commits, while <code>pull.ff only</code> does this by only allowing fast-forward merges.</p>"},{"location":"Local/git-cli/configuration-global/#references","title":"References","text":"<ul> <li>https://www.moncefbelyamani.com/first-things-to-configure-before-using-git/</li> </ul>"},{"location":"Local/git-cli/gh/","title":"gh","text":"<p><code>gh</code> is GitHub on the command line. It brings pull requests, issues, and other GitHub concepts to the terminal next to where you are already working with <code>git</code> and your code.</p>"},{"location":"Local/git-cli/gh/#getting-started","title":"Getting started","text":""},{"location":"Local/git-cli/gh/#installation","title":"Installation","text":"<pre><code>brew install gh\n</code></pre>"},{"location":"Local/git-cli/gh/#common-commands","title":"Common commands","text":"<pre><code># Lists PR associated with current branch, PRs created by you, PRs requesting review from you\ngh pr status\n</code></pre>"},{"location":"Local/git-cli/gh/#working-with-copilot","title":"Working with Copilot","text":"<pre><code># Installing Copilot in the CLI\ngh auth login\ngh extension install github/gh-copilot\ngh extension upgrade gh-copilot\n\n# Usage\ngh copilot SUBCOMMAND\ngh copilot SUBCOMMAND --help\n\n# Explain\ngh copilot explain\ngh copilot explain \"sudo apt-get\"\n\n# Suggest\ngh copilot suggest\ngh copilot suggest \"Install git\"\n</code></pre>"},{"location":"Local/git-cli/gh/#resources","title":"Resources","text":"<ul> <li>GitHub CLI</li> <li>GitHub CLI manual</li> <li>About GitHub Copilot in the CLI</li> </ul>"},{"location":"Local/git-cli/git-clone/","title":"Git Clone","text":"<p>When developers perform a git clone depth 1operation, the only thing they pull back from the remote repository is the latest commit on the specific git branch of interest. By default, it\u2019s the master branch that gets cloned and checked out.</p> <pre><code>$ git clone --depth 1 https://github.com/&lt;user&gt;/&lt;project&gt;.git\n\n$ git clone --depth 1 --branch release https://github.com/&lt;user&gt;/&lt;project&gt;.git\n</code></pre> <p>It\u2019s useful for developers to have a complete git history, along with the ability to inspect any git branch used for parallel feature development when programming. But continuous integration builds, and automation scripts rarely need a commit history. They usually only need one commit on a specific branch, along with any associated git submodules. Hence, that\u2019s what a git clone depth 1 provides.</p>"},{"location":"Local/git-cli/git-clone/#resources","title":"Resources","text":"<ul> <li>How and when to perform a git clone depth 1 example | TheServerSide blog</li> </ul>"},{"location":"Local/git-cli/git-fetch-vs-pull/","title":"Git Fetch vs Pull","text":"<p><code>git fetch</code> is used to retrieve the latest changes from the remote repository, but it does not merge them with your local branch. This command fetches all the branches from the remote repository, along with all the commits and files that have been updated. However, it does not modify your local working branch. After fetching, you can inspect the changes and decide whether to merge them into your local branch.</p> <p>The <code>git pull</code> command is a combination of two other Git commands: <code>git fetch</code> and <code>git merge</code>. It fetches changes from the remote repository and automatically merges them into the current local branch.</p>"},{"location":"Local/git-cli/git-fetch-vs-pull/#usage","title":"Usage","text":"<pre><code># Fetch all the changes from the origin remote repository\ngit fetch origin\n\n# Fetch and merge all the changes from the origin remote repository\ngit pull origin main\n</code></pre>"},{"location":"Local/git-cli/git-fetch-vs-pull/#storage","title":"Storage","text":"<p>The fetched changes are stored in a separate branch in your local repository. This branch is usually a remote-tracking branch, which tracks the state of the remote repository. For example, if your current branch is <code>main</code>, it'll store the retrieved changes in the <code>origin/main</code> branch in your local repository. This <code>origin/main</code> branch is a remote-tracking branch for the <code>main</code> branch on the <code>origin</code> remote.</p> <p>You can view these changes by checking out the remote-tracking branch with <code>git checkout origin/main</code>, or you can compare the changes with your local branch using <code>git diff main origin/main</code>.</p> <p>Remember, these changes are not merged into your local <code>main</code> branch until you run <code>git merge origin/main</code> or <code>git pull origin main</code>.</p>"},{"location":"Local/git-cli/git-fetch-vs-pull/#references","title":"References","text":"<ul> <li>Git Pull and Git Fetch: Understanding the Differences</li> </ul>"},{"location":"Local/git-cli/git-lfs/","title":"Git LFS","text":"<p>A Git extension for versioning large files (even couple GB in size).</p> <p>Git Large File Storage (LFS) <code>replaces</code> large files such as audio samples, videos, datasets, and graphics with <code>text pointers</code> inside Git, while storing the file contents on a remote server like GitHub.com or GitHub Enterprise.</p>"},{"location":"Local/git-cli/git-lfs/#usage","title":"Usage","text":"<p>First, you need to install Git LFS, then specify which files should be tracked by Git LFS. You can commit and push files to your repository as you normally would with Git. Git LFS <code>automatically replaces</code> the large files with text pointers and uploads the file contents to the Git LFS server.</p> <p>When you clone a repository with LFS files or pull changes that include LFS files, Git LFS downloads the file contents <code>as needed</code>.</p> <pre><code># Install Git LFS\ngit lfs install\n\n# Specify files to be tracked\n# This command creates or updates a .gitattributes file with the paths of files that should be tracked by Git LFS.\ngit lfs track \"*.jpg\"\n\n# Commit large file\ngit add file.jpg\ngit commit -m \"Add large file\"\ngit push origin main\n</code></pre>"},{"location":"Local/git-cli/git-lfs/#custom-server","title":"Custom server","text":"<pre><code># Set custom lfs server url\ngit config lfs.url \"https://your-lfs-server.com\"\n</code></pre> <p>If your Git LFS server requires authentication, you'll need to provide your credentials. Git LFS uses the same credentials as Git itself, so if you're already authenticated with Git, you should be able to use Git LFS without any additional steps.</p> <p>Alternatively, you can use a credential helper to store your username and password.</p> <pre><code># Set the credential helper\ngit config --global credential.helper store\n</code></pre> <p>The next time you push or pull from the server, Git will ask for your <code>username</code> and <code>password</code>, and then it will store them for future use.</p> <p>The <code>credential.helper</code> command sets the credential helper to <code>store</code>, which stores your credentials on disk in a plain text file. If you want a more secure method, you can use <code>cache</code> (which stores credentials in memory for a certain period of time) or <code>osxkeychain</code> (on macOS, which stores credentials in the secure keychain).</p>"},{"location":"Local/git-cli/git-lfs/#references","title":"References","text":"<ul> <li>git-lfs</li> <li>Git LFS Wiki</li> <li>man docs</li> <li>Details of how the Git LFS client works are in the official specification.</li> <li>Details of how the GIT LFS server works are in the API specification.</li> </ul>"},{"location":"Local/git-cli/git-merge-strategies/","title":"Merge Strategies","text":"<p>What merge actually does?</p>"},{"location":"Local/git-cli/git-merge-strategies/#references","title":"References","text":"<ul> <li>Git Merge | GitKraken</li> <li>Git Merge | Atlassian</li> <li>Merge strategies and squash merge | Microsoft</li> <li>Should You Squash Merge or Merge Commit?</li> <li>A Comprehensive Guide To Git Merge</li> </ul>"},{"location":"Local/git-cli/git-merge-vs-rebase/","title":"Merge vs Rebase","text":"<p>Your <code>main</code> branch should be the only source of truth for your application.</p> <p>Generally, you'll create a new branch off of your main branch, add new features in your local branch, create a pull request to the main branch, and then merge your code. When you're pushing new code to you main branch, you'd want to <code>merge</code> your changes in the main branch. This is because you are creating new commits.</p> <p>Every once in a while, before you merge your changes in main branch, you want your local branch to be up to date with the <code>main</code> branch. You local branch can get behind the main branch when more than one developer is working on the project. To catch up your local branch with the new commits in main branch, you'd use a strategy called <code>rebase</code>. You should never merge commits from your main branch to your local branch, because those commits were already accepted in your main branch, and your main branch is supposed to be the only source of truth. </p>"},{"location":"Local/git-cli/git-merge-vs-rebase/#git-merge","title":"Git Merge","text":"<p>In an ideal world you wouldn't normally need to merge commits via CLI. You'll normally merge to the main branch, and do that via pull requests. However if you do need to merge, here is how to do it.</p> <pre><code>$ git checkout &lt;destination_branch&gt;\n$ git merge &lt;source_branch&gt;\n$ git reflog\n$ git push origin &lt;destination_branch&gt;\n</code></pre>"},{"location":"Local/git-cli/git-merge-vs-rebase/#git-rebase","title":"Git Rebase","text":"<p>If you want to pull the latest commits from main branch and then apply the commits from your local branch on top of them, use rebase. </p> <p>Generally you'd use this before creating a pull request, to make sure it does not end up in a conflict, and that your code still runs as expected after taking the latest code from main branch. You do this for a fact that the code in main branch is the application code that has been accepted in production, so it is your responsibility to make sure that the code in local branch works as expected with the latest code in main branch.</p> <pre><code>$ git rebase &lt;main_branch&gt; &lt;local_branch&gt;\n\n# Default push (without force) will be rejected\n$ git push origin --force\n</code></pre> <p>Rebase does change your local commit id(s). It actually deleted the old commits, rebased main branch on your local branch, and then wrote brand new commits with new commit id(s).</p> <p>Note: if you rebase and try to push to GitLab or GitHub, the server won\u2019t allow the operation to be performed. To rebase to GitHub or GitLab, a developer must add the <code>\u2013force</code> switch to the git push command to compel the changes to be accepted.</p>"},{"location":"Local/git-cli/git-merge-vs-rebase/#references","title":"References","text":"<ul> <li>How to Git rebase a branch to master by example blog</li> </ul>"},{"location":"Local/git-cli/git-reflog-vs-log/","title":"Reflog vs Log","text":"<p>To view the log is to view the commit history.</p> <pre><code>$ git log --oneline\n$ git log --pretty=oneline\n$ git log --graph --all --oneline\n</code></pre> <p><code>log</code> is a public accounting of the repository's commit history while the <code>reflog</code> is a private, workspace-specific accounting of the repo's local commits.</p>"},{"location":"Local/git-cli/git-reflog-vs-log/#reflog","title":"Reflog","text":"<p>The <code>reflog</code> is a file found in <code>.git\\logs\\refs\\heads</code> that tracks the history of local commits for a given branch and excludes any commits that were potentially pruned away through Git garbage collection routines.</p> <pre><code>$ git reflog\n</code></pre>"},{"location":"Local/git-cli/git-reflog-vs-log/#references","title":"References","text":"<ul> <li>Git reflog vs. log: How these commit history tools differ blog</li> </ul>"},{"location":"Local/git-cli/installation/","title":"Git Installation","text":""},{"location":"Local/git-cli/installation/#verify","title":"Verify","text":"<pre><code>$ git --version\n\n# If git is installed locally, you'll see something like \n# git version 2.24.3 (Apple Git-128)\n</code></pre>"},{"location":"Local/git-cli/installation/#installation","title":"Installation","text":"<p>If you have XCode installed on Mac, Git may already be installed.</p> <pre><code>Option 1: (preferred)\nIf you have brew (Homebrew) installed already\n$ brew install git\n\nOption 2:\nIf not, download and run the installation from http://git-scm.com/download/mac\n\nLinux\n=====\n$ sudo dnf install git                // RPM-based distribution like Fedora, RHEL or CentOS\n$ sudo yum install git                // for older versions of Fedora\n$ sudo apt-get install git            // Debian-based distribution like Ubuntu\nThese are basic package management tools that comes pre-installed\n</code></pre>"},{"location":"Local/git-cli/sub-module/","title":"Git SubModule","text":"<p>It often happens that while working on one project, you need to use another project from within it. Perhaps it\u2019s a library that a third party developed or that you\u2019re developing separately and using in multiple parent projects. A common issue arises in these scenarios: you want to be able to treat the two projects as separate yet still be able to use one from within the other.</p> <p>Git addresses this issue using submodules. <code>Submodules</code> allow you to keep a Git repository as a subdirectory of another Git repository. This lets you clone another repository into your project and keep your commits separate.</p> <p>Submodules are registered in a <code>.gitmodules</code> files in the project repository.</p> <pre><code># Add a git submodule\ngit submodule add &lt;module_repo_path&gt;\n\n# Remove a git submodule\ngit submodule deinit &lt;module_name&gt;\n</code></pre>"},{"location":"Local/git-cli/sub-module/#references","title":"References","text":"<ul> <li>7.11 Git Tools - Submodules</li> <li>git-submodule command doc</li> </ul>"},{"location":"Local/homebrew/","title":"Homebrew","text":"<p>Official Site</p> <p>The missing MacOS package manager. Installs common libraries and utilities not bundled with OSX. Typically these are development related libraries. Installation is dependent on <code>XCode command line</code> tools (large dependency) and <code>ruby</code> to be installed.</p> <p>Homebrew typically deals with command line software. Most of the software are distributed under open source license. Default package install location is <code>/usr/local</code>.</p>"},{"location":"Local/homebrew/#installation","title":"Installation","text":"<pre><code>$ ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\nOR\n$ /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n\n# Make sure the following is set in your path\n$ export PATH=\"/usr/local/bin:$PATH\"\n</code></pre>"},{"location":"Local/homebrew/#verify","title":"Verify","text":"<pre><code>$ brew --version\n</code></pre>"},{"location":"Local/homebrew/#troubleshooting","title":"Troubleshooting","text":"<pre><code>$ brew doctor\n# Use this command after system changes (such as OS X or Xcode updates) or if you think you need to troubleshoot your\n# Homebrew installation. The output will tell you what\u2019s wrong and generally how to fix it.\n\n# Updates your Homebrew formula list in local with latest version available\n$ brew update\n# Note, this doesn\u2019t update all your installed packages!\n# What this does is update the versions and availability of packages you could install.\n# You want to run this often to keep your Homebrew installation up to date with the latest available software.\n\n# Upgrade all locally installed packages\n$ brew upgrade\n</code></pre>"},{"location":"Local/homebrew/#commands","title":"Commands","text":"<pre><code># Search all packages containing the keyword\n# It also lists available versions for exact match\n$ brew search &lt;keyword&gt;\n\n# Install a software package on local system.\n$ brew install [insert package name here]\n\n# list all the packages currently installed on your system\n$ brew list\n\n# After you install a package, you get a screenful of information or configuration steps.\n# This will let you see again.\n$ brew info [insert package name here]\n\n# Remove a software package from the local system.\n$ brew remove [insert package name here]\n\n# Remove a package you previously installed.\n$ brew uninstall [insert package name here]\n\n# Start/Stop a service\n$ brew services start [service name]\n$ brew services start tomcat\n$ catalina --version\n</code></pre>"},{"location":"Local/homebrew/#add-ons-cask","title":"Add-ons : Cask","text":"<p><code>brew cask</code> is an extension that allows management of graphical applications through Cask project. Search Cask packages here.</p>"},{"location":"Local/homebrew/#installation_1","title":"Installation","text":"<pre><code># Install Homebrew-Cask\n$ brew tap caskroom/cask\n\n# List packages installed by Cask\n$ brew cask list\n\n# Install a package via Cask\n$ brew cask install &lt;package&gt;\n\n# Installs a package again via Cask\n$ brew cask reinstall &lt;package&gt;\n\n# Uninstall an installed package via Cask\n$ brew cask uninstall &lt;package&gt;\n</code></pre>"},{"location":"Local/homebrew/#searching-casks","title":"Searching Casks","text":"<p>Go here. Type <code>t</code> and then the name of the package you are looking for. It'll filter the formula.</p>"},{"location":"Local/homebrew/#install-a-specific-version","title":"Install a specific version","text":"<ol> <li>Uninstall the package</li> <li>Go to the *.rb file on Github</li> <li>Get the file history</li> <li>Open the complete within the hash of the version you are looking to install     Example: https://github.com/caskroom/homebrew-cask/blob/e8816187ae43f52b598f15f45b3453e22727ac99/Casks/virtualbox.rb</li> <li>Install just like any other package</li> </ol> <p>Original source here.</p>"},{"location":"Local/homebrew/#add-ons-tap","title":"Add-ons : Tap","text":"<p><code>brew tap</code> allows Homebrew to tap into another repository of formulae. Taps are external sources of Homebrew formulae and/or external commands. They can be created by anyone to provide their own formulae and/or external commands to any Homebrew user.</p> <pre><code># List tapped repositories\n$ brew tap\n\n# Add tap\n$ brew tap &lt;tapname&gt;\n\n# Remove a tap\n$ brew untap &lt;tapname&gt;\n</code></pre>"},{"location":"Local/homebrew/#common-issues","title":"Common issues","text":""},{"location":"Local/homebrew/#permissions-issue","title":"Permissions issue","text":"<p>Sometimes when you switch users on the same machine, you may run into permissions issue. If you're an admin, run the following command to set permissions to the current logged in user</p> <pre><code>sudo chown -R $(whoami) $(brew --prefix)/*\n</code></pre>"},{"location":"Local/homebrew/brew_packages/","title":"Cheatsheet - Brew packages","text":""},{"location":"Local/homebrew/brew_packages/#cmatrix","title":"cmatrix","text":"<p>Shows matrix screen on terminal</p> <pre><code>$ brew install cmatrix\n$ cmatrix\n</code></pre>"},{"location":"Local/homebrew/brew_packages/#asciiquarium","title":"asciiquarium","text":"<pre><code>$ brew install asciiquarium\n$ asciiquarium\n</code></pre>"},{"location":"Local/homebrew/brew_packages/#toilet","title":"toilet","text":"<p>Turn any text into ascii character art</p> <pre><code>$ brew install toilet\n$ toilet &lt;text&gt;\n</code></pre>"},{"location":"Local/homebrew/brew_packages/#tetris","title":"tetris","text":"<pre><code>$ brew install samtay/tui/tetris\n$ tetris\n</code></pre>"},{"location":"Local/nginx-cli/","title":"Nginx Setup","text":""},{"location":"Local/nginx-cli/#installation","title":"Installation","text":"<pre><code>Option 1\n$ brew install nginx\n</code></pre>"},{"location":"Local/nginx-cli/#verification","title":"Verification","text":"<pre><code>/usr/local/var/www                            // Docroot\n/usr/local/etc/nginx/nginx.conf               // Default port set to 8080\n/usr/local/etc/nginx/servers/                 // nginx loads all the files here\n/usr/local/Cellar/nginx/1.12.2_1              // Install location\nnginx                                         // Start the server\nbrew services start nginx                     // Start the server as a background service\n\nOnce you start the server, navigate to\nhttp://localhost:8080\n</code></pre>"},{"location":"Local/terminal-bash/","title":"Terminal (Mac)","text":"<p>brew package manager</p> <ol> <li>Shells</li> <li>bash script</li> <li>Tips - Handling Files &amp; Directories</li> <li>Tips - Keyboard Shortcuts</li> <li>Kill a process</li> <li>Using alias</li> <li>Using curl</li> <li>Using find</li> <li>Using grep</li> <li>Cheatsheet</li> <li>Linux Commands by Alex Xu</li> </ol>"},{"location":"Local/terminal-bash/#plugins","title":"Plugins","text":""},{"location":"Local/terminal-bash/#auto-completion","title":"Auto Completion","text":"<pre><code># Bash version\n$ bash --version\n\n# Auto Completion\n## If running Bash 3.2 included with macOS\n$ brew install bash-completion\n## or, if running Bash 4.1+\n$ brew install bash-completion@2\n</code></pre>"},{"location":"Local/terminal-bash/#working-with-json","title":"Working with JSON","text":"<p>Bash doesn't ship with supporting JSON out of the box. If you do not have a command line utility (jq), install it</p> <pre><code>$ brew install jq\n</code></pre>"},{"location":"Local/terminal-bash/#examples","title":"Examples","text":"<pre><code># Read and use JSON in bash\n$ echo '{\"a\": {\"b\":123 } }' | jq '.a.b'    # returns 123 as output\n\n# Read API response\n$ curl https://swapi.co/api/people/2 | jq    # Pretty print JSON\n\n# Pass a JSON file to jq for filtering content\n$ jq '.dependencies | keys' package.json\n</code></pre>"},{"location":"Local/terminal-bash/about-commands/","title":"About Commands","text":""},{"location":"Local/terminal-bash/about-commands/#file-system-commands","title":"File System Commands","text":""},{"location":"Local/terminal-bash/about-commands/#cat","title":"cat","text":"<p>cat is a standard Unix utility that reads files sequentially, writing them to standard output. The name is derived from its function to <code>(con)catenate</code> files</p>"},{"location":"Local/terminal-bash/about-commands/#chattr","title":"chattr","text":"<p>chattr allows a user to set certain attributes of a file. <code>lsattr</code> is the command that displays the attributes of a file.</p>"},{"location":"Local/terminal-bash/about-commands/#chmod","title":"chmod","text":"<p>chmod - <code>change mode</code> is the command and system call used to change the access permissions and the special mode flags (the setuid, setgid, and sticky flags) of file system objects (files and directories).</p>"},{"location":"Local/terminal-bash/about-commands/#chown","title":"chown","text":"<p>chown - <code>change owner</code> is used to change the owner of file system files, directories. Unprivileged (regular) users who wish to change the group membership of a file that they own may use chgrp.</p>"},{"location":"Local/terminal-bash/about-commands/#cksum","title":"cksum","text":"<p>cksum generates a <code>checksum</code> value for a file or stream of data. The cksum command reads each file given in its arguments, or standard input if no arguments are provided, and outputs the file's 32-bit <code>cyclic redundancy check</code> (CRC) checksum and byte count. The CRC output by cksum is different from the CRC-32 used in zip, PNG and zlib.</p>"},{"location":"Local/terminal-bash/about-commands/#dd","title":"dd","text":"<p>dd command, the primary purpose of which is to convert and copy files.</p>"},{"location":"Local/terminal-bash/about-commands/#du","title":"du","text":"<p>du, <code>disk usage</code> is used to estimate file space usage \u2014 space used under a particular directory or files on a file system.</p>"},{"location":"Local/terminal-bash/about-commands/#df","title":"df","text":"<p>df, <code>disk free</code> is used to display the amount of available disk space for file systems on which the invoking user has appropriate read access. df is typically implemented using the <code>statfs</code> or <code>statvfs</code> system calls.</p>"},{"location":"Local/terminal-bash/about-commands/#file","title":"file","text":"<p>file is for recognizing the type of data contained in a computer file. It can even tell you the uncompressed byte size of a gzipped file.</p>"},{"location":"Local/terminal-bash/about-commands/#fuser","title":"fuser","text":"<p>fuser is used to show which processes are using a specified computer file, file system, or Unix socket. The command displays the process identifiers (PIDs) of processes using the specified files or file systems. In the default display mode, each PID is followed by a letter denoting the type of access.</p>"},{"location":"Local/terminal-bash/about-commands/#ln","title":"ln","text":"<p>ln is used to create a hard link or a symbolic link (<code>symlink</code>) to an existing file or directory. The use of a hard link allows multiple filenames to be associated with the same file since a hard link points to the inode of a given file, the data of which is stored on disk. On the other hand, symbolic links are special files that refer to other files by name.</p>"},{"location":"Local/terminal-bash/about-commands/#ls","title":"ls","text":"<p>ls is used to list computer files and directories in Unix and Unix-like operating systems.</p>"},{"location":"Local/terminal-bash/about-commands/#pax","title":"pax","text":"<p>pax is an archiving utility available for various operating systems. Rather than sort out the incompatible options that have crept up between tar and cpio, along with their implementations across various versions of Unix, the IEEE designed new archive utility pax that could support various archive formats with useful options from both archivers. The new pax format which is basically tar with additional extended attributes.</p>"},{"location":"Local/terminal-bash/about-commands/#split","title":"split","text":"<p>split is most commonly used to split a computer file into two or more smaller files.</p>"},{"location":"Local/terminal-bash/about-commands/#touch","title":"touch","text":"<p>touch is used to update the access date and/or modification date of a computer file or directory.</p>"},{"location":"Local/terminal-bash/about-commands/#type","title":"type","text":"<p>type is a command that describes how its arguments would be interpreted if used as command names. Where applicable, type will display the command name's path. Possible command types are:</p> <ul> <li>shell built-in</li> <li>function</li> <li>alias</li> <li>hashed command</li> <li>keyword</li> </ul>"},{"location":"Local/terminal-bash/about-commands/#process-commands","title":"Process Commands","text":""},{"location":"Local/terminal-bash/about-commands/#at","title":"at","text":"<p>at is used to schedule commands to be executed once, at a particular time in the future. It differs from cron, which is used for recurring executions.</p>"},{"location":"Local/terminal-bash/about-commands/#bg","title":"bg","text":"<p>bg - The POSIX standard specifies two commands for resuming suspended jobs in the background and foreground, respectively <code>bg</code> and <code>fg</code>.</p>"},{"location":"Local/terminal-bash/about-commands/#cron","title":"cron","text":"<p>cron is a job scheduler used to run job(s) periodically at fixed times, dates, or intervals. It typically automates system maintenance or administration.</p>"},{"location":"Local/terminal-bash/about-commands/#kill","title":"kill","text":"<p>kill is used to send signals to running processes. By default, the message sent is the termination signal, which requests that the process exit. Sometimes the signal sent may have nothing to do with process killing.</p>"},{"location":"Local/terminal-bash/about-commands/#lsof","title":"lsof","text":"<p>lsof is a command meaning \"<code>list open files</code>\", which is used in many Unix-like systems to report a list of all open files and the processes that opened them.</p>"},{"location":"Local/terminal-bash/about-commands/#ps","title":"ps","text":"<p>The ps program (short for \"<code>process status</code>\") displays the currently-running processes. A related Unix utility named <code>top</code> provides a real-time view of the running processes.</p>"},{"location":"Local/terminal-bash/about-commands/#time","title":"time","text":"<p>time is used to determine the duration of execution of a particular command.</p>"},{"location":"Local/terminal-bash/about-commands/#top","title":"top","text":"<p>top (<code>table of processes</code>) is a task manager program, found in many Unix-like operating systems, that displays information about CPU and memory utilization.</p>"},{"location":"Local/terminal-bash/about-commands/#search-commands","title":"Search Commands","text":""},{"location":"Local/terminal-bash/about-commands/#find","title":"find","text":"<p>find is a command-line utility that locates files based on some user-specified criteria and either prints the pathname of each matched object or, if another action is requested, performs that action on each matched object.</p>"},{"location":"Local/terminal-bash/about-commands/#grep","title":"grep","text":"<p>grep is a command-line utility for searching plain-text data sets for lines that match a regular expression.</p>"},{"location":"Local/terminal-bash/about-commands/#shell-builtin-commands","title":"Shell builtin Commands","text":""},{"location":"Local/terminal-bash/about-commands/#alias","title":"alias","text":"<p>alias enables a replacement of a word by another string. It is mainly used for abbreviating a system command, or for adding default arguments to a regularly used command.</p>"},{"location":"Local/terminal-bash/about-commands/#test","title":"test","text":"<p>test evaluates conditional expressions.</p>"},{"location":"Local/terminal-bash/about-commands/#wait","title":"wait","text":"<p>wait pauses until execution of a background process has ended.</p>"},{"location":"Local/terminal-bash/about-commands/#software-development-commands","title":"Software Development Commands","text":""},{"location":"Local/terminal-bash/about-commands/#ctags","title":"ctags","text":"<p>ctags is a programming tool that generates an index (or tag) file of names found in source and header files of various programming languages to aid code comprehension.</p>"},{"location":"Local/terminal-bash/about-commands/#make","title":"make","text":"<p>make is a build automation tool that automatically builds executable programs and libraries from source code by reading files called Makefiles which specify how to derive the target program. Make can be used to manage any project where some files need to be updated automatically from others whenever the others change in addition to building programs.</p>"},{"location":"Local/terminal-bash/about-commands/#strip","title":"strip","text":"<p>strip program removes information from executable binary programs and object files that is not essential or required for normal and correct execution, thus potentially resulting in better performance and sometimes significantly less disk space usage. The resulting file is a stripped binary.</p>"},{"location":"Local/terminal-bash/about-commands/#text-processing-commands","title":"Text Processing Commands","text":""},{"location":"Local/terminal-bash/about-commands/#awk","title":"awk","text":"<p>awk is a domain-specific language designed for text processing and typically used as a data extraction and reporting tool. Like sed and grep, it is a filter.</p>"},{"location":"Local/terminal-bash/about-commands/#csplit","title":"csplit","text":"<p>csplit is a utility that is used to split a file into two or more smaller files determined by context lines. The patterns may be line numbers or regular expressions.</p> <p>The split command also splits a file into pieces, except that all the pieces are of a fixed size (measured in lines or bytes).</p>"},{"location":"Local/terminal-bash/about-commands/#cut","title":"cut","text":"<p>cut is used to extract sections from each line of input \u2014 usually from a file.</p>"},{"location":"Local/terminal-bash/about-commands/#diff","title":"diff","text":"<p>diff is a data comparison tool that computes and displays the differences between the contents of files. Unlike edit distance notions used for other purposes, diff is line-oriented rather than character-oriented, but it is like Levenshtein distance in that it tries to determine the smallest set of deletions and insertions to create one file from the other. The utility displays the changes in one of several standard formats, such that both humans or computers can parse the changes, and use them for patching.</p> <p>The operation of diff is based on solving the longest common subsequence problem.</p>"},{"location":"Local/terminal-bash/about-commands/#fold","title":"fold","text":"<p>fold is used for making a file with long lines more readable on a limited width computer terminal by performing a line wrap.</p>"},{"location":"Local/terminal-bash/about-commands/#head","title":"head","text":"<p>head is used to display the beginning of a text file or piped data.</p>"},{"location":"Local/terminal-bash/about-commands/#iconv","title":"iconv","text":"<p>iconv, <code>internationalization conversion</code> is used to convert between different character encodings. It can convert from any of the encodings to any other, through <code>Unicode conversion</code>. </p>"},{"location":"Local/terminal-bash/about-commands/#more","title":"more","text":"<p>more is a command to view (but not modify) the contents of a text file one screen at a time. Programs of this sort are called pagers. more is a very basic pager, originally allowing only forward navigation through a file, though newer implementations do allow for limited backward movement.</p>"},{"location":"Local/terminal-bash/about-commands/#nl","title":"nl","text":"<p>nl is a Unix utility for <code>numbering lines</code>, either from a file or from standard input, reproducing output on standard output.</p>"},{"location":"Local/terminal-bash/about-commands/#sed","title":"sed","text":"<p>sed, <code>stream editor</code> is a Unix utility that parses and transforms text, using a simple, compact programming language.</p>"},{"location":"Local/terminal-bash/about-commands/#sort","title":"sort","text":"<p>sort prints the lines of its input or concatenation of all files listed in its argument list in sorted order. Sorting is done based on one or more sort keys extracted from each line of input. By default, the entire input is taken as sort key. Blank space is the default field separator. The command supports a number of command-line options that can vary by implementation.</p>"},{"location":"Local/terminal-bash/about-commands/#strings","title":"strings","text":"<p>strings finds and prints text strings embedded in binary files such as executables. It can be used on object files and core dumps.</p>"},{"location":"Local/terminal-bash/about-commands/#tail","title":"tail","text":"<p>tail is used to display the tail end of a text file or piped data.</p>"},{"location":"Local/terminal-bash/about-commands/#uniq","title":"uniq","text":"<p>uniq is a utility command, when fed a text file or standard input, outputs the text with adjacent identical lines collapsed to one, unique line of text. The command is a kind of filter program. Typically it is used after sort. It can also output only the duplicate lines (with the <code>-d</code> option), or add the number of occurrences of each line (with the <code>-c</code> option).</p>"},{"location":"Local/terminal-bash/about-commands/#wc","title":"wc","text":"<p>wc, <code>word count</code> generates one or more of the following statistics: newline count, word count, and byte count. If a list of files is provided, both individual file and total statistics follow.</p>"},{"location":"Local/terminal-bash/about-commands/#xargs","title":"xargs","text":"<p>xargs, <code>extended arguments</code> command is used to build and execute commands from standard input. It converts input from standard input into arguments to a command.</p>"},{"location":"Local/terminal-bash/about-commands/#user-environment-commands","title":"User Environment Commands","text":""},{"location":"Local/terminal-bash/about-commands/#env","title":"env","text":"<p>env is used to either print a list of environment variables or run another utility in an altered environment without having to modify the currently existing environment. Using env, variables may be added or removed, and existing variables may be changed by assigning new values to them.</p>"},{"location":"Local/terminal-bash/about-commands/#exit","title":"exit","text":"<p>exit causes the shell or program to terminate. If performed within an interactive command shell, the user is logged out of their current session, and/or user's current console or terminal connection is disconnected. Typically an optional exit code can be specified, which is typically a simple integer value that is then returned to the parent process.</p>"},{"location":"Local/terminal-bash/about-commands/#uname","title":"uname","text":"<p>uname prints the name, version and other details about the current machine and the operating system running on it.</p>"},{"location":"Local/terminal-bash/about-commands/#who","title":"who","text":"<p>who displays a list of users who are currently logged into the computer.</p>"},{"location":"Local/terminal-bash/bash-script/","title":"Bash Script","text":""},{"location":"Local/terminal-bash/bash-script/#index","title":"Index","text":"<ol> <li>Sample Script clone branch to temp</li> <li>Sample Script count files</li> <li>Sample Script functions</li> <li>Sample Script init js</li> </ol>"},{"location":"Local/terminal-bash/bash-script/#tips","title":"Tips","text":""},{"location":"Local/terminal-bash/bash-script/#create-and-run-bash-scripts","title":"Create and Run bash scripts","text":"<pre><code># For a new script, just alone this would fail with permission denied\n$ ./script.sh\n\n# Change permissions for user to be able to execute the file\n$ chmod u+x script.sh\n\n# Or change permissions for all user to be able to execute the file\n$ chmod +x script.sh\n\n# You can pass parameters to the script following $1, $2, $3, . . order in the script\n</code></pre>"},{"location":"Local/terminal-bash/bash-script/#making-a-script-executable","title":"Making a script executable","text":"<pre><code>$ cp script.sh /usr/local/bin/&lt;command&gt;\n</code></pre>"},{"location":"Local/terminal-bash/bash-script/#exit-status-in-bash","title":"Exit status in bash","text":"<p>In bash, every script, command or function that runs, returns an exit status.</p> <pre><code>Exit status can range from 0 to 255\nSuccessful execution of a command will return 0\n\nExample:\n$ ls\n$ echo $?\n$ ls donotexist\n$ echo $?\n</code></pre>"},{"location":"Local/terminal-bash/bash-script/#syntax","title":"Syntax","text":""},{"location":"Local/terminal-bash/bash-script/#conditional-statements","title":"Conditional Statements","text":"<pre><code>if [[ -z myvar ]]; then              # Check is the variable is empty\n    echo \"true\"\nelif [[ -e script.sh ]]; then        # Check is file exists\n    echo \"true\"\nelif [[ 1 -eq 1 ]]; then             # Numeric equal check\n    echo \"true\"\nelif [[ $USER = 'anshul' ]]; then    # Any condition\n    echo \"true\"\nelse\n    echo \"false\"\nfi\n\nTernary Operation\n[[ $USER = 'anshul' ]] &amp;&amp; echo \"true\" || echo \"false\"\n</code></pre>"},{"location":"Local/terminal-bash/bash-script/#handling-variables","title":"Handling Variables","text":""},{"location":"Local/terminal-bash/bash-script/#setting-and-unsetting-an-environment-variable","title":"Setting and Unsetting an environment variable","text":"<pre><code>Bash sets a lot of environment variables.\n$ env           # list all the current environment variables\n$ var=12        # Sets a variable (not an environment variable)\n$ export var    # makes the variable visible to all child processes in context of current shell\n$ unset var     # unsets the variable\n</code></pre>"},{"location":"Local/terminal-bash/kill-process/","title":"Kill a running process","text":"<p>If you know the port already:</p> <pre><code>$ kill -9 $(lsof -ti:&lt;port&gt;)\n$ kill -9 $(lsof -ti:8080)\n</code></pre>","tags":["terminal"]},{"location":"Local/terminal-bash/kill-process/#finding-the-process","title":"Finding the process","text":"<p>To find the process running on port:</p> <pre><code># Find process id of a program running on port 3000\n$ lsof -i tcp:3000\n\n# Find process id of a program listening on port 8080\n$ lsof -i :8080 | grep LISTEN\n</code></pre>","tags":["terminal"]},{"location":"Local/terminal-bash/kill-process/#finding-the-command-running","title":"Finding the command running","text":"<pre><code># Find the command running on PID\n$ ps -ef &lt;PID&gt;\n$ ps -ef 42975\n</code></pre>","tags":["terminal"]},{"location":"Local/terminal-bash/kill-process/#kill-the-process","title":"Kill the process","text":"<pre><code># Kill the process by PID\n$ kill -9 &lt;PID&gt;\n</code></pre>","tags":["terminal"]},{"location":"Local/terminal-bash/kill-process/#using-netstat","title":"Using netstat","text":"<pre><code>$ netstat -vanp tcp | grep 3000\n</code></pre>","tags":["terminal"]},{"location":"Local/terminal-bash/using-alias/","title":"Using alias","text":""},{"location":"Local/terminal-bash/using-alias/#useful-aliases","title":"Useful Aliases","text":"<pre><code>$ alias git_sync=\"git pull -r &amp;&amp; git push\"\n$ alias ll=\"ls -laG\"\n\n$ export PATH=\"$PATH:~/scripts\"\n</code></pre>"},{"location":"Local/terminal-bash/using-alias/#docker-compose","title":"Docker Compose","text":"<pre><code>$ alias dc='docker-compose'\n$ alias dcu='docker-compose up -d'\n$ alias dcd='docker-compose down --remove-orphans'\n$ alias dcl='docker-compose logs'\n$ alias dce='docker-compose exec'\n</code></pre>"},{"location":"Local/terminal-bash/using-alias/#applying-changes","title":"Applying changes","text":"<p>To apply these changes to current terminal window:</p> <pre><code># If using bash shell\nsource ~/.bash_profile\n\n# If using ZSH shell\nsource ~/.zshrc\n</code></pre>"},{"location":"Local/terminal-bash/using-alias/#applying-changes-permanently","title":"Applying changes permanently","text":"<p>Bash profile is a good spot to put small reusable aliases, functions, variables that we want us to have available anywhere in bash.</p> <p>To make your changes permanent, depending on the shell you are using, put all temporary commands in <code>.bash_profile</code> or <code>.zshrc</code> file.</p>"},{"location":"Local/terminal-bash/using-alias/#forgot-what-alias-does","title":"Forgot what alias does?","text":"<pre><code>$ type &lt;alias&gt;\n$ type ll\n</code></pre>"},{"location":"Local/terminal-bash/using-curl/","title":"Using curl","text":"<p>Making HTTP requests with curl</p>"},{"location":"Local/terminal-bash/using-curl/#get-with-curl","title":"GET with curl","text":"<pre><code># The most basic GET call\n$ curl &lt;url&gt;\n$ curl https://google.com\n\n# Display Response Headers\n$ curl -i https://swapi.co/api/people/2\n\n# Add Headers to the request\n$ curl -H \"Authorization: Bearer 123\" localhost:3000/api/posts\n</code></pre>"},{"location":"Local/terminal-bash/using-curl/#post-with-curl","title":"POST with curl","text":"<pre><code>$ curl -X POST -H \"Content-Type: application/json\" -d '{\"title\": \"new post\"}' localhost:3000/api/posts\n\n# Display Response Headers\n$ curl -i -X POST --data-urlencode title=\"new post\" active=\"y\" localhost:3000/api/posts\n\n# Add Headers to the request\n# with line break for readibility\n$ curl -X POST \\\n       -H \"Content-Type: application/json\" \\\n       -d '{\"title\": \"new post\"}' \\\n       localhost:3000/api/posts\n\n# Add Headers to the request\n$ curl --location --request POST \\\n       -H \"Content-Type: application/json\" \\\n       -H \"User-Agent: some_user_agent\" \\\n       -d '{\"title\": \"new post\"}' \\\n       localhost:3000/api/posts\n</code></pre>"},{"location":"Local/terminal-bash/using-curl/#other-options","title":"Other Options","text":"<pre><code># Write large json responses to file instead\n$ curl -iL https://google.com -o response.json\n\n# Format the response. jsome is a global node module\n$ curl -iL https://google.com | jsome\n\n-i        # include response headers\n-I        # return only response headers\n-iL       # Follow redirects if the server is trying to redirect the request\n-H        # Send headers in request\n-X        # Change the HTTP method of the request\n-d        # Pass a request body\n-o        # Output the response to a file\n-s        # Execution to be silent and without progress bar if any (like in case of storing to a file)\n--data-urlencode    # Post in a url-encoded form\n</code></pre>"},{"location":"Local/terminal-bash/using-curl/#filtering-response","title":"Filtering Response","text":"<pre><code># Returns only first line of response headers\n$ curl -I https://google.com | head -n 1\n\n# Returns 2nd part of first line\n$ curl -I https://google.com | head -n 1 | cut -d ' ' -f 2\n</code></pre>"},{"location":"Local/terminal-bash/using-find/","title":"Using find","text":""},{"location":"Local/terminal-bash/using-find/#options","title":"Options","text":"<pre><code>-name    # search by exact match of name\n-iname   # search by case insensitive match of name\n-type    # type of file or folder we are looking for\n-delete  # delete all the files/folders found with find command\n-exec    # Run an arbitrary command on returned files\n</code></pre>"},{"location":"Local/terminal-bash/using-find/#finding-files-directories","title":"Finding Files &amp; Directories","text":"<pre><code>$ find &lt;source_folder&gt; -name \"string or expression\"\n\n# Search all the files with png extension in current directory\n$ find . -name \"*.png\"\n\n# Search all the folders in current directory\n$ find . -type d\n\n# Search only directories containing name\n$ find . -type d -name \"abc\"\n\n# Delete all png files from current folder\n$ find . -name \"*.png\" -delete\n\n# Compress all the files returned.\n# {} signifies command to execute on each file\n$ find . -name \"*.png\" -exec pngquant {} \\;\n</code></pre>"},{"location":"Local/terminal-bash/using-grep/","title":"Using grep","text":""},{"location":"Local/terminal-bash/using-grep/#options","title":"Options","text":"<pre><code>-n          # lists line number the text was found on\n-v          # exclude text from line\n-C 1        # returns the context for each file (line one above and one below)\n-e          # pass in RegEx instead of string\n--color     # highlights the text searched\n\n# Note: multiple flags can be used together\n# Example: -c -v or -cv\n</code></pre>"},{"location":"Local/terminal-bash/using-grep/#usage","title":"Usage","text":"<pre><code># Search text with grep\n$ grep &lt;string&gt; &lt;file(s)&gt;\n\n# Search in one file\n$ grep \"awesome\" file.txt\n\n# Search in all txt files in current folder\n$ grep \"awesome\" ~/*.txt\n\n# Search in all txt files, including sub-folder files\n$ grep \"awesome\" ~/**/*.txt\n</code></pre>"},{"location":"Local/terminal-bash/using-grep/#expanded-view","title":"Expanded View","text":"<pre><code># Print two lines After line matching search string\n$ grep -A 2 \"awesome\" file.txt\n\n# Print two lines Before line matching search string\n$ grep -B 2 \"awesome\" file.txt\n\n# Print two lines before and two after the line matching search string\n$ grep -B 2 -A 2 \"awesome\" file.txt\n</code></pre>"},{"location":"Local/terminal-bash/using-grep/#filter","title":"Filter","text":"<pre><code># Reverse of searching a text\n# Search lines that do NOT have the text\n$ grep -v \"awesome\" file.txt\n</code></pre>"},{"location":"Local/terminal-bash/using-grep/#count","title":"Count","text":"<pre><code># Count number of occurrences\n$ grep -c \"awesome\" file.txt\n\n# This will only print out the count, and not the matching strings\n</code></pre>"},{"location":"Local/terminal-bash/using-grep/#formatting","title":"Formatting","text":"<pre><code># highlights the string occurrence in the file\n$ grep --color \"awesome\" file.txt\n</code></pre>"},{"location":"Local/terminal-bash/cheatsheet/","title":"Common Commands","text":""},{"location":"Local/terminal-bash/cheatsheet/#pipes","title":"Pipes","text":"<p>Pipes pass output of one command to the next command</p> <pre><code>$ uglify -c -m -- index.js | gzip -9 | wc -c\n\nuglify    # npm module installed globally\n-c        # compress file\n-m        # minify file\n-9        # Compress to maximum\n-c        # Return byte count instead of word count\n</code></pre>"},{"location":"Local/terminal-bash/cheatsheet/#fun-commands","title":"Fun commands","text":"<pre><code># Make computer say something\n$ say something\n\n# Find current wifi password (require admin credentials)\n$ security find-generic-password -wa &lt;wifi_name&gt;\n\n# Copy the output of any command on clipboard\n$ &lt;command&gt; | pbcopy\n\n# Keep mac awake as long as terminal is up. Press ctrl + c to stop.\n$ caffeinate\n\n# Change password\n# Do not forget to update keychain password, with the command from the result\n$ passwd\n\n# Copy files. Alternative to cp\n$ ditto source_file destination_file\n\n# Preview a file\n$ qlmanage -p &lt;file_name&gt;\n\n# compare two files\n$ diff file1 file2\n\n# Set an alarm from terminal for 12:45\n$ leave 1245\n\n# See history of all your commands\n$ history\n\n# Python web server\n# Can be used to browse files in your machine via browser\n$ python3 -m http.server\n$ open localhost:8000\n\n# Shutdown\n$ shutdown -h now\n\n# Restart\n$ shutdown -r now\n</code></pre>"},{"location":"Local/terminal-bash/cheatsheet/#apache","title":"Apache","text":"<pre><code>ports.conf\nsudo a2ensite vboxsf\nsudo a2dissite 000-default\nservice apache2 reload\n</code></pre>"},{"location":"Local/terminal-bash/cheatsheet/#commands","title":"Commands","text":"<pre><code>which command        # Returns the location of the binary installed\nenv                  # Prints out all the environment variables\nsudo env             # Prints out all the environment variables for sudo not the current user\ncommand -v ls        # Returns the location of the binary installed. Seems same as `which` command\nhostname\n\ntee    # utility copies standard input to standard output, making a copy in zero or more files. Output is unbuffered.\n\ndash               # built-in command interpreter\nsh                 # short hand for dash\nlsmod              # lists statuses of modules in the Linux Kernel\nreboot             # reboots the machine\nls -la /dev        # List all the available devices\nls -la /dev | grep cdrom               # filter on available devices\nls -la /media/cdrom                    # destination directory???\nsudo mount &lt;device&gt; &lt;destination&gt;      # Mount a device\nsudo mount /dev/cdrom /media/cdrom\nid                                     # Displays user and group information\ngetent group vboxsf                    # Display who all have access to the folder???\nsudo usermod -a -G vboxsf &lt;name&gt;       # Provide user access\nls -1 | grep apache\nsudoedit sites-available/vboxsf.conf\n</code></pre>"},{"location":"Local/terminal-bash/cheatsheet/#references","title":"References","text":"<ul> <li>YT - 50 macOS Tips and Tricks Using Terminal | NetworkChuck</li> </ul>"},{"location":"Local/terminal-bash/cheatsheet/brace_expansion/","title":"Braces Expansion","text":"<p>Brace expansion is a mechanism by which arbitrary strings can be generated. Patterns to be brace expanded take the form of an optional preamble, followed by a series of comma-separated strings between a pair of braces, followed by an optional postscript.</p> <p>This is a cool technique to perform multiple operations via a simple modification</p> <pre><code># copy existing file as config.json.backup\n$ cp config.json{,.backup}\n\n# Create multiple directory structures.\n$ mkdir -p project/{a,b,c}/src\n    # Validate the directory structures created above\n    $ tree\n\n# Returns pre-a-post, pre-b-post, pre-c-post\n$ echo pre-{a,b,c}-post\n\n# Returns integer sequence 1 to 10\n$ echo {1..10}\n\n# Returns char sequence from a to j\n$ echo {a..j}\n\nPart of the expansion are few unique commands\n$ !!                 # Runs the last command executed. Useful when you forgot to add sudo\n$ sudo !!\n\n$ !$                 # Refers to the last argument of the previous command\n$ touch script.sh\n$ chmod +x !$        # This will already know to replace script.sh here\n</code></pre>"},{"location":"Local/terminal-bash/cheatsheet/brace_expansion/#references","title":"References","text":"<ul> <li>How-to: Shell Expansion</li> </ul>"},{"location":"Local/terminal-bash/cheatsheet/configuration/","title":"Cheatsheet - System Configuration","text":""},{"location":"Local/terminal-bash/cheatsheet/configuration/#touch-id-for-sudo-password","title":"Touch ID for sudo password","text":"<pre><code># Use touch id as sudo password\n$ sudo nano /etc/pam.d/sudo\n\n# Add line below first comment\nauth sufficient pam_tid.so\n</code></pre>"},{"location":"Local/terminal-bash/cheatsheet/configuration/#screen-capture","title":"Screen Capture","text":"<pre><code># Change default screenshot file name\n$ defaults write com.apple.screencapture name &lt;name&gt;\n\n# Change default screenshot file type\n$ defaults write com.apple.screencapture type jpg\n\n# Change default screenshot file location\n$ defaults write com.apple.screencapture location ~/Desktop/screenshots\n</code></pre>"},{"location":"Local/terminal-bash/cheatsheet/configuration/#download-history","title":"Download History","text":"<pre><code># Mac keeps history of everything we download\n# View download history\n$ sqlite3 ~/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV* 'select LSQuarantineDataURLString from LSQuarantineEvent'\n\n# Clear download history\n$ sqlite3 ~/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV* 'delete from LSQuarantineEvent'\n</code></pre>"},{"location":"Local/terminal-bash/cheatsheet/configuration/#other","title":"Other","text":"<pre><code># Disable gatekeeper\n# Gatekeeper enforces code signing and verifies downloaded applications before allowing them to run\n$ disable gatekeeper\n\n# Install the apps you want to install, without being blocked by the gatekeeper\n$ sudo spctl --master-disable\n\n</code></pre>"},{"location":"Local/terminal-bash/cheatsheet/health/","title":"Cheatsheet - System Health","text":""},{"location":"Local/terminal-bash/cheatsheet/health/#system-internals","title":"System Internals","text":"<pre><code># Space left on hard drive\n$ df -h\n\n# Processes\n$ ps\n$ ps -ax\n\n# Find which processes are using the most cpu realtime\n$ top\n\n# Find which processes are using the most memory realtime\n$ top -o rsize\n\n# Kill a running process\n$ ps -ax | grep &lt;name&gt;\n$ kill -9 &lt;process_id&gt;\n\n# How long has the mac been up?\n$ uptime\n</code></pre>"},{"location":"Local/terminal-bash/cheatsheet/network/","title":"Cheatsheet - Network","text":""},{"location":"Local/terminal-bash/cheatsheet/network/#networking","title":"Networking","text":"<pre><code># Find your IP address\n$ ifconfig\n$ ifconfig en0\n$ ifconfig en0 | grep inet\n$ ifconfig en0 | grep inet | awk '{print $2}'\n\n# Check if a website is up and running\n$ ping &lt;website_or_ip&gt;\n\n# See path through the internet you're taking to get to the website, including latency\n$ traceroute &lt;website_or_ip&gt;\n\n# DNS of a website\n$ dig &lt;website_or_ip&gt;\n\n# Flush DNS cache\n$ sudo killall -HUP mDNSResponder\n$ sudo killall -HUP mDNSResponderHelper\n$ sudo dscacheutil -flushcache\n</code></pre>"},{"location":"Local/terminal-bash/shell_setup/","title":"Shell","text":""},{"location":"Local/terminal-bash/shell_setup/#find-shell","title":"Find Shell","text":"<pre><code># Show default shell. Should be zsh\n$ echo $0\n\n# Change shell\n$ chsh -s /bin/zsh\n\n</code></pre>"},{"location":"Local/terminal-bash/shell_setup/#shell-configuration-file","title":"Shell Configuration File","text":"<p>Specific to zsh shell. Your root folder should have a file called <code>.zshrc</code>. Use this configuration in your config file.</p> <p>Remember to replace <code>my_user_name</code> with the one you are using right now.</p> <pre><code># Once done updating the config file, apply the configuration update by\n$ source ~/.zshrc\n</code></pre>"},{"location":"Local/terminal-bash/shell_setup/#shell-theme-manager","title":"Shell Theme Manager","text":"<p>Install <code>Oh My Zsh</code> from the website, and apply the configuration.</p> <pre><code># It'll report missing depedencies:\n\n# Install ruby\n$ brew install ruby\n\n# In most cases, applying source file changes will give colorls missing\n$ sudo gem install colorls\n\n\n# Install additional plugins\n$ brew install zsh-autosuggestions\n$ brew install zsh-syntax-highlighting\n\n</code></pre>"},{"location":"Local/terminal-bash/shell_setup/#references","title":"References","text":"<ul> <li>YT - How To Make Your macOS Terminal Amazing From Scratch | KantanCoding</li> <li>YT - How to setup your Mac Terminal to be beautiful | typecraft</li> <li>YT - The Ultimate Mac Terminal Setup - Beginner Tutorial | warpdotdev</li> </ul>"},{"location":"Local/terminal-bash/shells/","title":"Shell (MacOS)","text":"<p>A shell is a program that exposes an operating system's services to a human user or other programs. Command-line shells require the user to be familiar with commands and their calling syntax, and to understand concepts about the shell-specific scripting language.</p> <p>A shell in a way is similar to a programming language. To write the program, you would want to understand it's syntax and underlying concepts.</p>"},{"location":"Local/terminal-bash/shells/#background","title":"Background","text":"<p>Starting with macOS Catalina (10.15), Apple set the default shell to the Z shell (zsh). In previous macOS versions, the default was Bash.</p> <p>Each shell supports a configuration file in your macOS Home folder that gets read every time you open a new terminal window (or tab). This allows the shell environment to be set up properly with your preferences, and so that the tools you depend on are ready to use.</p> <p>In <code>zsh</code>, the configuration file is <code>~/.zshrc</code>. In <code>bash</code>, it\u2019s <code>~/.bash_profile</code>. Some people might tell you to add things to your <code>~/.bashrc</code>. Thank them for their help, and teach them that <code>.bashrc</code> does not get read automatically on a Mac when you open up a new shell window.</p>"},{"location":"Local/terminal-bash/shells/#which-shell-am-i-using","title":"Which shell am I using?","text":"<p>Option 1</p> <pre><code># print out the shell you're using\n$ echo $0\n</code></pre> <p>Option 2</p> <pre><code># A trick that works in all shells.\n# Type any bogus command that you know doesn\u2019t exist\n$ somerandomtext\n\n# As a result, you'll get something like\nsh: somerandomtext: command not found\nor\nzsh: command not found: somerandomtext\n</code></pre>"},{"location":"Local/terminal-bash/shells/#changing-shells","title":"Changing Shells","text":"<pre><code>$ chsh -s $(which bash)\n$ chsh -s $(which zsh)\n</code></pre> <p>Note: When you switch shells, if you expect to have the same configuration, make sure to copy the contents of <code>~/.bash_profile</code> into <code>~/.zshrc</code> or vice versa. Also look out for any code that is not compatible with both shells.</p>"},{"location":"Local/terminal-bash/shells/#references","title":"References","text":"<ul> <li>Shell (computing)</li> <li>Which Shell Am I Using? How Can I Switch? blog</li> <li>5 Ways to Read and Edit Hidden Files or Dotfiles blog</li> <li>Bash (Unix shell)</li> <li>Zsh (Z Shell)</li> </ul>"},{"location":"Local/terminal-bash/shells/bash-profile/","title":"Bash Profile","text":"<p>Bash shell uses a few startup files to set up the environment. These files determine certain Bash shell configurations for the shell itself and system users.</p> <p>Good spot to put small reusable aliases, functions, variables that we want us to have available anywhere in bash.</p>"},{"location":"Local/terminal-bash/shells/bash-profile/#references","title":"References","text":"<ul> <li>Difference Between .bashrc, .bash-profile, and .profile | baeldung</li> <li>Bash profile</li> </ul>"},{"location":"Local/terminal-bash/shells/customize-fig/","title":"Fig","text":"<p>Official website</p> <p>Fig lets you add visual autocomplete to any CLI tool or script.</p>"},{"location":"Local/terminal-bash/shells/customize-fig/#font","title":"Font","text":"<p>A better font for coding and terminals. Reading lots of text on screen gives me headaches at times. There are some amazing fonts that can be used to make this a whole lot easier.</p> <p>I use https://github.com/tonsky/FiraCode in almost all applications.  I've updated it for terminals, IDEs, and even default fonts in web browsers.  :heart:</p> <p>making the terminal pretty and more functional (there are many of these, but I use this one a bit customized) https://github.com/spaceship-prompt/spaceship-prompt</p> <p>and ... probably the most used terminal trick is reverse-search I think it is hard to find what I need (since the view port is only one line) this plugin super charges reverse search with PECO https://github.com/jimeh/zsh-peco-history</p>"},{"location":"Local/terminal-bash/shells/customize-hyper/","title":"Hyper","text":"<p>Hyper is an Electron-based terminal. Built on HTML/CSS/JS. Fully extensible.</p> <p>Follow installation and customization from the main website documentation</p>"},{"location":"Local/terminal-bash/shells/customize-iterm2/","title":"iTerm2","text":"<p>iTerm2 is an open source replacement for Apple's Terminal. It's highly customizable and comes with a lot of useful features.</p>"},{"location":"Local/terminal-bash/shells/customize-iterm2/#iterm-installation","title":"iTerm Installation","text":"<pre><code>$ brew install --cask iterm2\n</code></pre>"},{"location":"Local/terminal-bash/shells/customize-iterm2/#zsh","title":"Zsh","text":"<p>The Z shell (also known as <code>zsh</code>) is a Unix shell that is built on top of <code>bash</code> (the default shell for macOS) with additional features. It's recommended to use <code>zsh</code> over <code>bash</code>.</p> <pre><code>$ brew install zsh\n</code></pre>"},{"location":"Local/terminal-bash/shells/customize-iterm2/#oh-my-zsh","title":"Oh My Zsh","text":"<p>Oh My Zsh is an open source, community-driven framework for managing your zsh configuration. It comes with a bunch of features out of the box and improves your terminal experience.</p> <pre><code>$ sh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\"\n</code></pre>"},{"location":"Local/terminal-bash/shells/customize-iterm2/#references","title":"References","text":"<ol> <li>Source blog</li> </ol>"},{"location":"Local/terminal-bash/shells/customize-warp/","title":"Warp","text":"<p>Warp is a modern, Rust-based terminal with AI built in so you and your team can build great software, faster.</p>"},{"location":"Local/terminal-bash/shells/customize-warp/#resources","title":"Resources","text":"<ul> <li>Warp</li> <li>Starship</li> </ul>"},{"location":"Local/terminal-bash/tips/handling-files-directories/","title":"Handling Files &amp; Directories","text":""},{"location":"Local/terminal-bash/tips/handling-files-directories/#list-files-and-directories","title":"List files and directories","text":"<pre><code>$ ls -l\n\n-l    # long listing of files\n-la   # long listing with hidden files\n-G    # Colorize output\n</code></pre>"},{"location":"Local/terminal-bash/tips/handling-files-directories/#view-contents-of-a-file","title":"View contents of a file","text":"<pre><code>$ cat &lt;file_name&gt;\n-n    # show line numbers\n\nTo browse large files with scrolling capability\n$ less &lt;file_name&gt;\n\npressing\ng            # takes you to the top of the file\nShift + g    # takes you to the bottom of the file\n/string      # search for a string in the file and highlight if exists\nq            # exit file listing\n\n$ open &lt;directory&gt;        # shows folder content in finder. Helpful especially in hidden folders\n$ open &lt;file&gt;             # opens the file in default editor tied to the file extension\n$ open &lt;file&gt; -a TextEdit # opens the file in specific application\n</code></pre>"},{"location":"Local/terminal-bash/tips/handling-files-directories/#create-and-delete-a-filefolder","title":"Create and Delete a file/folder","text":"<pre><code># Create a file\n$ touch file.txt\n\n# Write to a file\n$ echo 'hi' &gt; file.txt    # Overwrite previous content of a file if exists or create a new file\n$ echo 'hi' &gt;&gt; file.txt   # Writes to file in append mode\n\n# Delete a file\n$ rm file.txt\n\n# Create a folder\n$ mkdir folder\n$ mkdir -p a/b/c        # p flag lets us create intermediary directories defined in the path\n\n# Delete a directory\n$ rm -rf folder         # recursively removes the folder and everything in it.\n                          f forces not to ask for confirmation of removing the directory and contents\n\n$ cp                    # copy file. Use -R flag for recursive copy of multiple files\n$ mv                    # move file(s)\n</code></pre>"},{"location":"Local/terminal-bash/tips/keyboard-shortcuts/","title":"Keyboard Shortcuts","text":"<pre><code># When typing in bash shell, say you're typing in a really long line, press -\n\nCtrl + a    - To go to start of the line\nCtrl + e    - To go to end of the line\nCtrl + k    - To clear anything after the cursor\nCtrl + w    - To delete the last word\nCtrl + l    - clears the screen\n</code></pre>"},{"location":"Local/vagrant-cli/","title":"Vagrant","text":""},{"location":"Local/vagrant-cli/#installation","title":"Installation","text":"<pre><code>Option 1:\nDownload and install from\nhttps://www.vagrantup.com/downloads.html\n\nNOTE:\nAt times Package managers tend to have outdated versions of software you was looking to install.\nOr even sometimes they are missing dependencies. Make sure it is all good or default to option 1.\n\nOption 2:\n$ brew cask install vagrant\n$ brew cask install vagrant-manager    // helps you manage all your virtual machines in one place from menu-bar\n\nOption 3:\n$ gem install vagrant\n</code></pre>"},{"location":"Local/vagrant-cli/#supporting-softwares","title":"Supporting Softwares","text":"<pre><code>Forget anything down there, Just install Ansible instead\n\nChef\nInstructions at: https://docs.chef.io/install_dk.html\nInstalled at \"/opt/chefdk/\"\nTo uninstall \"$ sudo rm -rf /opt/chefdk\"\n\nOption 2\n$ gem install berkshelf\n$ sudo gem install net-ssh\n$ gem install chef\n</code></pre>"},{"location":"Local/vagrant-cli/cheatsheet/","title":"Vagrant Cheatsheet","text":""},{"location":"Local/vagrant-cli/cheatsheet/#info","title":"Info","text":"<pre><code>$ vagrant -v                        // get the vagrant version\n$ vagrant status                    // Status of the vagrant machine\n$ vagrant global-status             // Status of all vagrant machines\n$ vagrant global-status --prune     // Same as above, but prunes invalid entries\n$ vagrant provision --debug         // More verbose output\n$ vagrant push                      // Deploy code. (Lookup on how to configure deploying code)\n$ vagrant up --provision | tee provision.log    // Runs vagrant up, forces provisioning and logs all output to a file\n\n$ vagrant ssh                    // Connects to machine via SSH\n$ vagrant ssh &lt;boxname&gt;          // If you give your box a name in your Vagrantfile, you can ssh into it with box-name\n</code></pre>"},{"location":"Local/vagrant-cli/cheatsheet/#creatingdestroying","title":"Creating/Destroying","text":"<pre><code>$ vagrant init                // Initialize Vagrant with a Vagrantfile and .vagrant directory\n$ vagrant init &lt;boxpath&gt;      // Initialize Vagrant with a specific box\n\nFind a box at\nhttps://app.vagrantup.com/boxes/search\n\n$ vagrant destroy             // Stops and deletes all traces of the vagrant machine\n$ vagrant destroy -f          // Same as above, without confirmation\n</code></pre>"},{"location":"Local/vagrant-cli/cheatsheet/#startstop","title":"Start/Stop","text":"<pre><code>$ vagrant up                    // Starts vagrant environment\n$ vagrant resume                // Resume a suspended machine\n$ vagrant provision             // Forces reprovisioning of the vagrant machine\n$ vagrant reload                // Restarts vagrant machine, loads new Vagrantfile configuration\n$ vagrant reload --provision    // Restarts the machine and force provisioning\n$ vagrant halt                  // Stops the vagrant machine\n$ vagrant suspend               // Suspends a virtual machine (remembers state)\n</code></pre>"},{"location":"Local/vagrant-cli/cheatsheet/#boxes","title":"Boxes","text":"<pre><code>$ vagrant box list                // See a list of all installed boxes on your computer\n$ vagrant box add &lt;name&gt; &lt;url&gt;    // Download a box image to your computer\n$ vagrant box add precise64 http://files.vagrantup.com/precise64.box        // Example\n$ vagrant box outdated            // Check for vagrant box updates\n$ vagrant boxes remove &lt;name&gt;     // Deletes a box from the machine\n$ vagrant package                 // Packages a running virtualbox env in a reusable box\n</code></pre>"},{"location":"Local/vagrant-cli/cheatsheet/#saving-progress","title":"Saving progress","text":"<pre><code>$ vagrant snapshot save [options] [vm-name] &lt;name&gt;     // Allows us to save to we can rollback at later time.\n                                                       // vm-name is ofter default\n</code></pre>"},{"location":"Observability/splunk/","title":"Splunk","text":"<ol> <li>Browse Data Sources</li> </ol>"},{"location":"Observability/splunk/#common-queries","title":"Common Queries","text":"<p>List Specific Values</p> <pre><code>&lt;query&gt; | stats values(&lt;key&gt;)\n&lt;query&gt; | table &lt;key&gt;\n&lt;query&gt; | stats count by &lt;key&gt;\n</code></pre> <p>Data for timechart</p> <pre><code>&lt;query&gt; | timechart span=1m count by &lt;key&gt;\n</code></pre>"},{"location":"Observability/splunk/browse-datasources/","title":"Browse Splunk Data Sources","text":"<p>It can get overwhelming trying to find where the data is located, if the naming convention is not standard across applications.</p> <p>Use these queries as is in the Splunk query bar</p> <pre><code># List all available Indexes\n| eventcount summarize=false index=* | dedup index | fields index\n</code></pre> <pre><code># List all available Container Names\n| tstats values(container_name) where index=xxxx group by index\n</code></pre> <pre><code># List sourcetypes in an index\n| tstats values(sourcetype) where index=xxxx group by index\n</code></pre> <pre><code># List sources in an index\n| tstats values(source) where index=xxxx group by index\n</code></pre> <pre><code># List namespace in an index\n| tstats values(namespace) where index=xxxx group by index\n</code></pre>"},{"location":"Programming/golang/","title":"Go Lang","text":""},{"location":"Programming/golang/#installation","title":"Installation","text":"<pre><code>Option 1\nDownload and install the binary from https://golang.org/doc/install\n\nOption 2\n$ brew install go --cross-compile-common        // Find out more about this option\nOR\n$ brew install go\n\nSetting up Path\n$ export PATH=\"/usr/local/go/bin/:$PATH\"\nOR\nIf your workspace is anywhere other than $HOME/go, set GOPATH (specifies location of your workspace)\n$ export GOPATH=\"$HOME/sites-personal/\"\n$ export PATH=\"$GOPATH/bin/:$PATH\"\n\nSet GOBIN path to generate a binary file when go install is run.\n$ export GOBIN=\"$GOPATH/bin\"\n\nExtras\n======\n$ go get golang.org/x/tools/cmd/godoc\n$ go get golang.org/x/tools/cmd/vet\n$ go get github.com/golang/lint/golint\n</code></pre>"},{"location":"Programming/golang/#working-with-code","title":"Working with code","text":"<pre><code>To download a project from GitHub, run following command from anywhere\n$ go get github.com/heroku/go-getting-started\n$ cd $GOPATH/src/github.com/heroku/go-getting-started\n# This command fetches the application locally, compile and install the generated executables in $GOPATH/bin.\n\nCompile the code\n$ go install .\n</code></pre>"},{"location":"Programming/golang/#dependency-management","title":"Dependency Management","text":"<pre><code>1. govendor\n2. godep\n3. GB\n\nInstalling tools\ngo get -u github.com/kardianos/govendor\n\nAdding dependency\ngovendor fetch github.com/russross/blackfriday\n    Records the Blackfriday dependency and any of it\u2019s dependencies in vendor/vendor.json\n    Makes a copy of github.com/russross/blackfriday in vendor/\n</code></pre>"},{"location":"Programming/golang/#commands","title":"Commands","text":"<pre><code>go version                // Find the version\ngo help                   // Help\ngo help [command]         // Help on a command\ngodoc fmt Println         // Lookup a function in Go doc\ngo run &lt;filename&gt;.go      // Run a file\n</code></pre>"},{"location":"Programming/regular-expressions/","title":"Regular Expressions","text":"<p>This list is by no means complete, and you might encounter slight variations from language to language. However, these 12 symbols are the most commonly used regular expressions in Python and can be used to find and collect almost any string type.</p> Symbol Meaning Example Example matches * Matches the preceding character, subexpression, or bracketed character, 0 or more times. ab aaaaaaaa, aaabbbbb, bbbbbb + Matches the preceding character, subexpression, or bracketed character, 1 or more times. a+b+ aaaaaaaab, aaabbbbb, abbbbbb [] Matches any character within the brackets (i.e., \u201cPick any one of these things\u201d). [A-Z]* APPLE, CAPITALS, QWERTY () A grouped subexpression (these are evaluated first, in the \u201corder of operations\u201d of regular expressions). (ab) aaabaab, abaaab, ababaaaaab {m, n} Matches the preceding character, subexpression, or bracketed character between m and n times (inclusive). a{2,3}b{2,3} aabbb, aaabbb, aabb [^] Matches any single character that is not in the brackets. [^A-Z]* apple, lowercase, qwerty | Matches any character, string of characters, or subexpression separated by the <code>I</code> (note that this is a vertical bar, or pipe, not a capital i). b(a|i|e)d bad, bid, bed . Matches any single character (including symbols, numbers, a space, etc.). b.d bad, bzd, b$d, b d ^ Indicates that a character or subexpression occurs at the beginning of a string. ^a apple, asdf, a \\ An escape character (this allows you to use special characters as their literal meanings). \\^ \\| \\\\ ^ | \\ $ Often used at the end of a regular expression, it means \u201cmatch this up to the end of the string.\u201d Without it, every regular expression has a de facto \u201c.*\u201d at the end of it, accepting strings where only the first part of the string matches. This can be thought of as analogous to the ^ symbol. [A-Z][a-z]$ ABCabc, zzzyx, Bob ?! \"Does not contain.\" This odd pairing of symbols, immediately preceding a character (or regular expression), indicates that that character should not be found in that specific place in the larger string. This can be tricky to use; after all, the character might be found in a different part of the string. If trying to eliminate a character entirely, use in conjunction with a ^ and $ at either end. ^((?![A-Z]).)*$ no-caps-here, $ymb0ls a4e f!ne"},{"location":"Programming/cpp/","title":"C++","text":"<p>There is no installation needed. Make sure you have XCode command line tools installed</p> <pre><code># Version of C++ installed\n$ c++ --version\n</code></pre>"},{"location":"Programming/cpp/#compiling-files","title":"Compiling files","text":"<p>To be able to compile files from your terminal you can add the following alias in your bash profile.</p> <pre><code>alias cppcompile='c++ -std=c++11 -stdlib=libc++'\n</code></pre> <p>Then you can run all cpp file directly using cppcompile main.cpp and it will use C++11 so no errors in the case of using vectors, auto, sets etc.</p>"},{"location":"Programming/cpp/#project-resources","title":"Project Resources","text":"<ul> <li>How to program a game in C++: #0 - Introduction and Setup | YT DanZaidan (good explanation)</li> <li>Code-It-Yourself! First Person Shooter (Quick and Simple C++) | YT javidx9 + Playlist</li> <li>Let's make 16 games in C++: Chess | YT Playlist</li> <li>Creating a Linked List Project in C++ Part 1 | YT Playlist</li> </ul>"},{"location":"Programming/flutter/","title":"Flutter Mobile-app development","text":""},{"location":"Programming/flutter/#flutter","title":"Flutter","text":"<p><code>Flutter</code> is an open source framework by Google for building beautiful, natively compiled, multi-platform applications from a single codebase.</p> <p>Internally, Flutter consists of a <code>framework</code> built with <code>Dart</code> and <code>rendering engine</code> mostly built with <code>C++</code>.</p>"},{"location":"Programming/flutter/#dart","title":"Dart","text":"<p>Dart is a client-optimized language for fast apps on any platform.</p>"},{"location":"Programming/flutter/#flutter-vs-dart","title":"Flutter vs Dart","text":"<p>Dart is a general-purpose programming language that can be used for a wide range of applications, including web development, server-side programming, and mobile app development.</p> <p>Flutter, on the other hand, is a UI SDK specifically designed for building high-performance, cross-platform mobile apps. Flutter has more app-specific libraries, more often on user interface elements</p>"},{"location":"Programming/flutter/#resources","title":"Resources","text":"<ul> <li>Flutter | Official website</li> <li>Dart | Official website</li> <li>A simplified introduction to Dart and Flutter | FreeCodeCamp</li> <li>Build apps with Flutter | Google for Developers</li> </ul>"},{"location":"Programming/flutter/#tools","title":"Tools","text":"<ul> <li>DartPad (overview)</li> </ul>"},{"location":"Programming/flutter/creating-new-project/","title":"Creating a new project","text":"<p>If you're using <code>Visual Studio Code</code> editor, press <code>Cmd + Shift + P</code> to open command pallete and type in <code>Flutter</code> to create a new application. Follow instructions thereafter.</p>"},{"location":"Programming/flutter/creating-new-project/#connected-devices","title":"Connected Devices","text":"<p>I had two devices listed there:</p> <ol> <li><code>MacOS</code> - because I am working on a macbook</li> <li><code>Chrome</code> - because Chrome is installed on this machine and is my default browser</li> </ol> <p>To bring up an Android emulator, I'll need to perform additional steps.</p>"},{"location":"Programming/flutter/creating-new-project/#running-the-application","title":"Running the application","text":"<pre><code>$ flutter run\n</code></pre>"},{"location":"Programming/flutter/design-systems/","title":"Design Systems","text":"<p>The Flutter SDK comes with two types of widget libraries to support <code>Material</code> and <code>Cupertino</code> design systems.</p>"},{"location":"Programming/flutter/design-systems/#material-design","title":"Material Design","text":"<p>Material-Design is developed by Google which can be used to develop apps across different platforms \u2014 Android, iOS, web, and desktop platforms.</p>"},{"location":"Programming/flutter/design-systems/#cupertino-design","title":"Cupertino Design","text":"<p>Cupertino-Design is developed by Apple; hence it is based on Apple\u2019s Human Interface Guidelines (iOS design language).</p>"},{"location":"Programming/flutter/design-systems/#resources","title":"Resources","text":"<ul> <li>Which design language do you need? | Medium (behind paywall)</li> </ul>"},{"location":"Programming/flutter/installation/","title":"Installation","text":""},{"location":"Programming/flutter/installation/#using-homebrew","title":"Using Homebrew","text":"<pre><code>$ brew install --cask flutter\n</code></pre>"},{"location":"Programming/flutter/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>$ flutter --version\n</code></pre> <p>You should see something like:</p> <pre><code>Flutter 3.10.1 \u2022 channel stable \u2022 https://github.com/flutter/flutter.git\nFramework \u2022 revision d3d8effc68 (3 days ago) \u2022 2023-05-16 17:59:05 -0700\nEngine \u2022 revision b4fb11214d\nTools \u2022 Dart 3.0.1 \u2022 DevTools 2.23.1\n</code></pre>"},{"location":"Programming/flutter/installation/#missing-dependencies","title":"Missing Dependencies","text":"<p>Run <code>Flutter Doctor</code> command to display the status of your Flutter installation. It tells you what dependencies are missing.</p> <pre><code>$ flutter doctor\n</code></pre> <p>I was still missing some, since I got the following error</p> <pre><code>[!] Android toolchain - develop for Android devices (Android SDK version 30.0.3)\n    \u2717 cmdline-tools component is missing\n      Run `path/to/sdkmanager --install \"cmdline-tools;latest\"`\n      See https://developer.android.com/studio/command-line for more details.\n    \u2717 Android license status unknown.\n      Run `flutter doctor --android-licenses` to accept the SDK licenses.\n      See https://flutter.dev/docs/get-started/install/macos#android-setup for more details.\n[\u2717] Xcode - develop for iOS and macOS\n    \u2717 Xcode installation is incomplete; a full installation is necessary for iOS and macOS development.\n      Download at: https://developer.apple.com/xcode/download/\n      Or install Xcode via the App Store.\n      Once installed, run:\n        sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer\n        sudo xcodebuild -runFirstLaunch\n    \u2717 CocoaPods not installed.\n        CocoaPods is used to retrieve the iOS and macOS platform side's plugin code that responds to your plugin usage on the Dart side.\n        Without CocoaPods, plugins will not work on iOS or macOS.\n        For more info, see https://flutter.dev/platform-plugins\n      To install see https://guides.cocoapods.org/using/getting-started.html#installation for instructions.\n</code></pre> <p>Based on this feedback, I had to run the following:</p> <p>I use JetBrains Toolbox to manage JetBrains dependencies, hance I installed <code>cmdline-tools</code> from the <code>Android Studio IDE</code> since it is Android dependency.</p> <p>Alternatively you could use the command below:</p> <pre><code># Install cmdline-tools component\n$ sdkmanager --install \"cmdline-tools;latest\"\n</code></pre> <p>Accept Android licenses</p> <pre><code># Accept Android License\n$ flutter doctor --android-licenses\n</code></pre> <p>Skip xcode and cocoapods installations since I won't be building ios app for now. But they can be done using commands below.</p> <pre><code># Fix Xcode installation\n$ sudo xcode-select --switch /Applications/Xcode.app/Contents/Developer\n$ sudo xcodebuild -runFirstLaunch\n\n# Install CocoaPods\nsudo gem install cocoapods\n</code></pre>"},{"location":"Programming/flutter/installation/#additional-resources","title":"Additional Resources","text":"<ul> <li>Install Flutter</li> <li>How To Install Flutter on macOS | LevelUp | Medium</li> <li>Update the IDE and SDK tools | Android official</li> <li>Flutter | Material</li> <li>Introduction to Flutter | Google Developers</li> <li>Your first flutter app | Google Developers | Codelabs</li> </ul>"},{"location":"Programming/flutter/installation/#review-links","title":"Review Links","text":"<ul> <li>https://flutter.github.io/samples/material_3.html</li> <li></li> </ul>"},{"location":"Programming/java/","title":"Java","text":"<ul> <li>Version Manager</li> <li>Installing JDK</li> <li>Installing Gradle</li> <li>Installing Maven</li> <li>Java Performance</li> </ul>"},{"location":"Programming/java/#references","title":"References","text":"<ul> <li>Oracle | JDK 18 Documentation</li> <li>Oracle | JDK 18 Specs</li> <li>Oracle | Java 18 Core Libraries Developer Guide</li> </ul>"},{"location":"Programming/java/#tools","title":"Tools","text":""},{"location":"Programming/java/#online-compile-run","title":"Online Compile &amp; Run","text":"<ul> <li>codiva</li> <li>ideone</li> </ul>"},{"location":"Programming/java/01-version-manager/","title":"SDK Manager","text":"<p>sdkman is a great tool for managing parallel versions of multiple software development kits on most Unix based systems.</p>"},{"location":"Programming/java/01-version-manager/#installation","title":"Installation","text":"<pre><code>$ curl -s \"https://get.sdkman.io\" | bash\n$ source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n$ sdk version\n</code></pre>"},{"location":"Programming/java/01-version-manager/#list-available-candidates","title":"List Available Candidates","text":"<pre><code>$ sdk list\n$ sdk list java\n$ sdk list groovy\n</code></pre>"},{"location":"Programming/java/01-version-manager/#jdk-installation","title":"JDK Installation","text":"<pre><code># OpenJDK\n$ sdk install java x.y.z-open\n\n# Amazon Coretto\n$ sdk install java x.y.z-amzn\n</code></pre>"},{"location":"Programming/java/01-version-manager/#sdk-installation","title":"SDK Installation","text":"<pre><code># Spring Boot\n$ sdk install springboot\n\n# Scala\n$ sdk install scala\n$ sdk install scala 3.2.0\n</code></pre>"},{"location":"Programming/java/01-version-manager/#remove-a-specific-version","title":"Remove a specific version","text":"<pre><code>$ sdk uninstall scala 3.2.0\n</code></pre>"},{"location":"Programming/java/01-version-manager/#use-a-version","title":"Use a version","text":"<pre><code>$ sdk use scala 3.2.0\n</code></pre>"},{"location":"Programming/java/01-version-manager/#default-a-version","title":"Default a version","text":"<pre><code># Make a given version the default\n$ sdk default scala 3.2.0\n</code></pre>"},{"location":"Programming/java/01-version-manager/#check-current-version","title":"Check current version","text":"<pre><code>$ sdk current java\n\n# To see what is currnetly in use for all candidates installed\n$ sdk current\n</code></pre>"},{"location":"Programming/java/01-version-manager/#references","title":"References","text":"<ul> <li>SDKMan usage</li> <li>jenv</li> </ul>"},{"location":"Programming/java/02-installation/","title":"Installation","text":"<p>It is best to use a version manager instead of installing java directly, as jdk version requirements may change between projects.</p>"},{"location":"Programming/java/02-installation/#installing-jdk","title":"Installing JDK","text":"<p>Using SDKMan</p> <pre><code># List all the versions available for Java\n$ sdk list java\n\n# Install latest stable version of Java\n$ sdk install java\n\n# Install specific version of Java\n$ sdk install java x.y.z-open\n\n# Switch to another version of Java (installed already locally)\n$ sdk use java 11.0.1-open\n</code></pre>"},{"location":"Programming/java/02-installation/#verify-installation","title":"Verify Installation","text":"<pre><code># Prints the version of primary JDK\n$ java -version\n\n# Prints the version of primary Java compiler\n$ javac -version\n\n# List all JDK installed with path\n$ /usr/libexec/java_home -V\n\n# List out just the paths\n$ /usr/libexec/java_home -V 2&gt;&amp;1 | grep -E \"\\d\\.\\d\\.\\d(_\\d+)?.*,\" | cut -d , -f 1 | cut -c 5-\n</code></pre>"},{"location":"Programming/java/02-installation/#uninstall","title":"Uninstall","text":"<pre><code># Remove the JDK\n$ sudo rm -rf /Library/Java/JavaVirtualMachines/jdk&lt;version&gt;.jdk\n\n# Remove plugins\n$ sudo rm -rf /Library/PreferencePanes/Java*\n$ sudo rm -rf /Library/Internet\\ Plug-Ins/Java*\n$ sudo rm -rf /Library/Java/*\n$ sudo rm -rf /Library/LaunchAgents/com.oracle.java.Java-Updater.plist\n$ sudo rm -rf /Library/PrivilegedHelperTools/com.oracle.java.JavaUpdateHelper\n$ sudo rm -rf /Library/LaunchDaemons/com.oracle.java.Helper-Tool.plist\n$ sudo rm -rf /Library/Preferences/com.oracle.java.Helper-Tool.plist\n</code></pre>"},{"location":"Programming/java/02-installation/#installing-add-ons","title":"Installing Add-Ons","text":"<pre><code># Java REPL\n$ brew install javarepl            # Java REPL. Requires Java 8\n</code></pre>"},{"location":"Programming/java/03-gradle/","title":"Gradle","text":"<p>Gradle is a build management tool capable of bundling the project and effectively handle dependency management.</p>"},{"location":"Programming/java/03-gradle/#official-documentation","title":"Official Documentation","text":"<ul> <li>Getting Started guides</li> <li>Official User Manual</li> <li>Official Community Resources</li> <li>Maven vs Gradle comparison</li> <li>Gradle + Kotlin</li> <li>Gradle CLI completion</li> <li>Gradle Wrapper</li> </ul>"},{"location":"Programming/java/03-gradle/#installation","title":"Installation","text":"<p>In a lot of cases you may not need to install gradle globally. Instead your project may have a local gradle copy, that can be accessed in some cases by <code>$ ./gradlew -v</code></p> <p>Best way to install would be through sdkman. Alternatively use <code>brew</code>.</p> <pre><code># Option 1\n$ sdk install gradle\n\n# Option 2\n$ brew install gradle\n</code></pre>"},{"location":"Programming/java/03-gradle/#verify","title":"Verify","text":"<pre><code># Display gradle version for global installation\n$ gradle -v\n\n# Display gradle version for local installation\n$ ./gradlew -v\n\n# List tasks\n$ ./gradlew tasks\n</code></pre>"},{"location":"Programming/java/03-gradle/#manage-process-tasks","title":"Manage Process / Tasks","text":"<pre><code># Stop all running gradle processes\n$ ./gradlew --stop\n</code></pre>"},{"location":"Programming/java/03-gradle/#gradle-wrapper","title":"Gradle Wrapper","text":"<p>If existing Gradle-based build uses the Gradle Wrapper, you can easily upgrade by running the <code>wrapper</code> task, specifying the desired Gradle version:</p> <pre><code>$ ./gradlew wrapper --gradle-version=8.0 --distribution-type=bin\n</code></pre> <p>Note that it is not necessary for Gradle to be installed to use the Gradle wrapper. The next invocation of <code>gradlew</code> or <code>gradlew.bat</code> will download and cache the specified version of Gradle.</p>"},{"location":"Programming/java/04-maven/","title":"Maven","text":"<p>Maven is a build management tool capable of bundling the project and effectively handle dependency management.</p>"},{"location":"Programming/java/04-maven/#official-documentation","title":"Official Documentation","text":"<ul> <li>Apache Maven Guides</li> <li>Apache Maven POM reference</li> <li>Sonatype | Maven: The Complete Reference</li> </ul>"},{"location":"Programming/java/04-maven/#installation","title":"Installation","text":"<p>Best way to install would be through sdkman.</p> <pre><code># Option 1\n$ sdk install maven\n\n# Option 2\n$ brew install maven\nor\n$ brew install maven@3.3\n\n# Option 3\n# Download and install from http://maven.apache.org/download.cgi\n$ tar xzvf apache-maven-3.5.0-bin.tar.gz\n$ cp -R apache-maven-3.5.0 /opt/\n$ export PATH=/opt/apache-maven-3.5.0/bin:$PATH\n$ mvn -v\n</code></pre>"},{"location":"Programming/java/garbabe-collection/","title":"Garbage Collection","text":""},{"location":"Programming/java/garbabe-collection/#references","title":"References","text":"<ul> <li>How to get by without Concurrent Mark Sweep blog</li> </ul>"},{"location":"Programming/java/io-streams/","title":"I/O Streams","text":"<p>A <code>stream</code> is a sequence of data flowing from a source to a destination. <code>I/O streams</code> can be connected to a wide variety of data sources and destinations.</p> <p>A stream object may be:</p> <ul> <li>An <code>input stream</code> or an <code>output stream</code>,</li> <li>a <code>character-oriented stream</code> or a <code>byte-oriented stream</code>,</li> <li>a <code>processing stream</code> or an <code>ordinary stream</code>,</li> <li>and may be connected to a variety of sources or destinations.</li> </ul> <p>Not only are there many types of streams, there are many ways to connect them together. I/O is a big topic because of the wide variety of I/O devices and the wide variety of data formats.</p>"},{"location":"Programming/java/io-streams/#hierarchy","title":"Hierarchy","text":"<p>An <code>input stream</code> handles data flowing into a program. It is the raw method of getting information from a source. It grabs the data byte by byte without performing any kind of translation. If you are reading image data, or any binary file/data, this is the stream to use.</p> <p>A <code>Reader</code> is designed for <code>character streams</code>. If the information you are reading is all text, then the Reader will take care of the character decoding for you and give you unicode characters (or defined charset) from the raw input stream. If you are reading any type of text, this is the stream to use. In Java, you can wrap an <code>InputStream</code> and turn it into a Reader by using the <code>InputStreamReader</code> class.</p> <p>An <code>output stream</code> handles data flowing out of a program.</p> <p>The goal of <code>InputStream</code> and <code>OutputStream</code> is to abstract different ways to input and output: regardless of whether the stream is a file, a web page, or text. All that matters is that you receive information from the stream (or send information into that stream.)</p>"},{"location":"Programming/java/io-streams/#processing-streams","title":"Processing Streams","text":"<p>A <code>processing stream</code> operates on the data supplied by another stream. Often a processing stream acts as a buffer for the data coming from another stream. A <code>buffer</code> is a block of main memory used as a work area.</p> <p>For example, disks usually deliver data in blocks of 512 bytes, no matter how few bytes a program has asked for. Usually the blocks of data are buffered and delivered from the buffer to the program in the amount the program asked for.</p> <p>Example, the keyboard sends data to the InputStream <code>System.in</code> which is connected to a InputStreamReader stream which is connected to a BufferedReader stream. <code>System.in</code> is a stream object that the Java system automatically creates when your program starts running. The data is transformed along the way. The raw bytes from the keyboard are grouped together into a <code>String</code> object that the program reads using <code>stdin.readLine()</code>.</p>"},{"location":"Programming/java/io-streams/#character-streams-and-byte-streams","title":"Character Streams and Byte Streams","text":"<p>Fundamentally all data consist of patterns of bits grouped into 8-bit bytes. So, logically all streams could be called <code>byte streams</code>. <code>Byte streams</code> are intended for general purpose input and output. And data may be primitive data types or raw bytes.</p> <p><code>Character streams</code> are exclusively <code>optimized for character data</code> and perform some other useful character-oriented tasks. Often the source or destination of a character stream is a text file, a file that contains bytes that represent characters.</p> <p>Inside a Java program character data is represented with the <code>16-bit char</code> data type. While on a disk file, characters are represented in a format called <code>UTF (Unicode Transformation Format)</code>. This format uses one to four bytes per character and is intended to be a universal format\u2014one format for all text files in any language anywhere in the world. In Character streams, data is transformed from/to <code>16 bit Java char</code> used inside programs to the <code>UTF format</code> used externally.</p> <p>Usually a UTF text file is identical to an ASCII text file. <code>ASCII</code> is the standard way to represent characters that most computers have used for the past forty years. A file created with a text editor is usually an ASCII text file. A UTF text file can include non-ASCII characters such as Cyrillic, Greek, and Asian characters. By reading and writing UTF files, Java programs can process text from any of the World's languages.</p> <p>Note: the bytecode file created by the Java compiler contains machine instructions for the Java virtual machine. These are not intended to represent characters, and input and output of them must use byte streams. Similar is the case with image files, audio files, executable files, etc.</p>"},{"location":"Programming/java/io-streams/#readers-and-writers","title":"Readers and Writers","text":"<p><code>Readers</code> and <code>Writers</code> deal with character streams. These are abstract classes. A program must use classes derived from them. For example, a <code>BufferedReader</code> is a Reader, and <code>OutputStreamWriter</code> is a Writer.</p> <p></p> <p></p> <p>All the Readers are aimed at receiving 16-bit char data from a program, and all the Writers aim at sending the 16-bit char data to another destination, which may use a different character format (such as UTF format on a disk file).</p>"},{"location":"Programming/java/io-streams/#inputstream-and-outputstream","title":"InputStream and OutputStream","text":"<p><code>InputStream</code> is an abstract class from which all byte-oriented input streams are derived. Its descendant classes are used for general-purpose input (non-character input). These streams deliver data to a program in groups of 8-bit bytes. The bytes can be grouped into the size necessary for the type of data.</p> <p>For example, if a disk file contains 32-bit int data, data can be delivered to the program in 4-byte groups in the same format as Java primitive type int. Example <code>DataInputStream</code>, <code>FileInputStream</code>, etc.</p> <p></p> <p><code>OutputStream</code> is an abstract class from which all byte-oriented output streams are derived. Its descendant classes are used for general-purpose (non-character output). These streams are aimed at writing groups of 8-bit bytes to output destinations. The bytes are in the same format as Java primitive types.</p> <p>For example, 4-byte groups corresponding to type int can be written to a disk file. Example <code>FileOutputStream</code>, <code>DataOutputStream</code>, etc. We have used <code>PrintStream</code> many times already, because <code>System.out</code> is an object of that type.</p> <p></p>"},{"location":"Programming/java/io-streams/#object-streams","title":"Object Streams","text":"<p>Generally, our programs objects exists only in the main memory and lasts as long the program that created them was running. When the program stops running (or earlier), they get garbage collected.</p> <p>With an <code>ObjectOutputStream</code> an object can be written to disk and will remain there even after the program stops. This is the topic of object <code>serialization</code>. <code>ObjectOutputStream</code> streams write objects to a destination such as a disk file. <code>ObjectInputStream</code> streams read objects from a source.</p>"},{"location":"Programming/java/io-streams/#common-methods","title":"Common Methods","text":"<ul> <li><code>.read()</code> - InputStream returns byte values between 0 and 255 corresponding to the raw contents of the byte stream. Reader returns the character value which is between 0 and 65357 (because there are 65358 different unicode codepoints)</li> <li><code>.readLine()</code></li> <li><code>.readLines()</code></li> </ul>"},{"location":"Programming/java/io-streams/#todo","title":"TODO","text":"<ul> <li>Handling large files</li> <li>Sync vs Async reads/writes</li> <li>JSONStream, GZIPStream</li> </ul>"},{"location":"Programming/java/monitoring/","title":"Monitoring","text":""},{"location":"Programming/java/monitoring/#what-is-jmx","title":"What is JMX?","text":"<p>The <code>Java virtual machine (Java VM)</code> has built-in <code>instrumentation</code> that enables you to monitor and manage it using the <code>Java Management Extensions (JMX)</code> technology. These built-in management utilities are often referred to as <code>out-of-the-box management tools</code> for the Java VM. You can also monitor any appropriately instrumented applications using the <code>JMX API</code>.</p>"},{"location":"Programming/java/monitoring/#enabling-jmx","title":"Enabling JMX","text":"<p>To enable and configure the out-of-the-box JMX agent so that it can monitor and manage the Java VM, you must set certain system properties when you start the Java VM. A full set of out-of-the-box management properties described here. Alternatively you can use a tool that complies to the JMX specification, such as JConsole.</p>"},{"location":"Programming/java/monitoring/#metrics-library","title":"Metrics Library","text":"<p>Instrumentation can be achieved by using <code>native JMX</code> or using a modular project like Metrics (GitHub repo). <code>Metrics</code> provides a powerful way to measure the behaviour of your critical components and reporting them to a variety of systems like, JConsole, or making them available through a web server. To install Metrics, we only have to add metrics dependency.</p> <p>Note: If you are planning to use <code>Metrics</code> in production code, it is highly recommended that you put metrics logic into <code>AOP</code> whenever possible, to avoid mixing instrumentation code with business logic.</p> <p><code>Metrics</code> we can use 6 types of metrics:</p> <ol> <li>Gauges: an instantaneous measurement of a discrete value.</li> <li>Counters: a value that can be incremented and decremented. Can be used in queues to monitorize the remaining number of pending jobs.</li> <li>Meters: measure the rate of events over time. You can specify the rate unit, the scope of events or event type.</li> <li>Histograms: measure the statistical distribution of values in a stream of data.</li> <li>Timers: measure the amount of time it takes to execute a piece of code and the distribution of its duration.</li> <li>Healthy checks: as his name suggests, it centralize our service\u2019s healthy checks of external systems.</li> </ol>"},{"location":"Programming/java/monitoring/#references","title":"References","text":"<ul> <li>What Is JMX Monitoring? (blog) | LogicMonitor</li> <li>Chapter 2 - Monitoring and Management Using JMX Technology | Oracle</li> <li>Chapter 3 - Using JConsole | Oracle<ul> <li>The Java Monitoring and Management Console (jconsole) | OpenJDK</li> </ul> </li> <li>Yammer Metrics, A new way to monitor your application (blog) | Java Code Geeks</li> <li>Best Tools For JMX Monitoring (blog)</li> <li>Best JMX Monitoring Tools (blog)</li> <li>10 Best JMX Monitoring Tools</li> <li>Book | Testing Java Microservices: Using Arquillian, Hoverfly, AssertJ, JUnit, Selenium, and Mockito</li> </ul>"},{"location":"Programming/java/serialization/","title":"Serialization","text":"<p><code>Object serialization</code> is the process of saving an object's state to a sequence of bytes, as well as the process of rebuilding those bytes into a live object at some future time. The Java Serialization API provides a standard mechanism for developers to handle object serialization.</p> <p>An object is marked serializable by implementing the <code>java.io.Serializable</code> interface, which signifies to the underlying API that the object can be flattened into bytes and subsequently inflated in the future.</p> <p>You could use the default protocol as is, or customize it, or even write your own protocol to perform serialization.</p>"},{"location":"Programming/java/serialization/#references","title":"References","text":"<ul> <li>Oracle | Discover the secrets of the Java Serialization API<ul> <li>Sourced from here | InfoWorld</li> </ul> </li> </ul>"},{"location":"Programming/java/serialization/#additional-reads","title":"Additional Reads","text":"<ul> <li>Oracle | Java Object Serialization Specification: Contents<ul> <li>Oracle | Java Object Serialization Specification: 1 - System Architecture</li> <li>Oracle | Java Object Serialization Specification: 2 - Object Output Classes</li> <li>Oracle | Java Object Serialization Specification: 3 - Object Input Classes</li> <li>Oracle | Java Object Serialization Specification: 4 - Class Descriptors</li> <li>Oracle | Java Object Serialization Specification: 5 - Versioning of Serializable Objects</li> <li>Oracle | Java Object Serialization Specification: 6 - Object Serialization Stream Protocol</li> </ul> </li> <li>Oracle | Serialization Filtering</li> <li>Oracle | Object Serialization: Frequently Asked Questions</li> <li>InfoWorld | The Java serialization algorithm revealed</li> <li>InfoWorld | Intro to MicroStream: Super-fast serialization in Java</li> <li>InfoWorld | JavaOne 2011: Serialization: Tips, Tricks, and Techniques</li> <li>InfoWorld | Signed and sealed objects deliver secure serialized content</li> <li>Oracle | JDK 18 Documentation</li> <li>Oracle | JDK 18 Specs</li> <li>Oracle | Java 18 Core Libraries Developer Guide</li> </ul>"},{"location":"Programming/java/performance/","title":"Performance bottlenecks","text":"<p>An application can do \"more\" with better hardware, but it won't necessarily perform better. Scaling up the hardware is one way to solve the problem, but if there are performance bottlenecks, even scaling up may not always help. Hence this would be the time to look for improvements that can be done on the application itself consuming resources.</p> <p>Note: Before even solving a performance issue, make sure that one exists in the first place.</p>"},{"location":"Programming/java/performance/#hardware-resources","title":"Hardware Resources","text":"<p>To analyze a system's performance, we should examine the utilization, saturation and errors of every significant resource that makes up the system -- Brendan Gregg</p> <p>Here are the metrics to look for when analyzing performance on a hardware: </p> <ul> <li>Utilization - proportion of a resource that is used or the average time that the resource was busy servicing work<ul> <li>CPU cycle</li> <li>RAM capacity</li> <li>Disk capacity</li> <li>Disk I/O</li> <li>Network I/O</li> </ul> </li> <li>Saturation - degree to which the resource has extra work that it can't service<ul> <li>CPU run queue</li> <li>Disk  swap space</li> </ul> </li> <li>Errors - count of error events for a resource<ul> <li>Error handling</li> <li>Retries</li> <li>Fewer pool  resources</li> </ul> </li> </ul> <p>Performance is the amount of work accomplished by a computer system. This work can be defined at various level of granularity like:</p> <ul> <li>Requests per second</li> <li>Queries per second</li> <li>Frames per second</li> <li>Operations per second</li> <li>Instructions per second</li> </ul> <p>Fact: Sequential data access is generally faster than random access. This is because the disk-head has to travel lesser distance in accessing the data.</p>"},{"location":"Programming/java/performance/#software-resources","title":"Software Resources","text":"<ul> <li>Thread pools<ul> <li>Utilization - number of threads currently executing a task</li> <li>Saturation -  number of items in the thread pool work queue</li> </ul> </li> <li>Locks<ul> <li>Utilization - time the lock was held</li> <li>Saturation - number of threads queued and waiting on the lock</li> </ul> </li> <li>Threads/File descriptors<ul> <li>Utilization - number of threads or file descriptors in use</li> <li>Saturation - number of items waiting for the thread of file descriptor to be allocated</li> <li>Errors - number of allocation failures</li> </ul> </li> </ul>"},{"location":"Programming/java/performance/measuring-performance/","title":"Measuring Performance","text":""},{"location":"Programming/java/performance/measuring-performance/#performance-testing","title":"Performance Testing","text":"<ul> <li>Development activity</li> <li>Define application/component under test</li> <li>Generate load against the application</li> <li>Analyze results</li> </ul>"},{"location":"Programming/java/performance/measuring-performance/#java-performance","title":"Java Performance","text":""},{"location":"Programming/java/performance/measuring-performance/#topics-base","title":"Topics - Base","text":"<ul> <li>Application monitoring and performance testing</li> <li>Java profilers</li> <li>JVM tuning</li> <li>Search &amp; Data structures (performance of Java Streams)</li> <li>Memory management</li> <li>Optimizing concurrent code<ul> <li>how to size a thread pool</li> <li>how to use fork-joint pool</li> <li>how to minimize lock contention</li> </ul> </li> <li>Avoid doing expensive operations</li> </ul>"},{"location":"Programming/java/performance/measuring-performance/#topics-advanced","title":"Topics - Advanced","text":"<ul> <li>Microbenchmarking - use Java microbenchmarking harness library</li> <li>Statistical measurements</li> <li>Database query tuning</li> <li>Concurrency basics</li> </ul>"},{"location":"Programming/java/performance/monitoring/","title":"Production Monitoring","text":"<ul> <li>Operations activity</li> <li>Collect and aggregate metric data</li> <li>Metric data storage</li> <li>Analysis, visualization and alerting</li> </ul> <p>Production monitoring is broken into three parts</p> <ul> <li>Collect<ul> <li>Dropwizard metrics</li> <li>Micrometer metrics</li> <li>Netflix Spectator</li> <li>Google OpenCensus</li> <li>Prometheus Lib</li> </ul> </li> <li>Store<ul> <li>InfluxDB</li> <li>ElasticSearch</li> <li>Graphite</li> <li>OpenTSDB</li> <li>Prometheus TSDB</li> <li>Other commercial tools like DataDog, etc</li> </ul> </li> <li>Analyze/Visualize<ul> <li>Grafana</li> <li>Kibana</li> <li>JMX tools</li> <li>Prometheus dashboards</li> </ul> </li> </ul>"},{"location":"Programming/java/performance/performance-metrics/","title":"Application Performance Metrics","text":"<p>Performance is a key usability and quality metric. Phrases like making fast and improving scalability are vague and subjective and don't make very good performance targets. As a result, the industry has come up with widely accepted terms for quantifying performance:</p> <ul> <li>Latency (Response time) - amount of time in ms, required to complete a unit of work</li> <li>Elapsed time - measures time taken for a batch of operations to complete</li> <li>Throughput - amount of work an application can accomplish per unit of time (seconds)</li> </ul>"},{"location":"Programming/java/performance/performance-testing-tools/","title":"Performance Testing Tools","text":""},{"location":"Programming/java/performance/performance-testing-tools/#operating-system","title":"Operating System","text":""},{"location":"Programming/java/performance/performance-testing-tools/#linux-tools","title":"Linux tools","text":""},{"location":"Programming/java/performance/performance-testing-tools/#vmstat","title":"vmstat","text":"<p>For measuring CPU utilization.</p> <p>vmstat reports information about processes, memory, paging, block IO, traps, and cpu activity.</p> <pre><code>$ vmstat 1 10\n</code></pre> <p>CPU can be idle if doing nothing. However it can also be idle if the running application is blocked due to waiting on a lock or waiting on an I/O.</p> <p>Refer:</p> <ul> <li>How to use the vmstat command</li> <li>vmstat command in Linux with Examples</li> <li>Linux commands: exploring virtual memory with vmstat</li> </ul>"},{"location":"Programming/java/performance/performance-testing-tools/#iostat","title":"iostat","text":"<p>For measuring disk I/O saturation.</p> <p>The iostat command is used for monitoring system input/output device loading by observing the time the devices are active in relation to their average transfer rates. The iostat command generates reports that can be used to change system configuration to better balance the input/output load between physical disks.</p> <pre><code>$ iostat -xm\n</code></pre> <p>Note: install sysstat if iostat not available</p> <p>Refer:</p> <ul> <li>iostat command in Linux with examples</li> <li>How to use the Linux iostat command to check on your storage subsystem</li> </ul>"},{"location":"Programming/java/performance/performance-testing-tools/#macos-tools","title":"MacOS tools","text":""},{"location":"Programming/java/performance/performance-testing-tools/#vmmap","title":"vmmap","text":"<p>VMMap is a process virtual and physical memory analysis utility. It shows a breakdown of a process's committed virtual memory types as well as the amount of physical memory (working set) assigned by the operating system to those types.</p> <pre><code>$ vmmap --wide &lt;pid&gt;\n$ vmmap --wide 10366\n</code></pre> <p>Refer:</p> <ul> <li>How do you read the memory maps of a Mac process?</li> <li>Viewing Virtual Memory Usage</li> <li>Book | Mac OS X in a Nutshell</li> </ul>"},{"location":"Programming/java/performance/performance-testing-tools/#windows-tools","title":"Windows tools","text":""},{"location":"Programming/java/performance/performance-testing-tools/#typeperf","title":"typeperf","text":"<p>The typeperf command writes performance data to the command window or to a log file. To stop typeperf, press CTRL+C.</p> <p>Refer:</p> <ul> <li>SS64 | TypePerf.exe</li> </ul>"},{"location":"Programming/java/performance/performance-testing-tools/#other-monitoring-tools","title":"Other monitoring tools","text":""},{"location":"Programming/java/performance/performance-testing-tools/#netstat","title":"netstat","text":"<p>The netstat command generates displays that show network status and protocol statistics. You can display the status of TCP and UDP endpoints in table format, routing table information, and interface information. <code>netstat</code> displays various types of network data depending on the command line option selected.</p>"},{"location":"Programming/java/performance/performance-testing-tools/#nicstat","title":"nicstat","text":"<p>nicstat prints network traffic statistics.</p>"},{"location":"Programming/java/performance/performance-testing-tools/#sar","title":"sar","text":"<p>Collect, report, or save system activity information. The sar command writes to standard output the contents of selected cumulative activity counters in the operating system.</p> <p>Refer:</p> <ul> <li>SAR command in Linux to monitor system performance</li> </ul>"},{"location":"Programming/java/performance/performance-testing-tools/#java-jvm","title":"Java / JVM","text":""},{"location":"Programming/java/performance/performance-testing-tools/#jmc-java-mission-control","title":"JMC (Java Mission Control)","text":"<p>A tool for collecting low level and detailed runtime information from a JVM. JMC contains a graphical UI for Java Flight Recorder. JMC can connect to both local and remote processes, provided JMX monitoring is enabled on the server.</p> <p>Note: If you don't find jmc in your jdk, you could be using OpenJDK, install from here</p> <p>JMC integrates well with these tools:</p> <ul> <li>jstat (JVM statistics)</li> <li>jinfo (JVM configuration info)</li> <li>jmap (Java memory map)</li> <li>jstack (stack traces)</li> <li>jconsole (JMX MBeans)</li> </ul> <pre><code># Enable JMX monitoring on Java process\n$ java -Dcom.sun.management.jmxremote\n    -Dcom.sun.management.jmxremote.port=&lt;XXXX&gt;\n    -Djava.rmi.server.hostname=&lt;X.X.X.X&gt;\n    -Dcom.sun.management.jmxremote.authentication=false\n    -Dcom.sun.management.jmxremote.ssl=false\n</code></pre>"},{"location":"Programming/java/performance/performance-testing-tools/#jstat","title":"jstat","text":"<p>jstat (Java Virtual Machine Statistics Monitoring Tool)</p>"},{"location":"Programming/java/performance/performance-testing-tools/#jcmd","title":"jcmd","text":"<p>jcmd is command line equivalent of <code>Java Mission Control</code>. The only difference is that it cannot connect to remote processes. Hence it is useful when you want to monitor process on a remote server however are unable to enable JMX monitoring on the server. If you have ssh access to the remote server, you can ssh into it and run jcmd to get all the information you would've gotten via Mission control.</p>"},{"location":"Programming/java/performance/performance-testing-tools/#jmeter","title":"jMeter","text":"<p>A tool for load testing and analyzing the performance of web apps</p>"},{"location":"Programming/java/performance/performance-testing-tools/#stats-over-time","title":"Stats over time","text":"<p>All the tools above give you stats for the current time. To look at the stats over time or between specific timestamp, these stats needs to be exported to additional tools. </p> <p>More to come in this section................</p>"},{"location":"Programming/java/performance/profiling/","title":"Java Profilers","text":"<p>JVM generates a lot of events in either of the categories below:</p> <ul> <li>Instant events - one time events that have a timestamp and event data. Example: Exception events, Class load events, Object allocation events.</li> <li>Duration events - provide start time and end time of an activity. Example: Garbage collection, monitor wait, monitor contended.</li> </ul> <p>JVM also has profiling functions to provide data on JVM internal state. Example: <code>getThreadState()</code>, <code>getAllThreads()</code>, <code>getStackTrace()</code> and <code>getAllStackTraces()</code>. All these functions are called requestable events by Java Flight Recorder.</p> <p>Java profilers monitor JVM execution at the bytecode level. They can:</p> <ul> <li>Passively listen to JVM events</li> <li>Actively query JVM stats</li> <li>Modify bytecode</li> </ul>"},{"location":"Programming/java/performance/profiling/#profiling-activities","title":"Profiling activities","text":"<ul> <li>CPU profiling - finds what methods run the most frequently and use the most CPU time. Such methods are called Hot methods. This information can be retrieved either by sampling or instrumentation.</li> <li>Memory profiling - concerned with understanding what objects are using up memory and how memory is freed.</li> <li>Thread profiling - concerned with what states threads are in and why.</li> <li>I/O profiling</li> </ul>"},{"location":"Programming/java/performance/profiling/#profiling-metrics","title":"Profiling Metrics","text":"<p>Java profilers provide information on:</p> <ul> <li>thread execution and locks</li> <li>heap memory usage</li> <li>garbage collection</li> <li>hot methods</li> <li>exceptions</li> <li>class loading</li> <li>etc.</li> </ul>"},{"location":"Programming/java/performance/profiling/#tools","title":"Tools","text":"<ul> <li>JProfiler</li> <li>Yourkit Java Profiler</li> <li>Java VisualVM</li> <li>Netbeans Profiler</li> <li>Java Flight Recorder</li> <li>Application Performance Management products</li> </ul>"},{"location":"Programming/nodejs-cli/","title":"Node.js","text":"<p>A wrapper around VMs like V8/Chakra with built-in modules (fs, http, crypto, zip, ...) providing rich features through easy-to-use asynchronous APIs (no threads). If the built-in modules are not enough, you can build packages for Node.js using C++ or JavaScript depending on what you want. This makes Node a great platform for tools other than being a platform for backend servers. Node.js also ships with debugger and other utilities.</p> <ul> <li>Node ships with a reliable package manager (NPM) and it works with the NPM register (hosted at npmjs.org)</li> <li>Node has a reliable module dependency manager (referred to as CommonJS). This is basically the \"require\" function in Node, combined with the \"module\" object.</li> </ul>"},{"location":"Programming/nodejs-cli/#modern-javascript","title":"Modern JavaScript","text":""},{"location":"Programming/nodejs-cli/#scopes","title":"Scopes","text":"<ul> <li>Block scope - bleed out to the common namespace, when using var. Example for-loop</li> <li>Function scope - do not bleed out, when using var. Example function</li> </ul>"},{"location":"Programming/nodejs-cli/#this","title":"this","text":"<ul> <li>Regular function cares about who calls it. \"this\" refers to the caller of the function</li> <li>Arrow functions do not care about who calls them. \"this\" refers to the scope within. Makes it a great use case for events and listeners because it gives easy access to \"this\" of the defining environment.</li> <li>In any node module, by default, global \"this\" is associated with the exports object</li> </ul>"},{"location":"Programming/nodejs-cli/#questions","title":"Questions","text":"<pre><code>\u2022 Can var variable be accessed out of for-loop as well as a function?\n\u2022 Can let variable be accessed out of for-loop as well as a function?\n\n</code></pre>"},{"location":"Programming/nodejs-cli/#installation-path","title":"Installation Path","text":"<pre><code>Node.js - /usr/local/bin/node\nNPM     - /usr/local/bin/npm\n</code></pre>"},{"location":"Programming/nodejs-cli/#references","title":"References","text":"<ul> <li>http://sourabhbajaj.com/mac-setup/Node.js/</li> <li>https://nodejs.org/dist/latest/docs/api/</li> <li>https://nodejs.org/docs/latest-v18.x/api/index.html</li> <li>https://nodejs.org/en/docs/guides/</li> </ul>"},{"location":"Programming/nodejs-cli/#new-reference-topics","title":"New Reference Topics","text":"<ul> <li>https://v8.dev/blog/cost-of-javascript-2019#json</li> <li>https://v8.dev/features/import-assertions</li> <li>https://nodejs.org/docs/latest-v18.x/api/esm.html#modules-ecmascript-modules</li> <li>https://nodejs.org/docs/latest-v18.x/api/packages.html#determining-module-system</li> <li>https://nodejs.org/es/blog/npm/peer-dependencies/</li> <li>https://swc.rs/</li> </ul>"},{"location":"Programming/nodejs-cli/#contributing","title":"Contributing","text":"<p>https://nodejs.org/en/get-involved/</p>"},{"location":"Programming/nodejs-cli/api/","title":"API","text":""},{"location":"Programming/nodejs-cli/api/#schema-design-schema-validation","title":"Schema Design &amp; Schema Validation","text":"<pre><code>Schema Design\nhttp://json-schema.org/\n    http://json-schema.org/implementations.html\n    Java   - https://github.com/java-json-tools/json-schema-validator\n    Python - https://pypi.python.org/pypi/jsonschema\n    Go     - https://github.com/xeipuuv/gojsonschema\n\nSchema Validation\nhttps://github.com/ebdrup/json-schema-benchmark\nhttps://github.com/epoberezkin/ajv\nhttps://github.com/hapijs/joi\nhttps://validatejs.org/\n</code></pre>"},{"location":"Programming/nodejs-cli/api/#api-testing","title":"API Testing","text":"<pre><code>https://medium.com/@alicealdaine/top-10-api-testing-tools-rest-soap-services-5395cb03cfa9\nhttps://www.soapui.org/\nhttps://www.katalon.com/\nhttps://jmeter.apache.org/\nhttp://rest-assured.io/\n</code></pre>"},{"location":"Programming/nodejs-cli/api/#connecting-with-databases","title":"Connecting with Databases","text":"<pre><code>https://blog.logrocket.com/why-you-should-avoid-orms-with-examples-in-node-js-e0baab73fa5\n</code></pre>"},{"location":"Programming/nodejs-cli/dependency-management/","title":"Node.js Dependency Management","text":"<p>Dependencies in Node.js are managed using a package manager. However it is important to categorize dependencies correctly.</p>"},{"location":"Programming/nodejs-cli/dependency-management/#types-of-dependencies","title":"Types of dependencies","text":"<ul> <li>dependencies</li> <li>devDependencies</li> <li>peerDepedencies</li> </ul>"},{"location":"Programming/nodejs-cli/dependency-management/#tools","title":"Tools","text":""},{"location":"Programming/nodejs-cli/dependency-management/#babel","title":"Babel","text":"<p>Babel is a toolchain that is mainly used to convert ECMAScript 2015+ code into a backwards compatible version of JavaScript in current and older browsers or environments.</p> <p>Babel can also convert JSX syntax!</p>"},{"location":"Programming/nodejs-cli/dependency-management/#references","title":"References","text":""},{"location":"Programming/nodejs-cli/dependency-management/#-httpsnodejsorgesblognpmpeer-dependencies","title":"- https://nodejs.org/es/blog/npm/peer-dependencies/","text":""},{"location":"Programming/nodejs-cli/frameworks/","title":"Frameworks","text":"<pre><code>express.js\nnest.js\nkoa.js\nhapi.js\nsail.js\npartial.js\ntotal.js\nfastify.js\n\nsource:\nhttps://codebrahma.com/9-best-node-js-frameworks-developers/\nhttps://medium.com/car2godevs/there-are-expressjs-alternatives-590d14c58c1c\n</code></pre>"},{"location":"Programming/nodejs-cli/libraries/","title":"Libraries","text":"<pre><code>immutable.js\n</code></pre>"},{"location":"Programming/nodejs-cli/logging/","title":"Logging in Node.js","text":"<p>Logging help you know what the heck is going on in the application. However regardless of what you choose, logging affects performance of the application/request in some form, because that's additional work the process will have to do. There is no escaping that. However what can be done is, to find an effective strategy to make the work more efficient and less overhead.</p> <p>One of the ways to log is using <code>console.log</code>, which is ready-to-use, and without any other dependency or configuration. Hence it makes is so much tempting to use it without thinking. By default, writes to <code>stdout</code> and <code>stderr</code>.</p> <p>The <code>console.log</code> function is a synchronous process, meaning it is a blocking call, which directly adds latency on the action being performed, and in worst case can break the execution flow of the request because of accidental code. Hence choosing another implementation/library early on saves from a trouble down the road.</p>"},{"location":"Programming/nodejs-cli/logging/#what-do-we-get-from-libraries","title":"What do we get from libraries?","text":"<ul> <li>Latency improvement</li> <li>Ability to write logs to stream-like destinations (other than stdout and stderr), example: writing to a file</li> <li>Logging to multiple destinations</li> <li>Data serialization and redaction from logs</li> <li>Control over log visibility by level. In some cases, without having a need to reload a running application</li> <li>Safety from bad code</li> </ul>"},{"location":"Programming/nodejs-cli/logging/#popular-libraries","title":"Popular Libraries","text":"<ul> <li>Bunyan</li> <li>Morgan</li> <li>Pinojs - destination based on sonic-boom</li> <li>Winston</li> </ul>"},{"location":"Programming/nodejs-cli/logging/#anything-better-exists","title":"Anything better exists?","text":"<p>Logging using one of the libraries is great, but it's even better if APM tools do this automatically.</p>"},{"location":"Programming/nodejs-cli/logging/#references","title":"References","text":"<ul> <li>How Logging Affect Performance in Nodejs</li> <li>LogRocket - Best Practices Essential Guide</li> <li>LogRocket - Logging with Pino and AsyncLocalStorage in Node.js</li> <li>A Complete Guide to Pino Logging in Node.js</li> <li>A Complete Guide to Winston Logging in Node.js</li> <li>Node.js in production: runtime log snooping - Bunyan</li> </ul>"},{"location":"Programming/nodejs-cli/modules/","title":"Modules","text":"<p>Modules are independent, reusable code that are isolated from other project dependencies and serve as a local library.</p> <p>Think of it as writing an inner function, which serves a very specific task, however is not generic enough to be reusable in other places/projects. Modules are same to a project. They address separation of concern, without having a need to create a library out of that code, importing them or managing versions, etc.</p> <p>TLDR; Javascript has a few approaches to manage module system within a project. <code>ESM</code> is the most preferred approach of all. ESM supports both server as well as client side, synchronously as well as asynchronously.</p> <p>Node.js has two module systems: <code>CommonJS</code> modules and <code>ECMAScript</code> modules.</p>"},{"location":"Programming/nodejs-cli/modules/#commonjs","title":"CommonJS","text":"<p>This system relies on importing and exporting modules using the keywords require and exports. <code>require</code> is a function used to import functions from another module. <code>exports</code> is an object where any function put into it will get exported. The <code>module.exports</code> object is really specific to NodeJS.</p> <pre><code>  // utils.js\n  // we create a function \n  function add(r){\n    return r + r;\n  }\n  // export (expose) add to other modules\n  exports.add = add;\n</code></pre> <pre><code>  // index.js\n  var utils = require('./utils.js');\n  utils.add(4); // = 8\n</code></pre> <p>The commonJS team created this API as a <code>synchronous</code> one which is not that good for browsers... Moreover, Commonjs isn't natively understood by browsers; it requires either a loader library or some transpiling. This is designed for <code>server development</code> and is synchronous. i.e., the files are loaded one by one in order inside the file.</p>"},{"location":"Programming/nodejs-cli/modules/#asynchronous-module-definition-amd","title":"Asynchronous Module Definition (AMD)","text":"<p>AMD was born out of CommonJS to support <code>asynchronous</code> module loading. This is the module system used by RequireJS and that is working client-side (in <code>browsers</code>).</p> <pre><code>  // add.js\n  define(function() {\n    return add = function(r) {\n      return r + r;\n    }\n  });\n</code></pre> <pre><code>  // index.js\n  define(function(require) {\n    require('./add');\n    add(4); // = 8\n  }\n</code></pre>"},{"location":"Programming/nodejs-cli/modules/#universal-module-definition-umd","title":"Universal Module Definition (UMD)","text":"<p>UMD is based on AMD but with some special cases included to handle CommonJS compatibility. Unfortunately, this compatibility adds some complexity that makes it complicated to read / write.</p> <p>Just ignore it.</p>"},{"location":"Programming/nodejs-cli/modules/#es2015-modules-esm","title":"ES2015 Modules (ESM)","text":"<p>As those 3 formats are not that easy to read, hard to analyze for static code analyzer and not supported everywhere, the ECMA team created ES6. This format is really simple to read and write and supports both synchronous and asynchronous modes of operation.</p> <pre><code>  // add.js\n  export function add(r) {\n    return r + r;\n  }\n</code></pre> <pre><code>  // index.js\n  import add from \"./add\";\n  add(4); // = 8\n</code></pre> <p>Moreover, es modules can be <code>statically analyzed</code> which allow build tools (like Webpack or Rollup) to perform tree-shaking on the code. <code>Tree-shaking</code> is a process that removes unused code from bundles.</p> <p>Unfortunately, this format still have 2 cons (but they're improving): - Proposal for ESM to supports dynamically imported modules is in stage 4 now and all browsers support it except IE. - It isn't supported on all the browsers but fortunately, this can be \"fixed\" thanks to... transpiling !</p>"},{"location":"Programming/nodejs-cli/modules/#transpiling","title":"Transpiling","text":"<p>Transpiling is the process of translating one language or version of a language to another. So here, the idea is to transpile ES6 to ES5 to get a better browser support. Unfortunately, this transpilation has a cost as it adds some extra code to patch the missing features of ES6 that don't exist in ES5.</p> <p>The most famous transpiler that is usually used in this case is Babel.</p>"},{"location":"Programming/nodejs-cli/modules/#known-trade-offs","title":"Known Trade-offs","text":""},{"location":"Programming/nodejs-cli/modules/#esm-must-have-the-mjs-file-extension","title":"ESM must have the *.mjs file extension","text":"<p>You can\u2019t know for sure what kind of JavaScript code you\u2019re looking at just by parsing it. With backwards compatibility being a primary goal for Node.js, the author needs to opt-in into the new mode.</p>"},{"location":"Programming/nodejs-cli/modules/#cjs-can-only-import-esm-via-asynchronous-import","title":"CJS can only import ESM via asynchronous import()","text":"<p>Node.js will load ESMs asynchronously in order to match the browser behavior as close as possible. Hence, a synchronous <code>require()</code> of an ESM will not be possible. As a consequence, every function that depends on an ESM needs to be <code>asynchronous</code>.</p>"},{"location":"Programming/nodejs-cli/modules/#cjs-exposes-a-single-immutable-default-export-to-esm","title":"CJS exposes a single, immutable default export to ESM","text":"<p>There is just a single export called <code>default</code> which equals an immutable snapshot of <code>module.exports</code> when the CJS module has finished evaluating.</p>"},{"location":"Programming/nodejs-cli/modules/#module-scoped-variables-like-module-require-and-__filename-do-not-exist-in-esm","title":"Module-scoped variables like module, require and __filename do not exist in ESM","text":"<p>Given the engineering challenges that come with the integration of CJS and ESM into a single runtime, the CTC has done an incredibly good job evaluating the edge cases and trade-offs.</p> <p>A file extension is basically a hint on how a binary file should be interpreted. If a <code>module</code> is not a <code>script</code>, we should use a different file extension. Other tools like linters or IDEs can pick up the same information.</p>"},{"location":"Programming/nodejs-cli/modules/#references","title":"References","text":"<ul> <li>https://code-trotter.com/web/understand-the-different-javascript-modules-formats/</li> <li>https://medium.com/webpack/the-state-of-javascript-modules-4636d1774358</li> </ul>"},{"location":"Programming/nodejs-cli/nodejs/","title":"Node.js Tips &amp; Tricks","text":""},{"location":"Programming/nodejs-cli/nodejs/#node-cheatsheet","title":"Node Cheatsheet","text":"<pre><code>$ node                        # Opens up Node REPL in command line\n&gt; .save index.js              # Saves REPL session into a file\n&gt; .load index.js              # Loads content of a file into Node REPL\n&gt; c &lt;tab&gt; &lt;tab&gt;               # Lists all possible combination for the letter C or anything you type\n$ node index.js               # Runs a js script (index.js in this case)\n\n$ node -p \"some string\"       # Executes a string expression\n$ node -p \"os.cpus().length\"\n$ node -p \"process.versions.v8\"\n$ node -p \"process.argv\" hello 42\n\n$ node -h | less              # Help on node\n$ node --v8-options | grep \"in progress\"\n$ NODE_DEBUG=\"http,fs\" node index.js\n$ VAL1=10 node index.js       # Sets a variable key-value in process.env\n    $ export VAL1=10          # works same as above, just two steps\n    $ node index.js\n</code></pre>"},{"location":"Programming/nodejs-cli/nodejs/#fix-npm-global-permissions","title":"Fix npm global permissions","text":"<pre><code>$ mkdir ~/.npm-global\n$ npm config set prefix '~/.npm-global'\n$ export PATH=~/.npm-global/bin:$PATH\n$ source ~/.profile\n</code></pre>"},{"location":"Programming/nodejs-cli/nodejs/#global-installations","title":"Global Installations","text":"<pre><code>p5-manager\nhttp-server\nlive-server\n</code></pre>"},{"location":"Programming/nodejs-cli/nodejs/#local-installations","title":"Local Installations","text":"<pre><code>yup - Javascript object validation based on Joi\n</code></pre>"},{"location":"Programming/nodejs-cli/nvm-node-version-manager/","title":"Node Version Manager (nvm)","text":"<p>Node Version Manager is a bash script used to manage multiple released Node.js versions. It allows you to perform operations like install, uninstall, switch version, etc.</p>"},{"location":"Programming/nodejs-cli/nvm-node-version-manager/#installation","title":"Installation","text":"<pre><code>$ curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.8/install.sh | bash\n\nThe script:\n1. Clones the nvm repository to ~/.nvm\n2. Adds the source line to your profile (~/.bash_profile, ~/.zshrc, ~/.profile, or ~/.bashrc)\n3. Modules installed globally via npm will no longer be linked to the active version of Node when you install a new node\n</code></pre>"},{"location":"Programming/nodejs-cli/nvm-node-version-manager/#installing-a-node-release","title":"Installing a Node release","text":"<pre><code>Install a Node release by choosing one from Node releases page [https://nodejs.org/en/download/releases/]\nThe command nvm install 5.10.1 installs a stable release for us\n</code></pre>"},{"location":"Programming/nodejs-cli/nvm-node-version-manager/#uninstalling","title":"Uninstalling","text":"<pre><code>$ nvm use system\n$ npm uninstall -g &lt;package&gt;\nClose and reopen the terminal to see the changes into effect\n</code></pre>"},{"location":"Programming/nodejs-cli/nvm-node-version-manager/#commands","title":"Commands","text":"<pre><code>$ nvm ls-remote                 # lists all of the available versions of NodeJs &amp; iojs\n$ nvm ls                        # list locally installed version\n$ nvm install 0.12.3            # install the version 0.12.3 (see ls-remote for available options)\n$ nvm uninstall 0.12.3          # uninstall node version 0.12.3\n$ nvm use 0.12.3                # switch to and use the installed 0.12.3 version\n$ nvm which 0.12.2              # the path to the installed node version\n$ nvm current                   # what is the current installed nvm version\n$ nvm alias default 0.10.32     # set the default node to the installed 0.10.32 version\n$ nvm --help                    # the help documents\n</code></pre>"},{"location":"Programming/nodejs-cli/packages/","title":"Popular Packages","text":"<p>Here is a list of commonly used packages that may be helpful</p>"},{"location":"Programming/nodejs-cli/packages/#application","title":"Application","text":"Package Name Usage express Fast, minimalist node framework body-parser Body parsing middleware. Parse incoming request bodies in a middleware before your handlers. cookie-parser Parse cookie header and populate req-cookies keyed by the cookie names app-root-path Helps you access application's root path from anywhere in the application without resorting to relative paths. request Simplest way to make HTTP(s) calls request-promise Add-on to request with Promise support uuid Simple, fast generation of RFC4122 UUIDs config Manage application configurations across environments yup Request validation"},{"location":"Programming/nodejs-cli/packages/#database","title":"Database","text":"Package Name Usage mysql knex Batteries included, multi-dialect sql query builder"},{"location":"Programming/nodejs-cli/packages/#logging","title":"Logging","text":"Package Name Usage morgan Web Server log package bunyan pino winston"},{"location":"Programming/nodejs-cli/packages/#development","title":"Development","text":"Package Name Usage got Node.js - powerful HTTP request library ky Browser - tiny and elegant HTTP request library"},{"location":"Programming/nodejs-cli/packages/#local-development","title":"Local Development","text":"Package Name Usage nodemon A replacement wrapper for node. Automatically restarts the application on file change detection dotenv Loads environment variables from .env file into process.env nonce Returns unique and ever increasing timestamps"},{"location":"Programming/nodejs-cli/packages/#advanced","title":"Advanced","text":"Package Name Usage cluster Multi-core server manager for Node.js pm2 Process manager for node.js with built-in load balancer"},{"location":"Programming/nodejs-cli/packages/#interesting-packages-to-check-out","title":"Interesting packages to check out","text":"<pre><code>https://www.npmjs.com/package/memwatch-next\nhttps://www.npmjs.com/package/heapdump\nhttps://www.npmjs.com/package/autocannon\n</code></pre>"},{"location":"Programming/nodejs-cli/process-manager/","title":"Node.js Process Manager","text":"<p>The most challenging aspect when it comes to maintaining an application is keeping it alive and running. Additionally with Node.js, scaling a single threaded process can be hard to do; that\u2019s where Process managers comes in.</p>"},{"location":"Programming/nodejs-cli/process-manager/#process-managers-out-there","title":"Process Managers out there","text":"<ul> <li>PM2</li> <li>SystemD - https://en.wikipedia.org/wiki/Systemd</li> <li>Forever - https://github.com/foreverjs/forever</li> <li>StrongLoop's Process Manager - http://strong-pm.io/</li> </ul>"},{"location":"Programming/nodejs-cli/process-manager/#pm2","title":"PM2","text":"<p>PM2 is a battle tested, production ready runtime and process manager for Node.js applications. It comes with a built-in load balancer, as well, which makes scaling applications even easier. Best of all, it works on Linux, Windows, and macOS. You specify the process.json file and PM2 takes care of the rest.</p> <p>What all of this means is that PM2 allows you to keep your Node.js applications alive forever, and to reload them with zero downtime when you have updates to your application or server.</p>"},{"location":"Programming/nodejs-cli/process-manager/#cheatsheet-process-management","title":"Cheatsheet - Process Management","text":"<pre><code>$ pm2 start process_prod.json  # Start process(es) via process JSON file\n$ pm2 ls                       # Show a list of all applications\n$ pm2 stop &lt;app&gt;               # Stops a specific application\n$ pm2 start &lt;app&gt;              # Starts a specific application\n$ pm2 start &lt;app&gt; --watch      # Starts a specific application with watch enabled\n$ pm2 delete &lt;app&gt;             # stop and delete a process from the list\n$ pm2 kill                     # Kills all running applications\n$ pm2 restart                  # Restarts all running applications. App is unavailable for a short time\n$ pm2 reload                   # Reloads the app configuration (handy when you modify env variables)\n$ pm2 &lt;app&gt; scale N            # Scales the application to N number of instances (up or down)\n$ pm2 serve &lt;path&gt; &lt;port&gt;      # serve static files (like a frontend app) over HTTP\n</code></pre>"},{"location":"Programming/nodejs-cli/process-manager/#cheatsheet-stats","title":"Cheatsheet - Stats","text":"<pre><code>$ pm2 show &lt;id/name&gt;        # to get more details about an app\n$ pm2 describe &lt;id/name&gt;        # same as above\n$ pm2 env &lt;id/name&gt;         # to display environement variables\n$ pm2 monit                 # to monitor CPU and Memory usage server\n</code></pre>"},{"location":"Programming/nodejs-cli/process-manager/#cheatsheet-log-management","title":"Cheatsheet - Log management","text":"<pre><code>$ pm2 logs             # Outputs logs from all running applications\n$ pm2 logs &lt;app&gt;       # Outputs logs from only the app application\n$ pm2 flush            # Flushes all log data, freeing up disk space\n\nPM2 Modules\n$ pm2 install pm2-logrotate    # enable log rotation\n$ pm2 install pm2-server-monit # monitor the current server with more than 20+ metrics and 8 actions\n</code></pre>"},{"location":"Programming/nodejs-cli/process-manager/#cheatsheet-tips","title":"Cheatsheet - Tips","text":"<pre><code>$ pm2 completion install    # By default, CLI autocompletion is not installed with PM2\n$ pm2 reload all            # Zero downtime reload. Hot Reload allows to update an app without any downtime\n$ pm2 update                # Keep pm2 updated\n</code></pre>"},{"location":"Programming/nodejs-cli/process-manager/#cheatsheet-cluster-mode","title":"Cheatsheet - Cluster Mode","text":"<pre><code>In the context of clustering, you first need to be sure that your application has no internal state.\nAn internal state is typically some local data stored into its processes.\nIt can be an array of websocket connections or a local session-memory for example.\nUse Redis or other databases instead to share the states between processes.\n\n$ pm2 start api.js -i &lt;processes&gt;    # Starting an app in cluster mode that'll leverage all CPUs available\n$ pm2 start api.js -i max            # PM2 will auto-detect number of available CPUs &amp; run as many processes\n</code></pre>"},{"location":"Programming/nodejs-cli/process-manager/#cheatsheet-startup-hooks-generation","title":"Cheatsheet - Startup hooks generation","text":"<pre><code>$ pm2 startup      # Generate Startup Script\n$ pm2 save         # Freeze your process list across server restart\n$ pm2 unstartup    # Remove Startup Script\n</code></pre>"},{"location":"Programming/nodejs-cli/process-manager/#cheatsheet-ecosystem-file","title":"Cheatsheet - EcoSystem File","text":"<pre><code>Purpose of the ecosystem file is to gather all options and environment variables for all your applications\n\n$ pm2 init                    # Generate an ecosystem.config.js template\n\n$ pm2 start ecosystem.config.js                     # uses variables from `env`\n$ pm2 start ecosystem.config.js --env production    # uses variables from `env_production`\n$ pm2 start --only &lt;app_name&gt;    # Use ecosystem file only on a specific application\n\n$ pm2 restart ecosystem.config.js --update-env      # force refresh the environment\n</code></pre>"},{"location":"Programming/nodejs-cli/process-manager/#references","title":"References","text":"<pre><code>https://www.npmjs.com/package/pm2\nhttps://hackernoon.com/running-pm2-node-js-in-production-environments-13e703fc108a\nhttps://pm2.io/doc/en/runtime/overview/\nhttps://pm2.io/doc/en/runtime/guide/log-management/\nhttps://pm2.io/doc/en/runtime/guide/startup-hook/\n</code></pre>"},{"location":"Programming/nodejs-cli/todo/","title":"REVIEW FIRST","text":""},{"location":"Programming/nodejs-cli/todo/#review-links","title":"Review links","text":"<ul> <li>https://publish-subscribe.js.org/#/<ul> <li>https://github.com/r37r0m0d3l/publish_subscribe</li> </ul> </li> <li>https://r37r0m0d3l.github.io/json_sort/<ul> <li>https://github.com/r37r0m0d3l/json_sort</li> </ul> </li> <li>https://www.coderstool.com/json-sort</li> <li>https://github.com/await-of/of</li> </ul>"},{"location":"Programming/nodejs-cli/v8-engine/","title":"Google V8 Engine","text":"<p>Google V8 is a JavaScript engine initially created for Google Chrome, but it can also be used as a standalone. This makes it the perfect fit for Node.js, and it is the only part of the platform that actually \u201cunderstands\u201d JavaScript.</p> <p>V8 compiles JavaScript down to native code and executes it. During execution, it manages the allocation and freeing of memory as needed. This means that if we talk about memory management in Node.js we actually always talk about V8.</p>"},{"location":"Programming/nodejs-cli/v8-engine/#v8s-memory-scheme","title":"V8\u2019s memory scheme","text":"<p>A running program is always represented through some space allocated in memory. This space is called Resident Set. V8 uses a scheme similar to the Java Virtual Machine and divides the memory into segments:</p> <ul> <li>Code: the actual code being executed</li> <li>Stack: contains all value types (primitives like integer or Boolean) with pointers referencing objects on the heap and pointers defining the control flow of the program</li> <li>Heap: a memory segment dedicated to storing reference types like objects, strings, and closures</li> </ul> <p>Within Node.js, the current memory usage can easily be queried by calling <code>process.memoryUsage()</code>. This function will return an object containing:</p> <ul> <li>Resident set size</li> <li>Total size of the heap</li> <li>Heap actually used</li> </ul>"},{"location":"Programming/nodejs-cli/v8-engine/#garbage-collection","title":"Garbage Collection","text":"<p>The used heap graph is highly volatile but always stays within certain boundaries to keep the median consumption constant. The mechanism that allocates and frees heap memory is called garbage collection.</p> <p>If the graph constantly goes up even with the saw-tooth pattern, there is potentially a memory leak.</p> <p>If a program allocates memory that is never freed, the heap will constantly grow until the usable memory is exhausted, causing the program to crash. We call this a \"memory leak\"</p> <p>Garbage collection in Node.js is solely managed by V8. This means that we cannot actively allocate or deallocate memory in JavaScript.</p> <p>Garbage collection is a rather costly process because it interrupts the execution of an application, which naturally impacts its performance. To remedy this situation, V8 uses two types of garbage collection:</p> <ul> <li>Scavenge: fast but incomplete</li> <li>Mark-Sweep: relatively slow but frees all non-referenced memory</li> </ul> <p>By using the native module node-gc-profiler, we can gather even more information about garbage collection runs. The module subscribes to garbage collection events fired by V8 and exposes them to JavaScript. The object returned indicates the type of garbage collection and the duration.</p> <p>Wouldn\u2019t it be great if we could have a look into our heap to see what\u2019s currently in there? Fortunately, we can! V8 provides a way to dump the current heap, and V8-profiler exposes this functionality to JavaScript. If there is a memory leak, you may end up with a significant number of such files. You should monitor this closely and add some alerting capabilities to that module.</p>"},{"location":"Programming/nodejs-cli/v8-engine/#references","title":"References","text":"<pre><code>https://blog.codeship.com/understanding-garbage-collection-in-node-js/\nhttp://jayconrod.com/posts/55/a-tour-of-v8-garbage-collection\n\nhttps://www.xarg.org/2016/06/forcing-garbage-collection-in-node-js-and-javascript/\nhttps://medium.com/front-end-weekly/my-node-js-memory-leak-and-some-memory-management-and-garbage-collection-6281a5308b4e\n</code></pre>"},{"location":"Programming/nodejs-cli/package-manager/","title":"Package Managers","text":"<p>Package managers have evolved over the last few years. And because of that, Node has a few different options. Most popular of them being <code>npm</code></p> <pre><code># List all globally installed dependencies\nnpm list -g\n\n# find out where global packages are installed by default\nnpm config get prefix\n</code></pre>"},{"location":"Programming/nodejs-cli/package-manager/#options","title":"Options","text":"<ul> <li>npm</li> <li>yarn</li> <li>pnpm</li> </ul>"},{"location":"Programming/nodejs-cli/package-manager/npm/","title":"NPM (Node Package Manager)","text":"<p>Most popular choice for Node as a package manager. It puts modules in place so that node can find them, and manages dependency conflicts intelligently. It is extremely configurable to support a wide variety of use cases. Most commonly, it is used to publish, discover, install, and develop node programs.</p> <p>There is no need for additional installation steps. It gets installed automatically during Node.js installation.</p>"},{"location":"Programming/nodejs-cli/package-manager/npm/#installation","title":"Installation","text":""},{"location":"Programming/nodejs-cli/package-manager/npm/#option-1","title":"Option 1","text":"<pre><code># Option 4 -- Installs or updated NVM (Node Version Manager)\n$ curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.7/install.sh | bash\n$ nvm install 21              # install node 21\n$ nvm alias default 21        # to make node 21 the default\n</code></pre>"},{"location":"Programming/nodejs-cli/package-manager/npm/#cheatsheet","title":"Cheatsheet","text":"<pre><code># Clean install\n$ npm ci\n\n# Save config in .npmrc\n$ npm config set save=true\n$ npm config set save-exact=true\n\n# You may want to use prexxx and postxxx scripts in package.json\n</code></pre>"},{"location":"Programming/nodejs-cli/package-manager/npm/#npx","title":"NPX","text":"<p>A tool intended to let you run CLI of an NPM package without having to install them globally. NPX looks up in existing node_modules for the folder to find the executable first. You could do that same thing by referencing <code>.bin</code> folder in command-line, however this tool makes it simpler. It comes bundled with NPM starting version 5.2.0 and onwards.</p> <pre><code>$ npx eslint .\n</code></pre>"},{"location":"Programming/nodejs-cli/package-manager/npm/#commands-reference","title":"Commands Reference","text":""},{"location":"Programming/nodejs-cli/package-manager/npm/#commands-basic","title":"Commands - Basic","text":"<pre><code># Display current version installed\n$ npm --version\n\n# Updates npm version if an update is available\n$ npm i -g npm\n\n# Help on NPM\n$ npm help npm\n\n# Displays full usage info\n$ npm -l\n\n# Starting a new project and generate package.json\n$ npm init\n\n# List all npm configuration flags (regardless of project)\n$ npm config ls -l\n</code></pre>"},{"location":"Programming/nodejs-cli/package-manager/npm/#commands-package-management","title":"Commands - Package management","text":"<pre><code>#  Installs dependencies for the current project listed in its `package.json`\n$ npm install\n\n# List all global dependencies\n$ npm list -g --depth=0\n\n# Display Node and NPM configs\n$ npm config list\n\n# Short-hand/alias for npm install\n$ npm i\n\n# Installs a package globally\n$ npm i -g &lt;package&gt;\n\n# Installs package and adds it as a dependency in package.json\n$ npm install --save &lt;package_name&gt;\n\n# Installs package and adds it as a DEV-dependency in package.json\n$ npm install --save-dev &lt;package_name&gt;\n\n# Installs package and adds exact version as a dependency in package.json\n$ npm install --save-exact &lt;package_name&gt;\n\n$ npm uninstall &lt;package&gt;\n</code></pre>"},{"location":"Programming/nodejs-cli/package-manager/npm/#command-updating","title":"Command - Updating","text":"<pre><code># Update production packages\n$ npm update\n\n# Update dev packages\n$ npm update --dev\n\n# Update global packages\n$ npm update -g\n\n# Update a specific package\n$ npm update &lt;package_name&gt;\n\n# Check for outdated packages\n$ npm outdated &lt;package_name&gt;\n</code></pre>"},{"location":"Programming/nodejs-cli/package-manager/npm/#command-misc","title":"Command - Misc","text":"<pre><code># List packages\n$ npm ls\n\n# Adds warning for old versions\n$ npm deprecate PACKAGE@\"&lt; 0.2.0\" \"critical bug fixed in v0.2.0\"\n\n# Install from an absolute path\n$ npm i /path/to/repo\n</code></pre>"},{"location":"Programming/nodejs-cli/package-manager/npm/#reference","title":"Reference","text":"<ul> <li>https://docs.npmjs.com/</li> <li>https://www.npmjs.com/docs/orgs/</li> <li>https://npme.npmjs.com/docs/</li> <li>https://gist.github.com/AvnerCohen/4051934</li> </ul>"},{"location":"Programming/nodejs-cli/package-manager/pnpm/","title":"pnpm (Performance npm)","text":"<p>The motivation behind this project is to address the shortcomings of <code>npm</code>.</p> <p>Main issues when using npm:</p> <ol> <li><code>Disk space</code> - each project downloads its own copy of the dependent libraries, hence consuming a lot of disk space if you work on multiple projects</li> <li><code>Installation speed</code> - since each project downloads it's own dependencies, the time to get those dependencies increases as you add more of them.</li> </ol> <p>How pnpm solves this problem? Just like <code>git</code>, <code>pnpm</code> stores the delta between different versions of the same library, and in a central location. hence when a package is installed, its files are hard-linked (symlinks) from that single place, consuming no additional disk space.</p>"},{"location":"Programming/nodejs-cli/package-manager/pnpm/#installation","title":"Installation","text":"<pre><code># Using brew\n$ brew install pnpm\n\n# Using npm\n$ npm install -g pnpm\n</code></pre>"},{"location":"Programming/nodejs-cli/package-manager/pnpm/#usage","title":"Usage","text":"<p>Very similar to <code>npm</code> usage. Refer this</p> <pre><code># Update all packages in your project\npnpm update\n\n# Identify the dependency that depend on a packages\npnpm list [package-name]\n\n# Check for newer versions of a package\npnpm outdated [package-name]\n\n# Update package to a specific version\npnpm add [package-name]@[version]\n</code></pre>"},{"location":"Programming/nodejs-cli/package-manager/pnpm/#resources","title":"Resources","text":"<ul> <li>Official docs</li> </ul>"},{"location":"Programming/nodejs-cli/package-manager/yarn/","title":"Yarn (Package Manager)","text":"<p>An upcoming popular choice as a Package manager, developed by Facebook</p>"},{"location":"Programming/nodejs-cli/package-manager/yarn/#official-documentation","title":"Official Documentation","text":"<pre><code>https://yarnpkg.com/en/docs\nhttps://yarnpkg.com/en/packages\n</code></pre>"},{"location":"Programming/nodejs-cli/package-manager/yarn/#installation","title":"Installation","text":"<pre><code>Option 1\n========\n$ brew install yarn                    // Installs Yarn. Also installs Node.js if not already installed.\n$ brew install yarn --without-node     // Installs Yarn without Node.js, if you use nvm or similar\n$ brew upgrade yarn                    // To upgrade Yarn. Yarn will warn you if a new version is available.\n\nOption 2\n========\n$ npm install --global yarn\n\nRegardless of how you choose to install, Add yarn to PATH via .bash_profile\n$ export PATH=\"$PATH:/opt/yarn-[version]/bin\"\n</code></pre>"},{"location":"Programming/nodejs-cli/package-manager/yarn/#commands","title":"Commands","text":"<pre><code>$ yarn --version                        // Verify the version installed\n$ yarn init                             // Starting a new project\n$ yarn add &lt;package&gt;                    // Install a package and update dependencies in package.json\n$ yarn add &lt;package&gt; --dev              // Install a package and update dev dependencies in package.json\n$ yarn add &lt;package&gt; --exact            // Install a package with exact version and and update dependencies\n$ yarn remove &lt;package&gt;                 // Removing a dependency\n$ yarn install                          // Install all dependencies of the project\n$ yarn                                  // Short-hand for yarn install\n</code></pre>"},{"location":"Programming/python/","title":"Python","text":"<p>Official Documentation</p>"},{"location":"Programming/python/#running-online","title":"Running online","text":"<p>Google Colab allows you to write and execute Python notebooks in your browser, without having to install a dependency locally. You can save the notebooks in GitHub or Google drive directly.</p>"},{"location":"Programming/python/#review-links","title":"Review Links","text":"<ul> <li>Fast API</li> <li>Using FastAPI to Build Python Web APIs</li> </ul>"},{"location":"Programming/python/#additional-readings","title":"Additional Readings","text":"<ul> <li>Clean Architectures in Python by Leonardo Giordani (Digital Cat Books)</li> <li>Architecture Patterns with Python by Harry J.W. Percival and Bob Gregory (O\u2019Reilly)</li> <li>Microservice APIs by Jos\u00e9 Haro Peralta (Manning)</li> <li>Domain-driven design | Wikipedia</li> </ul>"},{"location":"Programming/python/cheatsheet-conda/","title":"Conda Cheatsheet","text":""},{"location":"Programming/python/cheatsheet-conda/#conda-basics","title":"Conda Basics","text":"<pre><code># Verify conda is installed, check version number\n$ conda info\n\n# Update conda to the current version\n$ conda update conda\n\n# Install a package included in Anaconda\n$ conda install &lt;package&gt;\n# Install multiple packages in a single command\n$ conda install numpy matplotlib seaborn scikit-learn\n\n# Run a package after install. Example *Spyder\n$ spyder\n\n# Update any installed program\n$ conda update &lt;package&gt;\n\n# Command line help\n$ conda install --help\n\n# Open Anaconda Navigator\n$ anaconda-navigator\n</code></pre>"},{"location":"Programming/python/cheatsheet-conda/#using-environments","title":"Using Environments","text":"<pre><code># Create new environment named py39, install Python 3.9\n$ conda create --name py39 python=3.9\n\n# Activate the new environment to use it\n$ source activate py39\n\n# Deactivate the current environment\n$ source deactivate\n\n# List all my environments. Active env is shown with env\n$ conda env list\n\n# Make exact copy of an environment\n$ conda create --clone py39 --name py39-2\n\n# List all Packages and versions installed in active environment\n$ conda list\n\n# List the history of each change to the current environment\n$ conda list --revisions\n\n# Restore environments to a previous revision\n$ conda install --revision 2\n\n# Save environment to a text file\n$ conda list --explicit &gt; bio-env.txt\n\n# Delete an environment and everything in it\n$ conda env remove --name bio-env\n\n# Create environment from a text file\n$ conda env create --file bio-env.txt\n\n# Stack commands: create a new env, name it and install biopython package\n$ conda create --name bio-env biopython\n</code></pre>"},{"location":"Programming/python/cheatsheet-conda/#finding-conda-packages","title":"Finding Conda packages","text":"<pre><code># Use Conda to search for a package\n$ conda search &lt;package&gt;\n\n# See list of all packages in Anaconda\n$ https://docs.anaconda.com/a\n</code></pre>"},{"location":"Programming/python/cheatsheet-conda/#installing-and-updating-packages","title":"Installing and Updating packages","text":"<pre><code># install a new package (Jupyter Notebook) in active env\n$ conda install jupyter\n\n# Run an installed package (Jupyter Notebook)\n$ jupyter-notebook\n\n# Install a new package toolz in a different env (bio-env)\n$ conda install --name bio-env toolz\n\n# Update a package in current environment\n$ conda update scikit-learn\n\n# Install a package from a specified channel\n$ conda install --channel conda-forge boltons\n\n# Install a package directly from PiPy in current env using Pip\n$ pip install boltons\n\n# Remove one of more packages from a specified env\n$ conda remove --name bio-env toolz boltons\n</code></pre>"},{"location":"Programming/python/manager-build-backend/","title":"Build Backend","text":"<p>Tools like <code>pip</code> and <code>build</code> do not actually convert your sources into a <code>distribution package</code> (like a <code>wheel</code>); that job is performed by a <code>build backend</code>. The build backend determines how your project will specify its configuration, including metadata and input files. Build backends have different levels of functionality.</p> <p>Some of the popular build backends are: hatchling, Setuptools, Flit, PDM, and others.</p>"},{"location":"Programming/python/manager-build-backend/#configuring-metadata","title":"Configuring Metadata","text":"<p>Create a <code>pyproject.toml</code>, which is very similar to <code>package.json</code> if you're familiar.</p>"},{"location":"Programming/python/manager-build-backend/#development","title":"Development","text":""},{"location":"Programming/python/manager-build-backend/#generating-distribution-archives","title":"Generating distribution archives","text":"<pre><code># Make sure you have the latest version of the build tool\npython3 -m pip install --upgrade build\n\n# Run this command from the same directory where pyproject.toml is located\npython3 -m build\n\n# This should output a lot of text and once completed should generate two files in the dist directory, with extensions `tar.gz` and `.whl`\n</code></pre>"},{"location":"Programming/python/manager-build-backend/#uploading-the-distribution-archives","title":"Uploading the distribution archives","text":"<p>The first thing you\u2019ll need to do is register an account on TestPyPI, which is a separate instance of the package index intended for testing and experimentation. To securely upload your project, you\u2019ll need a PyPI API token.</p> <pre><code>python3 -m pip install --upgrade twine\n\npython3 -m twine upload --repository testpypi dist/*\n</code></pre>"},{"location":"Programming/python/manager-build-backend/#resources","title":"Resources","text":"<ul> <li>Packaging Python Projects</li> <li>Configuring metadata</li> <li>What is pyproject.toml file for? | StackOverflow</li> <li>What Are Python Wheels and Why Should You Care?</li> <li>Twine is a utility for publishing Python packages to PyPI and other repositories. It provides build system independent uploads of source and binary distribution artifacts for both new and existing projects.</li> </ul>"},{"location":"Programming/python/manager-package-pip/","title":"Pip Package Manager","text":"<p>There are few package managers specific for Python, and pip is the preferred one.</p> <p>Homebrew doesn't know how to install pip or distribute. Luckily both can be easily installed with python scripts available on web, if needed. <code>pip</code> is shipped with python by default.</p>"},{"location":"Programming/python/manager-package-pip/#installation","title":"Installation","text":"<pre><code>$ curl -O https://raw.github.com/pypa/pip/master/contrib/get-pip.py\n$ python get-pip.py\n\nVersion check\n$ pip -V\n$ pip2 -V\n$ pip3 -V\n\n# Upgrading pip\n$ pip install -U pip\n\n# Check for any conflicts in installed packages\n$ pip check\n\n# Update list of pinned requirements\n$ pip freeze &gt; requirements.txt\n</code></pre>"},{"location":"Programming/python/manager-package-pip/#packages","title":"Packages","text":"<pre><code># Vectors and matrices in Python\n$ pip install numpy\n\n# Other scientific computing tools\n$ pip install scipy\n\n# Dataframes in Python!\n$ pip install pandas\n\n# Most commonly used Biocomputing Python library\n$ pip install biopython\n\n# IPython Notebook is part of Jupyter now. Another populate Biocomputing tool\n$ pip install jupyter\n</code></pre> <p>Note: If you already have a <code>requirements.txt</code> file listing all desired dependencies, then run <code>$ pip install -r requirements.txt</code> or <code>$ pip install --requirement requirements.txt</code>.</p> <p>Additional options:</p> <ul> <li><code>-U</code> - upgrade if it had already installed</li> <li><code>--no-index</code> - ignore package index (only looking at <code>--find-links</code> url instead)</li> <li><code>-f, --find-links &lt;URL&gt;</code> - If a URL or path to an HTML file, then parse for links to archives. If a local path or <code>file://</code> URL that's a directory, then look for archives in the directory listing.</li> </ul>"},{"location":"Programming/python/manager-package-pip/#special-case","title":"Special case","text":"<p>Often, you will want a fast install from local archives, without probing PyPI.</p> <p>First, download the archives that fulfill your requirements:</p> <pre><code>$ pip install --download &lt;DIR&gt; -r requirements.txt\n</code></pre> <p>Then, install using \u2013find-links and \u2013no-index:</p> <pre><code>$ pip install --no-index --find-links=[file://]&lt;DIR&gt; -r requirements.txt\n</code></pre>"},{"location":"Programming/python/manager-package-pip/#managing-requirements","title":"Managing requirements","text":"<ul> <li>Create and active your virtual environment</li> <li>Installed desired dependencies in the virtual environment</li> <li>Call <code>$pip freeze &gt; requirements.txt</code> to export all installed dependencies into the file.</li> </ul> <p>Note: once you export all dependencies, it may require some cleanup because of previously installed dependencies in the source copy of python dependencies.</p>"},{"location":"Programming/python/manager-package-poetry/","title":"Poetry Package Manager","text":"<p>poetry is a dependency/package and virtual environment manager. Poetry comes with all the tools you might need to manage your projects in a <code>deterministic</code> way.</p>"},{"location":"Programming/python/manager-version/","title":"Python Version Manager","text":""},{"location":"Programming/python/manager-version/#installation-wo-version-manager","title":"Installation w/o version manager","text":"<pre><code># Install latest python3 version\n$ brew install python3\n\n# Installs specific python version\n$ brew install python@3.12\n</code></pre> <p>Verify installation using</p> <pre><code># Find the user base binary directory\n$ python -m site --user-base\n\n$ python\n# Find if installed version is 32 bit or 64 bit\n&gt; import struct;print(struct.calcsize(\"P\") * 8)\n</code></pre>"},{"location":"Programming/python/manager-version/#using-pyenv","title":"Using pyenv","text":"<p><code>pyenv</code> is a tool that allows you to manage multiple versions of Python on your machine. It's not specifically designed for creating virtual environments, but it allows you to switch between different versions of Python on the fly.</p> <pre><code>$ brew install pyenv\n$ eval \"$(pyenv init -)\"\n\n$ brew install pyenv-virtualenv\n$ eval \"$(pyenv virtualenv-init -)\"\n\n$ pyenv virtualenv venv27\n# shows you the list of existing virtualenvs and conda environments\n$ pyenv virtualenvs\n\n$ pyenv shell venv27\n</code></pre>"},{"location":"Programming/python/manager-version/#using-anaconda","title":"Using Anaconda","text":"<pre><code># Install different version of Python in a new environment named py39\n$ conda create -n py39 python=3.9 anaconda\n$ conda create --name py39 python=3.9\n\n# Switch to the new environment that has a different version of Python\n$ conda activate py39\n$ source activate py39\n\n# Show the locations of all versions of Python that are currently in the path\n$ which -a python\n\n# Show version information for the current active Python\n$ python --version\n</code></pre>"},{"location":"Programming/python/manager-version/#references","title":"References","text":"<ul> <li>pyenv</li> </ul>"},{"location":"Programming/python/notebook-colab/","title":"Google Colab","text":"<p>Colab, short for Google Colaboratory, is a free cloud service based on Jupyter Notebooks for machine learning education and research. It provides a platform where you can use and share Jupyter notebooks with others without having to download, install, or run anything on your own computer.</p> <p>Here's a basic guide on how to use Google Colab:</p> <ol> <li>Go to the Google Colab website.</li> <li>Sign in with your Google account.</li> <li>Click on <code>File</code> &gt; <code>New notebook</code> to create a new notebook.</li> <li>You can write Python code in the cells. To run the code in a cell, click on the play button on the left side of the cell or use the keyboard shortcut <code>Shift + Enter</code>.</li> <li>You can add new cells by clicking on <code>+ Code</code> or <code>+ Text</code> buttons in the toolbar. <code>+ Code</code> adds a new cell for writing code while <code>+ Text</code> adds a new cell for writing text.</li> <li>You can import data files or notebooks by clicking on <code>File</code> &gt; <code>Upload notebook</code> or <code>File</code> &gt; <code>Upload dataset</code>.</li> <li>You can save your notebook by clicking on <code>File</code> &gt; <code>Save</code>. The notebook will be saved to your Google Drive.</li> <li>You can share your notebook with others by clicking on the <code>Share</code> button in the top right corner. You can choose to share the whole notebook or a link to the notebook.</li> </ol> <p>Remember, Google Colab provides a runtime environment, so you don't have to worry about setting up Python or any packages on your local machine. It also provides free access to hardware accelerators like GPUs and TPUs which can be very useful for machine learning tasks.</p>"},{"location":"Programming/python/notebook-jupyter/","title":"Jupyter Notebook","text":""},{"location":"Programming/python/notebook-jupyter/#installation","title":"Installation","text":"<pre><code>$ pip install notebook\n</code></pre>"},{"location":"Programming/python/notebook-jupyter/#running","title":"Running","text":"<pre><code># Running Notebook\n$ jupyter notebook\n</code></pre> <p>Jupyter Notebooks are stored in the directory from which you launched the Jupyter Notebook server.</p> <p>In the address bar, type localhost:8888 and press Enter. This is the default address for Jupyter Notebook. If you have changed the port number during the installation or startup of Jupyter Notebook, replace 8888 with your port number.</p>"},{"location":"Programming/python/virtual-environment/","title":"Virtual Environment","text":"<p>It is always a good idea to work in a virtual environment. This way you'll never have to deal with dependency version management in your local machine or cleaning up after your work is done. Sure, it does take more space on your disk, however the benefits far outweigh the cons.</p> <p>For each project, instead of installing the required packages globally, it is best to install them in an isolated folder within the project itself (commonly named <code>venv</code>). The advantage is that different projects might require different versions of packages, and it would be hard to manage that if you install packages globally. It also allows you to keep your global /usr/local/lib/python2.7/site-packages folder clean, containing only critical or big packages that you always need (like IPython, Numpy).</p>"},{"location":"Programming/python/virtual-environment/#venv","title":"venv","text":"<p><code>venv</code> is a built-in virtual environment module in Python 3.3 and onwards, that allows you to create virtual environments in Python. It creates a new Python environment with its own site directories, which can be used to install and manage packages for specific projects. It's simple, lightweight, and easy to use.</p>"},{"location":"Programming/python/virtual-environment/#usage","title":"Usage","text":"<pre><code># To run the venv module as a standalone program\n$ venv &lt;env_name&gt;\n\n# Create use the environment as a Python module\n$ python3 -m venv &lt;env_name&gt;\n$ python3 -m venv venv\n\n# Active virtual environment as current environment\n$ source venv/bin/activate\n\n# To leave the virtual environment\n$ deactivate\n</code></pre>"},{"location":"Programming/python/virtual-environment/#virtualenv","title":"VirtualEnv","text":"<p>A third-party tool that creates an isolated Python environments. It works with both Python 2 and 3 and allows you to create virtual environments with different Python versions.</p>"},{"location":"Programming/python/virtual-environment/#installation","title":"Installation","text":"<pre><code># Installation\n$ pip install virtualenv\n\n# Verify the version\n$ virtualEnv --version\n</code></pre>"},{"location":"Programming/python/virtual-environment/#setup","title":"Setup","text":"<pre><code># Setup virtual env for the project myProject\n$ cd myProject &amp;&amp; virtualenv venv\n\n# Have virtualenv inherit globally installed packages\n$ virtualenv venv --system-site-packages\n\n# Note: These commands create a venv sub-directory in your project where everything is installed.\n\n# Create Python3 Virtual Environment\n$ virtualEnv -p python3 venv\n</code></pre> <p>Note: Remember, virtual environment can only be created based off an existing python version installed locally.</p>"},{"location":"Programming/python/virtual-environment/#usage_1","title":"Usage","text":"<p>To use a virtual environment, uou need to activate it first (in every terminal window, where you are working on your project).</p> <pre><code># Active virtual environment\n$ source venv/bin/activate\n\n# You should see a (venv) appear at the beginning of your terminal prompt indicating that you are working inside the virtualenv. Now when you install anything, it will be installed in the venv folder, and not conflict with other projects\n\n# To leave the virtual environment\n$ deactivate\n</code></pre> <p>Note: Remember to add venv to your project's .gitignore file so you don't include all of that in your source code! NOte: Preferably install big packages (like Numpy), or ones you always use (like IPython) globally. Rest can be in virtualenv.</p>"},{"location":"Programming/python/virtual-environment/#virtualenvwrapper","title":"Virtualenvwrapper","text":"<p>Used for easier management of different virtual environments for multiple packages. Extension to <code>virtualEnv</code>.</p>"},{"location":"Programming/python/virtual-environment/#features","title":"Features","text":"<ol> <li>Organizes all of your virtual environments in one place (~/.virtualenv). Does not add them to the project directory.</li> <li>Wrappers for managing your virtual environments (create, delete, copy).</li> <li>Use a single command to switch between environments.</li> <li>Tab completion for commands that take a virtual environment as argument.</li> <li>User-configurable hooks for all operations.         http://virtualenvwrapper.readthedocs.io/en/latest/scripts.html#scripts</li> <li>Plugin system for more creating sharable extensions.         http://virtualenvwrapper.readthedocs.io/en/latest/plugins.html#plugins</li> </ol> <p>Note: if you have anaconda installed, the packages are installed at location: (probably have virtualenvwrapper already) 1. ~/anaconda2/lib/python2.7/site-packages 2. ~/.local/lib/python2.7/site-packages</p> <pre><code># Installation\n$ pip install virtualenvwrapper\n\n# Shell startup file\n$ source /usr/local/bin/virtualenvwrapper.sh\n</code></pre> <p>Instructions:: latest docs</p>"},{"location":"Programming/python/virtual-environment/#other-tools","title":"Other tools","text":"<p>This combination of pip and venv is so common that people started combining them to save steps, and avoid that source shell wizardry. One such package is pipenv, but a newer rival called poetry is becoming more popular.</p>"},{"location":"Programming/python/virtual-environment/#resources","title":"Resources","text":"<ul> <li>What is the difference between venv, pyvenv, pyenv, virtualenv, virtualenvwrapper, pipenv, etc?</li> <li>Understanding Python virtual environments using venv and virtualenv</li> </ul>"},{"location":"Programming/python/concurrency/","title":"Concurrency","text":"<p>Threading has its uses, but speeding up computations isn't one of them. Threading has more to do with waiting for input and notifying the other thread on that same core in a timely and SAFE manner, when you want to get a more \"real time\" response from user input as an example.</p> <p>To set some context:</p> <ul> <li><code>CPU-bound</code> programs are those that are pushing the CPU to its limit.</li> <li><code>I/O-bound</code> programs are the ones that spend time waiting for Input/Output. I/O-bound programs sometimes have to wait for a significant amount of time till they get what they need from the source due to the fact that the source may need to do its own processing before the input/output is ready</li> </ul>"},{"location":"Programming/python/concurrency/#global-interpreter-lock","title":"Global Interpreter Lock","text":"<p><code>Python's Global Interpreter Lock (GIL)</code>, a mutex (or a lock), allows only one CPU thread to execute Python bytecodes at a time, and hold the control of the Python interpreter. It can be a performance bottleneck in CPU-bound and multi-threaded code.</p>"},{"location":"Programming/python/concurrency/#problems-gil-solved","title":"Problems GIL solved","text":"<p>Python uses reference counting for memory management. It means that objects created in Python have a reference count variable that keeps track of the number of references that point to the object. When this count reaches zero, the memory occupied by the object is released.</p> <p>This reference count variable needed protection from race conditions where two threads increase or decrease its value simultaneously. Because it can cause either <code>leaked memory</code> that is never released or, even worse, <code>incorrectly release</code> the memory while a reference to that object still exists. This reference count variable can be kept safe by adding locks to all data structures that are shared across threads so that they are not modified inconsistently.</p> <p>The GIL is a single lock on the interpreter itself which adds a rule that execution of any Python bytecode requires acquiring the interpreter lock. This prevents deadlocks (as there is only one lock) and doesn\u2019t introduce much performance overhead. But it effectively makes any CPU-bound Python program single-threaded.</p> <p>In the multi-threaded version the GIL <code>prevented</code> the CPU-bound threads from executing in parallel. The GIL does not have much impact on the performance of I/O-bound multi-threaded programs as the lock is shared between threads while they are waiting for I/O.</p>"},{"location":"Programming/python/concurrency/#multiprocessing","title":"multiprocessing","text":"<p>The most popular way is to use a multi-processing approach where you use <code>multiple processes</code> instead of <code>threads</code>. Each Python process gets its own Python interpreter and memory space so the GIL won\u2019t be a problem. Python has a <code>multiprocessing</code> module which lets us create processes easily.</p> <p>The time won't drop by the same ratio as the number of threads because process management has its own <code>overheads</code>. Multiple processes are heavier than multiple threads, so, this could become <code>a scaling bottleneck</code>.</p>"},{"location":"Programming/python/concurrency/#alternative-interpreter","title":"Alternative interpreter","text":"<p>Python has multiple interpreter implementations. CPython, Jython, IronPython and PyPy, written in C, Java, C# and Python respectively, are the most popular ones. GIL exists only in the original Python implementation that is CPython. If your program, with its libraries, is available for one of the other implementations then you can try them out as well.</p> <pre><code># Find the interpreter being used\n&gt;&gt;&gt; import platform\n&gt;&gt;&gt; platform.python_implementation()\n'CPython'\n\n# Find the interpreter path\n&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.executable\n'/usr/local/opt/python@3.12/bin/python3.12'\n</code></pre>"},{"location":"Programming/python/concurrency/#modules-available","title":"Modules available","text":"<p>The choice between asyncio, multiprocessing, and threading in Python depends on the nature of the tasks your program needs to perform.</p> <ol> <li> <p>Asyncio: This is a good choice for <code>I/O-bound tasks</code>, especially when those tasks involve high-latency operations like network requests. Asyncio can handle thousands of network connections concurrently using a single thread, by using <code>non-blocking I/O operations</code> and an <code>event loop</code> to manage tasks. However, asyncio is not suitable for CPU-bound tasks because it cannot utilize multiple cores due to Python's Global Interpreter Lock (GIL).</p> </li> <li> <p>Threading: The threading module is also suitable for <code>I/O-bound tasks</code>, and it can be <code>simpler to use</code> than asyncio for straightforward cases. However, due to the GIL, it's not effective for CPU-bound tasks. Also, managing a large number of threads can lead to significant overhead.</p> </li> <li> <p>Multiprocessing: This module is effective for CPU-bound tasks because it bypasses the GIL by using separate processes instead of threads, allowing your program to utilize multiple cores. However, <code>inter-process communication</code> can be slower and more complex than inter-thread communication, and there is a <code>higher overhead</code> associated with starting a new process compared to starting a new thread.</p> </li> </ol> <p>In summary, asyncio, threading, and multiprocessing each have their strengths and are better suited to different types of tasks. It's important to understand the nature of the tasks your program will be performing in order to choose the most appropriate tool.</p>"},{"location":"Programming/python/concurrency/#note-on-performance","title":"Note on performance","text":"<p>Asyncio creates <code>a single thread</code> and uses an <code>event loop</code> to manage tasks, while the threading module creates a <code>separate thread for each task</code>. This allows multiple tasks to run in parallel, if possible, but it also means that each thread consumes system resources. Managing a large number of threads can lead to significant <code>overhead</code>, as each thread consumes system resources. This can become a bottleneck when dealing with a large number of concurrent tasks.</p> <p>Hence in this case, asyncio would be a better choice. However, it's important to note that asyncio requires a different programming style (<code>asynchronous</code>, using <code>coroutines</code> and the <code>async/await</code> syntax), which can be more complex than traditional multi-threading.</p>"},{"location":"Programming/python/concurrency/#additional-modules-to-explore","title":"Additional modules to explore","text":"<p>There are several other popular libraries in Python for concurrent programming that offer different features and performance characteristics. Here are a few:</p> <ol> <li> <p>concurrent.futures: This module is part of the standard library and provides a high-level interface for asynchronously executing callables. It provides a simple way to manage threads or processes, and also provides a way to handle timeouts, cancelling tasks, and getting results back from functions.</p> </li> <li> <p>greenlet: This is a third-party library that provides lightweight \"green\" threads, also known as coroutines. Greenlets run in the same thread and are scheduled cooperatively, which means they provide a way to write concurrent code in a sequential style. However, they do not utilize multiple cores.</p> </li> <li> <p>gevent: This is a third-party library that provides a high-level synchronous API on top of the libev or libuv event loop and greenlet. It's designed for high concurrency I/O-bound workloads and can handle thousands of network connections in a single thread.</p> </li> <li> <p>Twisted: This is a third-party library that is an event-driven networking engine. It supports TCP, UDP, SSL/TLS, IP multicast, Unix domain sockets, a large number of protocols, and much more. It's designed for high concurrency I/O-bound workloads.</p> </li> <li> <p>Tornado: This is a third-party library that provides a web server and an asynchronous networking library, which can scale to tens of thousands of open connections, making it ideal for long polling, WebSockets, and other applications that require a long-lived connection to each user.</p> </li> </ol> <p>Each of these libraries has its own strengths and weaknesses, and the best one to use depends on the specific requirements of your application.</p>"},{"location":"Programming/python/concurrency/#resources","title":"Resources","text":"<ul> <li>What Is the Python Global Interpreter Lock (GIL)?</li> <li>multiprocessing package</li> </ul>"},{"location":"Programming/python/concurrency/asyncio/","title":"asyncio","text":"<p>asyncio is a library to write single-threaded concurrent code using <code>coroutines</code>, <code>multiplexing I/O access over sockets</code> and other resources, running network clients and servers, and other related primitives, using the <code>async/await</code> syntax.</p> <p>asyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc. It is often a perfect fit for <code>IO-bound</code> and <code>high-level structured network code</code>.</p> <p>It provides a set of high-level APIs to:</p> <ul> <li><code>run Python coroutines</code> concurrently and have full control over their execution;</li> <li>perform <code>network IO and IPC</code>;</li> <li>control <code>subprocesses</code>;</li> <li>distribute tasks via <code>queues</code>;</li> <li><code>synchronize</code> concurrent code;</li> </ul> <p>Additionally, there are low-level APIs for library and framework developers to:</p> <ul> <li>create and manage <code>event loops</code>, which provide asynchronous APIs for <code>networking</code>, running <code>subprocesses</code>, handling <code>OS signals</code>, etc;</li> <li>implement efficient protocols using <code>transports</code>;</li> <li><code>bridge</code> callback-based libraries and code with async/await syntax.</li> </ul>","tags":["python-package","concurrency","parallelism","async","await"]},{"location":"Programming/python/concurrency/asyncio/#coroutines","title":"Coroutines","text":"<p><code>Coroutines</code> in Python are a form of asynchronous programming and are a feature of the asyncio library. They are a special type of function that can be <code>paused</code> and <code>resumed</code>, allowing Python to handle other tasks in the meantime. This is particularly useful for I/O-bound tasks, where waiting for input/output operations can lead to significant idle time.</p> <p>Coroutines are <code>lighter than threads</code> and can be <code>more efficient for I/O-bound tasks</code>. They allow you to write asynchronous code in a sequential style. However, they are <code>not suitable for CPU-bound tasks</code> as they do not utilize multiple cores of the CPU.</p> <p>Coroutines are defined using async def and are run by calling await on them. Here's a simple example:</p> <pre><code>import asyncio\n\nasync def hello_world():\n    print(\"Hello\")\n    await asyncio.sleep(1)\n    print(\"World\")\n\n# Running the coroutine\nasyncio.run(hello_world())\n</code></pre> <p>When <code>await asyncio.sleep(1)</code> is called, the coroutine pauses, allowing other tasks to run in the meantime.</p> <p>There is no hard limit on the number of coroutines you can create. Each coroutine takes up a small amount of memory, so the limit would be when your system runs out of memory to allocate for new coroutines. However, this number is typically quite large, and you're more likely to hit other system limits or performance issues before you hit this one.</p> <p>Having a large number of coroutines does not necessarily mean your program will perform better. The optimal number depends on many factors, including the nature of the tasks being performed and the specifics of the system the program is running on.</p>","tags":["python-package","concurrency","parallelism","async","await"]},{"location":"Programming/python/concurrency/asyncio/#sample-code","title":"Sample code","text":"<pre><code>import asyncio\n\nasync def print_numbers():\n    for i in range(10):\n        print(i)\n        await asyncio.sleep(1)\n\nasync def print_letters():\n    for letter in 'abcdefghij':\n        print(letter)\n        await asyncio.sleep(1)\n\ntask1 = asyncio.create_task(print_numbers())\ntask2 = asyncio.create_task(print_letters())\n\nawait task1\nawait task2\n</code></pre>","tags":["python-package","concurrency","parallelism","async","await"]},{"location":"Programming/python/concurrency/asyncio/#resources","title":"Resources","text":"<ul> <li>asyncio</li> </ul>","tags":["python-package","concurrency","parallelism","async","await"]},{"location":"Programming/python/concurrency/multiprocessing/","title":"multiprocessing","text":"<p>With CPU-intensive Python applications, the usual solution is to use multiple processes, and let the OS manage them. Python has a multiprocessing module for this.</p> <p><code>multiprocessing</code> is a package that supports spawning processes using an API similar to the threading module. The multiprocessing package offers both local and remote concurrency, effectively side-stepping the Global Interpreter Lock by using subprocesses instead of threads. Due to this, the multiprocessing module allows the programmer to fully leverage multiple processors on a given machine.</p>","tags":["python-package","concurrency","parallelism"]},{"location":"Programming/python/concurrency/multiprocessing/#single-core-cpu","title":"Single-core CPU","text":"<p>Single-core does not mean single process for Python. The multiprocessing module in Python creates separate processes, which are independently scheduled by the operating system. This means that even on a single-core CPU, the operating system will switch between the processes, giving each a slice of CPU time. This is known as <code>context switching</code> and it gives the illusion of simultaneous execution.</p>","tags":["python-package","concurrency","parallelism"]},{"location":"Programming/python/concurrency/multiprocessing/#multi-core-cpu","title":"Multi-core CPU","text":"<p>The benefits of multiprocessing are more pronounced on a multi-core CPU, where each process can run on a separate core truly in parallel. On a single-core CPU, the overhead of context switching between processes can sometimes make multiprocessing less efficient than single-threaded execution for CPU-bound tasks. </p>","tags":["python-package","concurrency","parallelism"]},{"location":"Programming/python/concurrency/multiprocessing/#processes-per-cpu","title":"Processes per CPU","text":"<p>The number of processes that can be started on a single-core CPU is not directly limited on a single core. The operating system's scheduler is responsible for managing processes and threads, and it can handle many processes at once, on a single-core CPU.</p> <p>The actual limit on the number of processes is determined by the operating system and system resources such as memory.</p> <p>For example, on a Unix-like system, you can use the <code>ulimit -u</code> command to check the limit on the number of user processes. On newer mac machines, I saw this command returning 2784!</p> <p>Theoretically create as many <code>multiprocessing.Process</code> objects as you want, but the actual number that can run effectively will depend on your system's resources. If you create too many, you may run into issues with system performance or hit system-imposed limits.</p> <p>Note: The <code>ulimit -u</code> command provides the limit on the number of user processes for the entire system, not per CPU core. There isn't a direct way to get a limit per CPU core because the operating system's scheduler manages how processes are distributed across cores, and it doesn't set a limit per core. Instead, it dynamically schedules processes across all available cores based on process priority and current system load.</p>","tags":["python-package","concurrency","parallelism"]},{"location":"Programming/python/concurrency/multiprocessing/#inter-process-communication-ipc","title":"Inter-process communication (IPC)","text":"<p>IPC is facilitated through several mechanisms, including:</p> <ol> <li>Queues: The multiprocessing module provides a <code>Queue</code> class that is a near clone of <code>queue.Queue</code>. It can be used to pass messages between processes. Queues are thread and process safe.</li> <li>Pipes: The Pipe() function returns a pair of connection objects connected by a pipe which by default is duplex (two-way). The two connection objects returned by <code>Pipe()</code> represent the two ends of the pipe. Each connection object has <code>send()</code> and <code>recv()</code> methods.</li> <li>Shared Memory: Data can be stored in a shared memory map using <code>Value</code> or <code>Array</code>. For example, a <code>multiprocessing.Value('d', 0.0)</code> creates a double precision float in shared memory, and <code>multiprocessing.Array('i', range(10))</code> creates an array of integers in shared memory.</li> <li>Server process: A manager object returned by <code>Manager()</code> controls a server process which holds Python objects and allows other processes to manipulate them using proxies.</li> </ol> <pre><code># Example of using a Queue for IPC\nfrom multiprocessing import Process, Queue\n\n# the function puts some data into the queue, and this data is then retrieved in the main process.\ndef f(q):\n    q.put([42, None, 'hello'])\n\nif __name__ == '__main__':\n    q = Queue()\n    p = Process(target=f, args=(q,))\n    p.start()\n    print(q.get())    # prints \"[42, None, 'hello']\"\n    p.join()\n</code></pre>","tags":["python-package","concurrency","parallelism"]},{"location":"Programming/python/concurrency/multiprocessing/#sample-code","title":"Sample code","text":"<pre><code>from multiprocessing import Process\n\ndef print_numbers():\n    for i in range(10):\n        print(i)\n\ndef print_letters():\n    for letter in 'abcdefghij':\n        print(letter)\n\np1 = Process(target=print_numbers)\np2 = Process(target=print_letters)\n\np1.start()\np2.start()\n\np1.join()\np2.join()\n</code></pre>","tags":["python-package","concurrency","parallelism"]},{"location":"Programming/python/concurrency/threading/","title":"threading","text":"<p>The <code>threading</code> module is used to <code>run multiple threads</code> (tasks, function calls) at the same time.</p> <p>When a Python program is running a single thread, it's essentially using <code>only one CPU core</code>, even if multiple cores are available. The threading module allows a program to create multiple threads, which can run independently of each other. These threads can be <code>scheduled to run on different CPU cores</code>, thus <code>potentially</code> improving the performance of the program.</p> <p>However, due to GIL, even though multiple threads might be scheduled on different CPU cores, only one of them can make progress at a time.</p> <p>There won't be much performance improvement in compute-intensive tasks. However I/O-bound tasks will see significant performance improvement. By using multiple threads, a Python program can initiate an I/O operation on one thread, and while it's waiting for that operation to complete, it can switch to another thread and do some computation there. This way, the program can make progress on computation while waiting for I/O, thus improving its overall performance.</p> <p>Python's threading module doesn't allow for true parallel execution of Python bytecodes due to the GIL, it can still provide benefits by allowing a program to make progress on computation while waiting for I/O operations to complete.</p>","tags":["python-package","concurrency","parallelism"]},{"location":"Programming/python/concurrency/threading/#single-core-cpu","title":"Single-core CPU","text":"<p>Multiple threads can be created using the threading module even on a single-core CPU. The operating system's <code>scheduler</code> will handle the switching between threads, giving each thread a small <code>slice of CPU time</code>, giving the <code>illusion</code> of simultaneous execution. This is known as <code>concurrent execution</code> or <code>context switching</code>.</p>","tags":["python-package","concurrency","parallelism"]},{"location":"Programming/python/concurrency/threading/#multi-core-cpu","title":"Multi-core CPU","text":"<p>On a multi-core cpu, multiple threads can be executed simultaneously, one on each core, known as <code>parallel execution</code>. However, due to GIL only one thread can execute Python bytecodes at a time.</p> <p>This means even though multiple threads might be scheduled on different CPU cores, only one of them can make progress at a time. The only way to gain performance on multi-core CPU is to use the <code>multiprocessing</code> module, which creates separate processes and thus bypasses the GIL, and providing better performance.</p>","tags":["python-package","concurrency","parallelism"]},{"location":"Programming/python/concurrency/threading/#inter-thread-communication","title":"Inter-thread communication","text":"<p>If you want to communicate between threads within the same process. The module's mechanisms include:</p> <ol> <li>Shared Variables: Threads within the same process can communicate with each other by modifying shared variables. However, this can lead to race conditions if not managed properly. To avoid race conditions, you can use locks.</li> <li>Queues: The <code>queue.Queue</code> class from the threading module can be used for safe communication between threads. It is thread-safe, which means it uses locks to prevent multiple threads from accessing or modifying the data at the same time.</li> <li>Events: An event is a simple communication mechanism that allows one thread to signal one or more other threads that a certain event has occurred.</li> <li>Condition: A condition variable is a more advanced version of an event object that allows a thread to not only wait for an event but also to atomically release a lock while waiting for the event.</li> </ol> <pre><code>import threading\nimport queue\n\n# The function takes items from the queue and processes them\n# When there are no more items, it breaks the loop and ends.\ndef worker(q):\n    while True:\n        item = q.get()\n        if item is None:\n            break\n        print(f'Working on {item}')\n        q.task_done()\n\n# The main function creates the threads and puts items in the queue.\ndef main():\n    q = queue.Queue()\n    threads = []\n    for i in range(5):\n        t = threading.Thread(target=worker, args=(q,))\n        t.start()\n        threads.append(t)\n\n    for item in range(10):\n        q.put(item)\n\n    # block until all tasks are done\n    q.join()\n\n    # stop workers\n    for i in range(5):\n        q.put(None)\n    for t in threads:\n        t.join()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>","tags":["python-package","concurrency","parallelism"]},{"location":"Programming/python/concurrency/threading/#sample-code","title":"Sample code","text":"<pre><code>import threading\n\ndef print_numbers():\n    for i in range(10):\n        print(i)\n\ndef print_letters():\n    for letter in 'abcdefghij':\n        print(letter)\n\nt1 = threading.Thread(target=print_numbers)\nt2 = threading.Thread(target=print_letters)\n\nt1.start()\nt2.start()\n\nt1.join()\nt2.join()\n</code></pre>","tags":["python-package","concurrency","parallelism"]},{"location":"Programming/python/concurrency/threading/#resources","title":"Resources","text":"<ul> <li>threading module constructs higher-level threading interfaces on top of the lower level <code>_thread</code> module.</li> </ul>","tags":["python-package","concurrency","parallelism"]},{"location":"Programming/python/frameworks/","title":"Python Frameworks","text":"<ul> <li>Flask</li> <li>Fast API</li> <li>Django</li> </ul> <p>Flask and Django are the most popular Python web frameworks, although FastAPI\u2019s popularity is growing faster. All three handle the basic web server tasks, with varying learning curves. FastAPI seems to have a cleaner syntax for specifying routes, and its support of ASGI allows it to run faster than its competitors in many cases.</p>"},{"location":"Programming/python/frameworks/#features","title":"Features","text":""},{"location":"Programming/python/frameworks/#server-sent-events","title":"Server-sent events","text":"<p>Push data to a client as needed. Supported by FastAPI (sse-starlette), Flask (Flask-SSE), and Django (Django EventStream).</p>"},{"location":"Programming/python/frameworks/#queues","title":"Queues","text":"<p>Job queues, publish-subscribe, and other networking patterns are supported by external packages like <code>ZeroMQ</code>, <code>Celery</code>, <code>Redis</code>, and <code>RabbitMQ</code>.</p>"},{"location":"Programming/python/frameworks/#websockets","title":"WebSockets","text":"<p>Supported by FastAPI (directly), Django (Django Channels), and Flask (third-party packages).</p>"},{"location":"Programming/python/frameworks/#database","title":"Database","text":"<p>Flask and FastAPI do not include any database handling in their base packages, but database handling is a key feature of Django.</p> <p>A common choice for Flask and FastAPI developers is SQLAlchemy. FastAPI utilizes SQLAlchemy and Pydantic for the SQLModel package.</p>"},{"location":"Programming/python/frameworks/#other-web-frameworks","title":"Other Web Frameworks","text":"<ul> <li>Bottle is a very minimal (single Python file) package, good for a quick proof of concept.</li> <li>Litestar - Similar to FastAPI \u2014 it\u2019s based on ASGI/Starlette and Pydantic\u2014but has its own opinions</li> <li>AIOHTTP - An ASGI client and server, with useful demo code</li> <li>Socketify.py is a new entrant that claims very high performance.</li> </ul>"},{"location":"Programming/python/frameworks/django/","title":"Django","text":"<p>Django is bigger and more complex than Flask or FastAPI, targeting <code>perfectionists with deadlines,</code> according to its website. Its built-in object-relational mapper (ORM) is useful for sites with major database backends. It\u2019s more of a monolith than a toolkit. Whether the extra complexity and learning curve are justified depends on your application.</p> <p>Although Django was a traditional WSGI application, version 3.0 added support for ASGI.</p> <p>Unlike Flask and FastAPI, Django likes to define routes (associating URLs with web functions, which it calls view functions) in a single URLConf table, rather than using decorators. This makes it easier to see all your routes in one place, but makes it harder to see what URL is associated with a function when you\u2019re looking at just the function.</p>"},{"location":"Programming/python/frameworks/django/#installation","title":"Installation","text":"<pre><code>$ pip install Django==2.1.3\n</code></pre>"},{"location":"Programming/python/frameworks/django/#common-commands","title":"Common Commands","text":"<pre><code>$ python -m django --version\n</code></pre>"},{"location":"Programming/python/frameworks/fast-api/","title":"FastAPI","text":"<p>Like any web framework, FastAPI helps you to build web applications.</p> <p>Some advantages claimed by the website include: * <code>Performance</code>: As fast as Node and Golang in some cases, unusual for Python frameworks. * <code>Faster development</code>: No sharp edges or oddities. * <code>Better code quality</code>: Type hinting and models help reduce bugs. * <code>Autogenerated documentation and test pages</code>: Much easier than hand editing OpenAPI descriptions.</p> <p>FastAPI uses: * Python type hints * Starlette for the web machinery, including async support * Pydantic for data definitions and validation * Special integration to leverage and extend the others</p>"},{"location":"Programming/python/frameworks/fast-api/#web-server","title":"Web Server","text":"<p>FastAPI itself does not include a web server, but recommends <code>Uvicorn</code>.</p>"},{"location":"Programming/python/frameworks/fast-api/#http-requests","title":"HTTP Requests","text":"<p>The way that FastAPI provides data from various parts of the HTTP requests is one of its best features, and an improvement on how most Python web frameworks do it. All the arguments that you need can be declared and provided directly inside the path function, using the definitions above (<code>Path</code>, <code>Query</code>, etc.), and by functions that you write. This uses a technique called <code>dependency injection</code>.</p> <p>FastAPI converts HTTP header keys <code>to lowercase</code>, and dash (<code>-</code>) to underscore (<code>_</code>).</p> <p>By default, FastAPI converts whatever you return from your endpoint function <code>to JSON</code>.</p> <p>By default, FastAPI returns a <code>200</code> status code; exceptions raise <code>40x</code> codes.</p>"},{"location":"Programming/python/frameworks/fast-api/#response-model","title":"Response Model","text":"<p>It\u2019s possible to have different classes with many of the same fields, except one is specialized for user input, one for output, and one for internal use:</p> <ul> <li>Remove some sensitive information from output (like deidentifying personal medical data, if you\u2019ve encountered HIPPA requirements).</li> <li>Add fields to user input (like a creation date and time).</li> </ul> <p>You can return other data types than the default JSON from a FastAPI path function in different ways. One method is to use the <code>response_model</code> argument in the path decorator to goose FastAPI to return something else. FastAPI will drop any fields that were in the object that you returned but are not in the object specified by response_model.</p>"},{"location":"Programming/python/frameworks/flask/","title":"Flask","text":"<p>Flask calls itself a microframework. It provides the basics, and you download third-party packages to supplement it as needed. It\u2019s smaller than Django, and faster to learn when you\u2019re getting started.</p> <p>Flask is synchronous, based on WSGI rather than ASGI. A new project called quart is replicating Flask and adding ASGI support.</p>"},{"location":"Programming/python/packages/black/","title":"black","text":"<p>Black - Source formatting</p>"},{"location":"Programming/python/packages/black/#usage","title":"Usage","text":"<pre><code># To format files in current directory\n$ black .\n\n# Alternate way to run black as a package, if running as script doesn't work\n$ python3 -m black .\n\n## Preferred way would be to create a git hook and have this check run automatically on each commit.\n</code></pre>"},{"location":"Programming/python/packages/httpie/","title":"httpie","text":"<p>A command-line text web client, similar to <code>curl</code>. Although Curl is the best known text web client, <code>Httpie</code> is easier to use. Also, it <code>defaults to JSON encoding and decoding</code>, which is a better match for FastAPI.</p> <pre><code>$ pip install httpie\n</code></pre>"},{"location":"Programming/python/packages/httpie/#usage","title":"Usage","text":""},{"location":"Programming/python/packages/httpie/#get","title":"GET","text":"<pre><code># Show response with headers\n$ http localhost:8000/hello\n\n# Show response without headers\n$ http -b localhost:8000/hello\n\n# Show response with request headers and response headers\n$ http -v localhost:8000/hello\n</code></pre>"},{"location":"Programming/python/packages/httpie/#post","title":"POST","text":"<pre><code># Send request with body\n$ http localhost:8000/hello name=Champ\n</code></pre>"},{"location":"Programming/python/packages/httpx/","title":"httpx","text":"<p>A synchronous/asynchronous web client package. Use in Python in <code>interactive mode</code>, and type after the <code>&gt;&gt;&gt;</code> prompt.</p> <pre><code>$ pip install httpx\n</code></pre>"},{"location":"Programming/python/packages/httpx/#usage","title":"Usage","text":""},{"location":"Programming/python/packages/httpx/#get","title":"GET","text":"<pre><code>&gt;&gt;&gt; import httpx\n&gt;&gt;&gt; r = httpx.get(\"http://localhost:8000/hello\")\n&gt;&gt;&gt; r.json()\n'Hello? World?'\n</code></pre>"},{"location":"Programming/python/packages/httpx/#post","title":"POST","text":"<pre><code>&gt;&gt;&gt; import httpx\n&gt;&gt;&gt; r = httpx.post(\"http://localhost:8000/hello\", json={\"name\": \"Champ\"})\n&gt;&gt;&gt; r.json()\n'Hello Champ!'\n</code></pre>"},{"location":"Programming/python/packages/mypy/","title":"mypy","text":"<p>mypy is a standard type checker, to catch incorrect types assigned to variables.</p>","tags":["python-package","code-quality","type-check"]},{"location":"Programming/python/packages/pre-commit/","title":"pre-commit","text":"<p>pre-commit run hooks on every commit automatically.</p> <pre><code># Install manually or add the package in the requirements.txt\n$ pip install pre-commit\n\n# Alternatively, install using homebrew\n$ brew install pre-commit\n</code></pre>"},{"location":"Programming/python/packages/pre-commit/#config-yaml","title":"Config yaml","text":"<p>Sample pre-commit config yaml saved as <code>.pre-commit-config.yaml</code>.</p> <pre><code>repos:\n-   repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v2.3.0\n    hooks:\n    -   id: check-yaml\n    -   id: end-of-file-fixer\n    -   id: trailing-whitespace\n-   repo: https://github.com/psf/black\n    rev: 22.10.0\n    hooks:\n    -   id: black\n</code></pre>"},{"location":"Programming/python/packages/pre-commit/#usage","title":"Usage","text":"<pre><code># Check current version\n$ pre-commit --version\n\n# Install the git hook script (once you have the configuration file)\n$ pre-commit install\n\n# Note, this is only a one-time step. Once the git hooks are created, there is no need to run install again.\n\n# Manually run against all files\n$ pre-commit run --all-files\n</code></pre>"},{"location":"Programming/python/packages/pydantic/","title":"pydantic","text":"<p>Pydantic is the most widely used data validation library for Python. Fast and extensible, Pydantic plays nicely with your linters/IDE/brain. Define how data should be in pure, canonical Python 3.7+; validate it with Pydantic.</p>"},{"location":"Programming/python/packages/pydantic/#why-pydantic","title":"Why pydantic?","text":"<ul> <li>Powered by type hints</li> <li>Speed</li> <li>JSON Schema</li> <li>Strict and Lax mode</li> <li>Dataclasses, TypedDicts and more</li> <li>Customisation</li> <li>Ecosystem</li> <li>Battle tested</li> </ul>"},{"location":"Programming/python/packages/pydantic/#usage","title":"Usage","text":"<pre><code>from datetime import datetime\nfrom typing import Tuple\n\nfrom pydantic import BaseModel\n\n\nclass Delivery(BaseModel):\n    timestamp: datetime\n    dimensions: Tuple[int, int]\n\n\nm = Delivery(timestamp='2020-01-02T03:04:05Z', dimensions=['10', '20'])\nprint(repr(m.timestamp))\n#&gt; datetime.datetime(2020, 1, 2, 3, 4, 5, tzinfo=TzInfo(UTC))\nprint(m.dimensions)\n#&gt; (10, 20)\n</code></pre>"},{"location":"Programming/python/packages/pydantic/#complex-data-structure-approaches","title":"Complex data structure approaches","text":"<p>Comparison of three approaches on YouTube</p> <ul> <li>dataclasses - Part of standard Python.</li> <li>attrs - Third party, but a superset of dataclasses.</li> <li>Pydantic - Also third party, but integrated into FastAPI, so an easy choice if you\u2019re already using FastAPI</li> </ul>"},{"location":"Programming/python/packages/pydantic/#reasoning","title":"Reasoning","text":"<p>One takeaway is that Pydantic stands out for <code>validation</code>, and its integration with FastAPI catches many potential data errors. Another is that Pydantic relies on <code>inheritance</code> (from the BaseModel class), and the other two use Python decorators to define their objects. This is more a matter of style.</p> <p>In another comparison, Pydantic outperformed older validation packages like <code>marshmallow</code> and the intriguingly named <code>Voluptuous</code>. Another big plus for Pydantic is that it uses standard Python type hint syntax; older libraries pre-dated type hints and rolled their own.</p> <p>Pydantic provides ways to specify any combination of these checks:</p> <ul> <li>Required versus optional</li> <li>Default value if unspecified but required</li> <li>The data type or types expected</li> <li>Value range restrictions</li> <li>Other function-based checks if needed</li> <li>Serialization and deserialization</li> </ul>"},{"location":"Programming/python/packages/pytest/","title":"pytest","text":"<p>pytest - unit tests</p>"},{"location":"Programming/python/packages/requests/","title":"requests","text":"<p>A synchronous web client package. Unlike <code>httpx</code>, it does not support asynchronous calls. Use in Python in <code>interactive mode</code>, and type after the <code>&gt;&gt;&gt;</code> prompt.</p> <pre><code>$ pip install requests\n</code></pre>"},{"location":"Programming/python/packages/requests/#usage","title":"Usage","text":""},{"location":"Programming/python/packages/requests/#get","title":"GET","text":"<pre><code>&gt;&gt;&gt; import requests\n&gt;&gt;&gt; r = requests.get(\"http://localhost:8000/hello\")\n&gt;&gt;&gt; r.json()\n'Hello World!'\n</code></pre>"},{"location":"Programming/python/packages/requests/#post","title":"POST","text":"<pre><code>&gt;&gt;&gt; import requests\n&gt;&gt;&gt; r = requests.post(\"http://localhost:8000/hello\", json={\"name\": \"Champ\"})\n&gt;&gt;&gt; r.json()\n'Hello Champ!'\n</code></pre>"},{"location":"Programming/python/packages/starlette/","title":"starlette","text":"<p>Starlette is a lightweight ASGI framework/toolkit, which is ideal for building async web services in Python.</p> <p>Much of FastAPI\u2019s web code is based on the Starlette package. It can be used as a web framework in its own right, or as a library for other frameworks, such as FastAPI. Like any other web framework, Starlette handles all the usual HTTP request parsing and response generation. It\u2019s similar to Werkzeug, the package that underlies Flask.</p> <p>Its most important feature is its support of the modern Python asynchronous web standard: ASGI. Until now, most Python web frameworks (like Flask and Django) have been based on the traditional synchronous WSGI standard. Because web applications so frequently connect to much slower code (e.g., database, file, and network access), ASGI avoids the <code>blocking</code> and <code>busy waiting</code> of WSGI-based applications.</p>","tags":["python-package","concurrency","parallelism","async","await"]},{"location":"Programming/python/packages/starlette/#types-of-concurrency","title":"Types of Concurrency","text":"<p>In <code>parallel computing</code>, a task is spread at the same time across multiple dedicated CPUs. This is common in \u201cnumber crunching\u201d applications like graphics and machine learning.</p> <p>In <code>concurrent computing</code>, each CPU switches among multiple tasks. Some tasks take longer than others, and we want to reduce the total time needed. reading a file or accessing a remote network service is literally thousands to millions of times slower than running calculations in the CPU.</p> <p>With CPU-intensive Python applications, the usual solution is to use multiple processes, and let the OS manage them. Python has a multiprocessing module for this.</p> <p>You can also run threads of control within a single process. Python\u2019s threading package manages these.</p> <p>Threads are often recommended when your program is I/O-bound, and multiple processes when you\u2019re CPU-bound.</p> <p>Traditionally, Python kept the process-based and thread-based libraries separate. Developers had to learn the arcane details of either to use them. A more recent package called concurrent.futures is a higher-level interface that makes them easier to use.</p> <p>FastAPI actually also manages threads for normal synchronous functions (<code>def</code>, not <code>async def</code>) via threadpools. </p> <p>The use of async and await on their own does not make code run faster. In fact, it might be a little slower, from async setup overhead. The main use of async is to avoid long waits for I/O.</p> <p><code>Green threads</code> such as greenlet, gevent and eventlet provide another mysterious mechanism. These are cooperative (not preemptive). They\u2019re similar to OS threads, but run in user space (i.e, your program) rather than in the OS kernel. They work by monkey-patching some standard Python functions to make concurrent code look like normal sequential code: they give up control when they would block waiting for I/O.</p> <p>OS threads are <code>lighter</code> (use less memory) than OS processes, and green threads are lighter than OS threads. In some benchmarks, all the async methods were generally faster than their sync counterparts.</p>","tags":["python-package","concurrency","parallelism","async","await"]},{"location":"Programming/python/packages/uvicorn/","title":"uvicorn","text":"<p>An asynchronous web server. Comes with a file-change watcher.</p>"},{"location":"Programming/python/packages/uvicorn/#usage","title":"Usage","text":"<pre><code># To start uvicorn via command line, if not starting via main file\n$ uvicorn hello:app --reload\n\n# To start uvicorn via main file, add the following code\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(\"server:app\", reload=True)\n</code></pre>"},{"location":"Programming/spring-boot/","title":"Spring Boot","text":"<ul> <li>Terms</li> </ul>"},{"location":"Programming/spring-boot/#key-terms-to-understand","title":"Key terms to understand","text":"<ul> <li><code>Bean</code>, any Spring-managed resource is referred to as a bean. Beans can utilize any interface implementation to satisfy their dependency.</li> <li><code>Interfaces</code> - The use of interfaces also allows Spring to utilize JDK dynamic proxies (the <code>Proxy pattern</code>) to provide powerful concepts such as aspect-oriented programming (<code>AOP</code>) for crosscutting concerns.</li> <li><code>Dependency injection</code> (DI), a concept that describes how dependent objects are connected at runtime by an external party. The DI implementation is based on JavaBeans and Interfaces.</li> <li><code>Inversion of Control</code> (IoC), is a design principle in which generic reusable components are used to control the execution of problem-specific code, as in retrieving dependencies.</li> <li><code>Aspect-oriented programming</code> (AOP), provides the ability to implement crosscutting logic \u2014 that is, logic that applies to many parts of your application \u2014 in a single place and to have that logic applied across your application automatically.</li> </ul>"},{"location":"Programming/spring-boot/#more-reading-resources","title":"More reading resources","text":"<ul> <li>GraalVM Native Image Support</li> <li>It\u2019s time to move your applications to Java 17. Here\u2019s why</li> <li>GraphQL Java</li> <li>Vault Project</li> </ul>"},{"location":"Programming/spring-boot/#resources","title":"Resources","text":"<ul> <li>Spring JavaDoc | Official</li> <li>Spring Framework Reference | Official</li> </ul>"},{"location":"Programming/spring-boot/filters-interceptors/","title":"Filters &amp; Interceptors","text":""},{"location":"Programming/spring-boot/filters-interceptors/#filter","title":"Filter","text":"<p>Filter components are executed by the servlet container for each incoming HTTP request and for each HTTP response. Requests always first pass through Filter instances, before reaching a Servlet.</p> <p>If the application has multiple custom filters, the order of execution can be defined with <code>@Order</code> annotation.</p>"},{"location":"Programming/spring-boot/filters-interceptors/#interceptor","title":"Interceptor","text":"<p>Spring Interceptors are similar to Servlet Filters. An interceptor just allows custom pre-processing with the option of prohibiting the execution of the handler itself, and custom post-processing, having access to Spring Context.</p>"},{"location":"Programming/spring-boot/filters-interceptors/#handlerinterceptor","title":"HandlerInterceptor","text":"<p>HandlerInterceptor instances are executed as part of the request handling inside the DispatcherServlet (which implements <code>javax.servlet.Servlet</code>).</p>"},{"location":"Programming/spring-boot/filters-interceptors/#handlerinterceptor-methods","title":"HandlerInterceptor Methods","text":"<ul> <li>prehandle() \u2013 called before the execution of the actual handler</li> <li>postHandle() \u2013 called after the handler is executed</li> <li>afterCompletion() \u2013 called after the complete request is finished and the view is generated</li> </ul> <p>These methods return a boolean value. It tells Spring to further process the request (true) or not (false). Returning <code>true</code> to send the request further to the handler method. Returning <code>false</code> tells Spring to stop the execution.</p>"},{"location":"Programming/spring-boot/filters-interceptors/#registering-handlerinterceptor","title":"Registering HandlerInterceptor","text":"<p>Every interceptor must be registered by overriding <code>addInterceptors()</code> method of <code>WebMvcConfigurer</code> interface.</p>"},{"location":"Programming/spring-boot/filters-interceptors/#filters-vs-handlerinterceptors","title":"Filters vs HandlerInterceptors","text":"<ul> <li>Filter is related to the Servlet API and HandlerIntercepter is a Spring specific concept.</li> <li>Interceptors will only execute after Filters.</li> <li>Fine-grained pre-processing tasks are suitable for HandlerInterceptors (authorization checks, etc.)</li> <li>Content handling related or generic flows are well-suited for Filters (such as multipart forms, zip compression, image handling, logging requests, authentication etc.)</li> <li>Interceptor\u2019s postHandle method will allow you to add more model objects to the view but you can not change the HttpServletResponse since it's already committed.</li> <li>Filter\u2019s doFilter method is much more versatile than Interceptor\u2019s postHandle. You can change the request or response and pass it to the chain or even block the request processing.</li> <li>A HandlerInterceptor gives more fine-grained control than a filter because you have access to the actual target \u201chandler\u201d. You can even check if the handler method has a specific annotation.</li> </ul>"},{"location":"Programming/spring-boot/filters-interceptors/#blog-references","title":"Blog References","text":"<ul> <li>HandlerIntercepter | ozenero blog</li> </ul>"},{"location":"Programming/spring-boot/servlet/","title":"Servlet","text":"<p>A Servlet is used to extend the capabilities of servers that host applications, accessed via request-response programming model.</p> <p>Java Servlet technology defines HTTP-specific servlet classes. The <code>javax.servlet</code> and <code>javax.servlet.http</code> packages provide interfaces and classes for writing servlets. Servlet in \"javax.servlet\" package declares three essential methods for the life cycle of a servlet \u2014 init(), service(), and destroy().</p>"},{"location":"Programming/spring-boot/servlet/#servlet-container","title":"Servlet Container","text":"<p>Servlets run in a servlet container which handles the networking side (e.g. parsing an HTTP request, connection handling etc). Example, Tomcat.</p>"},{"location":"Programming/spring-boot/servlet/#servletinputstream","title":"ServletInputStream","text":"<p>ServletInputStream class is a component of Java package <code>javax.servlet</code>, It is an abstract class that provides an input stream for reading binary data from a client request, including an efficient readLine method for reading data one line at a time.</p>"},{"location":"Programming/spring-boot/spring-boot-starter-modules/","title":"Spring boot starter modules","text":"Module Description spring-boot-starter This is the simplest Spring Boot starter that adds the spring-core module as a dependency for your project. It can be used to create a very simple Spring application. It is used mostly for learning purposes and for creating base projects, that encapsulate common functionality shared among other modules in a project. spring-boot-starter-aop Adds the spring-aop as a dependency for your project. spring-boot-starter-data-* This type of starter adds various Spring dependencies for working with data in your project. The * replaces the technology from which data is coming. For example, spring-boot-starter-data-jdbc adds classes for creating Spring Repository beans for handling data from databases supporting a JDBC driver: MySQL, PostgreSQL, Oracle, etc. spring-boot-starter-web Configures minimal dependencies for creating a web application. spring-boot-starter-security Configures minimal dependencies for securing a Spring web application. spring-boot-starter-webflux Configures minimal dependencies for creating a reactive web application. spring-boot-starter-actuator Configures Spring Boot Actuator, which enables a set of endpoints for monitoring a Spring web application. spring-boot-starter-test Configures the following set of libraries: Spring Test, JUnit, Hamcrest, and Mockito."},{"location":"Programming/spring-boot/spring-modules/","title":"Spring Modules","text":"Module Description aop This module contains all the classes you need to use Spring\u2019s AOP features within your application. You also need to include this JAR in your application if you plan to use other features in Spring that use AOP, such as declarative transaction management. Moreover, classes that support integration with AspectJ are packed in this module. aspects This module contains all the classes for advanced integration with the AspectJ AOP library. For example, if you are using Java classes for your Spring configuration and need AspectJ-style annotation-driven transaction management, you need this module. beans This module contains all the classes for supporting Spring\u2019s manipulation of Spring beans. Most of the classes here support Spring\u2019s bean factory implementation. For example, the classes required for processing the Spring XML configuration file and Java annotations are packed into this module. context This module contains classes that provide many extensions to Spring Core. You will find that all classes need to use Spring\u2019s ApplicationContext feature, along with classes for Enterprise JavaBeans (EJB), Java Naming and Directory Interface (JNDI), and Java Management Extensions (JMX) integration. Also contained in this module are the Spring remoting classes, classes for integration with dynamic scripting languages (for example, JRuby, Groovy, and BeanShell), JSR-303 (\u201cBean Validation\u201d), scheduling and task execution, and so on. context-indexer This module contains an indexer implementation that provides access to the candidates that are defined in META-INF/spring.components. The core class CandidateComponentsIndex is not meant to be used externally. context-support This module contains further extensions to the spring-context module. On the user-interface side, there are classes for mail support and integration with templating engines such as Velocity, FreeMarker, and JasperReports. Also, integration with various task execution and scheduling libraries, including CommonJ and Quartz, are packaged here. core This is the main module that you will need for every Spring application. In this JAR file, you will find all the classes that are shared among all other Spring modules (for example, classes for accessing configuration files). Also, in this JAR, you will find selections of extremely useful utility classes that are used throughout the Spring codebase and that you can use in your own application. expression This module contains all support classes for Spring Expression Language (SpEL). instrument This module includes Spring\u2019s instrumentation agent for JVM bootstrapping. This JAR file is required for using load-time weaving with AspectJ in a Spring application. jcl This module is only present for binary compatibility with existing Apache Commons Logging usage, such as in Apache Commons Configuration. jdbc This module includes all classes for JDBC support. You will need this module for all applications that require database access. Classes for supporting data sources, JDBC data types, JDBC templates, native JDBC connections, and so on, are packed in this module. jms This module includes all classes for Java Message Service (JMS) support. messaging This module contains key abstractions taken from the Spring Integration project to serve as a foundation for message-based applications and adds support for STOMP messages. orm This module extends Spring\u2019s standard JDBC feature set with support for popular ORM tools including Hibernate, JDO, and JPA. Many of the classes in this JAR depend on classes contained in the spring-jdbc JAR file, so you definitely need to include that in your application as well. oxm This module provides support for Object/XML Mapping (OXM). Classes for the abstraction of XML marshalling and unmarshalling and support for popular tools such as Castor, JAXB, XMLBeans, and XStream are packed into this module. r2dbc This module makes R2DBC easier to use and reduces the likelihood of common errors. It provides simple error handling and a family of unchecked concise exceptions agnostic of the underlying RDBM (Reactive Database Manager). test Spring provides a set of mock classes to aid in testing your applications, and many of these mock classes are used within the Spring test suite, so they are well tested and make testing your applications much simpler. Certainly we have found great use for the mock HttpServletRequest and HttpServletResponse classes in unit tests for our web applications. On the other hand, Spring provides a tight integration with the JUnit unit-testing framework, and many classes that support the development of JUnit test cases are provided in this module; for example, SpringExtension integrates the Spring TestContext Framework into JUnit 5\u2019s Jupiter programming model. tx This module provides all classes for supporting Spring\u2019s transaction infrastructure. You will find classes from the transaction abstraction layer to support the Java Transaction API (JTA) and integration with application servers from major vendors. web This module contains the core classes for using Spring in your web applications, including classes for loading an ApplicationContext feature automatically, file upload support classes, and a bunch of useful classes for performing repetitive tasks such as parsing integer values from the query string. webflux This module contains core interfaces and classes for the Spring Web Reactive model. webmvc This module contains all the classes for Spring\u2019s own MVC framework. If you are using a separate MVC framework for your application, you won\u2019t need any of the classes from this JAR file. websocket This module provides support for JSR-356 (\u201cJava API for WebSocket\u201d)."},{"location":"Programming/spring-boot/spring-projects/","title":"Spring Projects","text":"<p><code>Spring Boot</code> consists of a set of libraries (called starters) that provide default application templates that can be customized easily to quickly develop multiple types of stand-alone, production-grade Spring-based applications.</p> <p><code>Spring Framework</code> consists of a set of libraries that provide support for dependency injection, transaction management, data access, messaging, and other core functionalities for any type of application. The Spring Framework now includes the Spring WebFlux framework that represents the Spring reactive-stack designed to build fully non-blocking, with back-pressure support reactive applications on servers such as Netty, Undertow, and Servlet 3.1+ containers.</p> <p><code>Spring Data</code> consists of a set of libraries that provide a consistent programming model for accessing various databases both relational (e.g., MySQL and Oracle) and nonrelational (e.g., MongoDB and CouchBase). Support for in-memory databases (e.g., H2 and MongoDB) is included, which is pretty useful for testing applications without the drag of a concrete database. Also, Spring Data R2DBC makes it easy to access reactive databases.</p> <p><code>Spring Security</code> provides the ability to secure applications easily, with a simple model for authentication and authorization.</p> <p><code>Spring Cloud</code> provides a set of common tools for writing microservices applications destined to run in distributed systems.</p> <p><code>Spring Cloud Data Flow</code> provides a set of common tools for streaming and batch processing of data between microservices running in Cloud Foundry and Kubernetes.</p> <p><code>Spring Integration</code> provides support for building Spring applications that make use of lightweight messaging and integrate with external systems via declarative adapters.</p> <p><code>Spring Session</code> provides an API and implementations for managing a user\u2019s session information.</p> <p><code>Spring HATEOAS</code> provides some APIs to ease creating REST representations that follow the HATEOAS principle when working with Spring and especially Spring MVC. Some developers/architects consider that the hypermedia5 pollutes the REST data and a better solution is to use Swagger6 to expose (and document) an application\u2019s API or Spring REST Docs.</p> <p><code>Spring for GraphQL</code> provides the tools to build Spring applications on GraphQL Java. GraphQL7 is a query language to retrieve data from a server.</p> <p><code>Spring REST Docs</code> provides the tools to expose and document a Spring application\u2019s API.</p> <p><code>Spring Batch</code> is a framework that provides the tools to build lightweight and robust Spring applications that handle immense volumes of data.</p> <p><code>Spring AMQP</code> provides the tools to build AMQP-based messaging solutions using Spring.</p> <p><code>Spring CredHub</code> is part of the Spring Cloud project family and provides client-side support for storing, retrieving, and deleting credentials from a CredHub server running in a CloudFoundry platform.</p> <p><code>Spring Flo</code> is a JavaScript library that offers a basic embeddable HTML5 visual builder for pipelines and simple graphs. Spring Cloud Data Flow is an extension of this project.</p> <p><code>Spring for Apache Kafka</code> provides tools for building Kafka-based messaging solutions using Spring.</p> <p><code>Spring LDAP</code> is a library to simplify LDAP programming in Java, built on the same principles as Spring JDBC.</p> <p><code>Spring Shell</code> provides the tools to build a full-featured shell (aka command line) application by depending on the Spring Shell jars and adding their own commands (which come as methods on Spring beans).</p> <p><code>Spring Statemachine</code> is a framework for application developers to use state machine concepts with Spring applications.</p> <p><code>Spring Vault</code> provides familiar Spring abstractions and client-side support for accessing, storing, and revoking secrets. It offers both low-level and high-level abstractions for interacting with HashiCorp\u2019s Vault8, freeing the user from infrastructural concerns.</p> <p><code>Spring Web Flow</code> extends Spring MVC to provide the tools for implementing the \"flows\" of a web application. A flow encapsulates a sequence of steps that guide a user through the execution of some business task. It spans multiple HTTP requests, has state, deals with transactional data, is reusable, and may be dynamic and long-running in nature.</p> <p><code>Spring Web Services</code> (Spring-WS) is a product of the Spring community focused on creating document-driven web services. Spring Web Services aims to facilitate contract-first SOAP service development, allowing for the creation of flexible web services using one of the many ways to manipulate XML payloads.</p> <p><code>Spring Native</code> (currently still considered experimental, but quicky being adopted in the industry) provides support for compiling Spring applications to native executables using the GraalVM native-image compiler.</p> <p><code>Spring Initializr</code> (not actually a project, but good to know), available at https://start.spring.io, provides a quick start for creating custom Spring Boot projects completely configurable according to the developer\u2019s necessities: programming language, build tool, Spring Boot version, and project requirements (database access, web access, event messaging, security, etc.).</p>"},{"location":"Programming/spring-boot/spring-projects/#resources","title":"Resources","text":"<ul> <li>Spring Projects (GitHub + Experimental)</li> <li>Spring Boot + Projects official documentation</li> <li>Book | Pro Spring 6 with Kotlin</li> <li>Jackson library</li> </ul>"},{"location":"Programming/spring-boot/terms/","title":"Terms","text":""},{"location":"fun_stuff/anonymous-feedback/","title":"Anonymous Feedback","text":"<p>One way to ask for anonymous feedback from your peers is to stick a link in your Slack profile.</p> <p>Create an account on admonymous to get a personalized link.</p>"},{"location":"fun_stuff/bypassing_paywall/","title":"Bypassing Paywall","text":"","tags":["bypass","paywall"]},{"location":"fun_stuff/bypassing_paywall/#private-tor","title":"Private Tor","text":"<p>Open an incognito window (with Tor) in Brave browser, and hit the link.</p> <p><code>Tor</code> hides your IP address from the sites you visit, by routing your browsing through several Tor servers before it reaches your destination. Tor can slow down browsing and some sites might not work at all.</p>","tags":["bypass","paywall"]},{"location":"fun_stuff/bypassing_paywall/#archiveph","title":"archive.ph","text":"<p>archive.ph can be used to archive contents of a webpage, or retrieve saved snapshots from the archive</p>","tags":["bypass","paywall"]},{"location":"fun_stuff/cdn_images/","title":"CDN Images","text":"<p>Downloading a lot of images or even large images on a website consumes a lot of bandwidth. Adding network bandwidth overhead to it makes it even worse. Image cache servers come to resue, along with other helpful features like resizing images, etc.</p>","tags":["CDN","assets","images","cache"]},{"location":"fun_stuff/cdn_images/#wsrvnl","title":"wsrv.nl","text":"<p>wsrv.nl is an image cache &amp; resize service. Our servers resize your image, cache it worldwide, and display it.</p>","tags":["CDN","assets","images","cache"]},{"location":"fun_stuff/free_assets/","title":"Free Assets","text":"<p>Sometimes it is not easy to find free assets, even though some great resources exist.</p>","tags":["free","assets","icons","images","videos","music"]},{"location":"fun_stuff/free_assets/#icons","title":"Icons","text":"<ul> <li>SVG Repo</li> <li>icons8</li> </ul>","tags":["free","assets","icons","images","videos","music"]},{"location":"fun_stuff/free_assets/#stock-images","title":"Stock Images","text":"<ul> <li>Pixabay - for Stock images, Illustrations, Vectors, Videos, Music, Sound Effects, GIFs, and Users</li> <li>Pexels - for Stock images, Videos, and Users</li> <li>Unsplash - for Stock images</li> <li>FreeImages - for Stock, Vector, Clipart, Icons, PSD, and Illustrations</li> <li>Free-Images - for Stock, B&amp;W, Vector, Fine Art, and Illustrations</li> </ul>","tags":["free","assets","icons","images","videos","music"]},{"location":"fun_stuff/terminal_gif_maker/","title":"Terminal GIF maker","text":""},{"location":"fun_stuff/terminal_gif_maker/#resources","title":"Resources","text":"<ul> <li>Terminal Gif Maker</li> <li>terminalizer</li> <li>How to make an animated GIF of your terminal commands</li> </ul>"},{"location":"fun_stuff/weather/","title":"Weather","text":"<ul> <li>https://wttr.in/</li> <li>https://wttr.in/New+York</li> <li>https://wttr.in/Paris</li> </ul>"},{"location":"why-nots/android-app-ideas/","title":"Android App ideas","text":"<p>Certainly! Here are some Android app ideas that you might find interesting:</p> <ol> <li>Fitness Challenge App:</li> <li>Create an app that allows users to set fitness goals, track their progress, and engage in challenges with friends.</li> <li> <p>Include features like workout tracking, achievements, and social sharing.</p> </li> <li> <p>Language Learning with Augmented Reality:</p> </li> <li> <p>Develop an app that uses augmented reality to teach users new languages. Users can point their phone's camera at objects, and the app will display the name of the object in the language they are learning.</p> </li> <li> <p>Travel Itinerary Planner:</p> </li> <li>Build an app that helps users plan their trips by suggesting itineraries, local attractions, and restaurants based on their interests and preferences.</li> <li> <p>Include features for budget tracking and real-time travel updates.</p> </li> <li> <p>Mindfulness and Relaxation App:</p> </li> <li>Create an app that offers guided meditation sessions, relaxation exercises, and stress-relief activities.</li> <li> <p>Include features like calming sounds, daily mindfulness reminders, and progress tracking.</p> </li> <li> <p>Expense Tracker with AI Insights:</p> </li> <li> <p>Develop an expense tracking app that uses artificial intelligence to provide insights into spending habits, budget recommendations, and personalized financial tips.</p> </li> <li> <p>Plant Care Reminder:</p> </li> <li>Build an app for plant enthusiasts that reminds users when to water their plants, fertilize them, and provides tips for plant care.</li> <li> <p>Include a plant identification feature using image recognition.</p> </li> <li> <p>AR-based Educational Flashcards:</p> </li> <li> <p>Create an app for students that uses augmented reality to turn traditional flashcards into interactive 3D models, aiding in visual learning.</p> </li> <li> <p>Task Management with Gamification:</p> </li> <li> <p>Develop a task management app that incorporates gamification elements to motivate users to complete their tasks and achieve goals.</p> </li> <li> <p>Local Community Marketplace:</p> </li> <li> <p>Build an app that connects local buyers and sellers for goods and services. Include features like secure transactions, ratings, and a community forum.</p> </li> <li> <p>Home Automation Control:</p> <ul> <li>Create an app that allows users to control and monitor their smart home devices, including lights, thermostats, and security systems.</li> </ul> </li> <li> <p>Recipe Sharing and Meal Planning:</p> <ul> <li>Develop an app that allows users to share and discover recipes, create meal plans, and generate shopping lists based on chosen recipes.</li> </ul> </li> <li> <p>Language Translation for Photos:</p> <ul> <li>Build an app that translates text in images or photos using optical character recognition (OCR) and language translation services.</li> </ul> </li> <li> <p>Virtual Study Groups:</p> <ul> <li>Create a platform that facilitates virtual study groups for students, providing features like video conferencing, document sharing, and collaborative note-taking.</li> </ul> </li> <li> <p>Digital Art Gallery:</p> <ul> <li>Build an app that showcases digital art from emerging artists, allowing users to explore and purchase unique pieces.</li> </ul> </li> <li> <p>Community-driven Events Calendar:</p> <ul> <li>Develop an app that aggregates and displays local community events, allowing users to discover and participate in activities happening around them.</li> </ul> </li> </ol> <p>When developing an app, consider the target audience, user experience, and how your app can solve a problem or fulfill a need. Additionally, stay updated on the latest Android features and design guidelines for a more polished and user-friendly application.</p>"},{"location":"why-nots/benchmarking/","title":"HTTP Server Benchmarking","text":"<p>Benchmarking (a.k.a. load testing) shows you how many requests per second your application server is capable of handling.</p> <p>There are several tools available for benchmarking your Hypertext Transfer Protocol (HTTP) server.</p>"},{"location":"why-nots/benchmarking/#references","title":"References","text":"<ul> <li>ApacheBench</li> <li>ab</li> <li>Datadog How to use ApacheBench</li> <li>Apache JMeter</li> <li>locust</li> <li>Gatling.io</li> <li>hey</li> <li>wrk</li> <li>wrk2</li> <li>wrk and wrk2 - getting started</li> <li>siege</li> <li>HailStorm</li> <li>kraken by OctoPerf</li> <li>kraken - Javascript benchmarking utility</li> </ul>"},{"location":"why-nots/extras/","title":"Extras","text":""},{"location":"why-nots/extras/#topics","title":"Topics","text":"<ul> <li>airbyte<ul> <li>airbyte incremental cdc</li> </ul> </li> <li>snowplow</li> <li></li> </ul>"},{"location":"why-nots/online-code-editors/","title":"Online Code Editors","text":""},{"location":"why-nots/online-code-editors/#references","title":"References","text":"<ul> <li>Repl.it</li> <li>Repl.it comparison with competitors</li> </ul>"},{"location":"why-nots/project-documentation/","title":"Project Documentation","text":""},{"location":"why-nots/project-documentation/#documentation-frameworks","title":"Documentation Frameworks","text":"<ul> <li>Docusaurus</li> </ul>"},{"location":"why-nots/project-documentation/#tools","title":"Tools","text":"<ul> <li>Shields - for project status badges</li> </ul>"},{"location":"why-nots/project-documentation/#references","title":"References","text":"<ul> <li>12 ways to get more GitHub stars for your open-source project</li> </ul>"},{"location":"why-nots/static-site-generator/","title":"Static Site Generator","text":""},{"location":"why-nots/static-site-generator/#deployment","title":"Deployment","text":"<ul> <li>Netlify</li> <li>Vercel</li> <li>Cloudflare</li> <li>fly.io</li> </ul>"},{"location":"why-nots/static-site-generator/#top-comments-from-blogs","title":"Top comments from blogs","text":""},{"location":"why-nots/static-site-generator/#cloudflare","title":"Cloudflare","text":"<p>Cloudflare is a lot more generous with bandwidth. It's basically unmetered. Cloudflare Pages also provide preview deployments and you can handle server-side logic and SSR with Cloudflare Workers.</p> <p>Cloudflare provides most what Vercel does through Pages and Workers, and additionally they also provide storage (KV database, object storage and blob storage), email routing and other services \u2013 as long as you're ok to be locked in with their services.</p> <p>If you are looking for a platform that offers unlimited builds and deploys of static websites and serverless functions, Cloudflare Pages may be the better option</p>"},{"location":"why-nots/static-site-generator/#vercel","title":"Vercel","text":"<p>Vercel's free plan is limited to 100gb or so, and gets really expensive if you are using more than 1tb a month. Bandwidth is basically data transfer. vercel (and most other web hosts) will meter how much data your site is using and charge you for it</p> <p>Vercel does have the best support nextjs's SSR features, compared to cloudflare which only supports ssr on nextjs version 12.</p> <p>Vercel has a convenient feature where it will auto-deploy your feature branches and post a preview link on github, so it's great if you are working on a team and you want to QA other peoples work before merging.</p> <p>Vercel is a great choice for Next.js, but their server-side hosting is limited to \"serverless\" functions. They also don't provide much extra services outside of core hosting, for example you'll have to pick some other hosting for a database or files storage.</p> <p>Vercel isn\u2019t allowing commercial use while Cloudflare does, with the free plan.</p>"},{"location":"why-nots/static-site-generator/#flyio","title":"Fly.io","text":"<p>Fly is the closest one to Heroku, it lets you run a \u201ctraditional\u201d stateful server (e.g. Express) and a database in a free tier, so it's nice choice if you want to use for example MERN stack.</p>"},{"location":"why-nots/static-site-generator/#references","title":"References","text":"<ul> <li>Generators<ul> <li>docsify</li> </ul> </li> <li>Deployment services<ul> <li>Cloudflare vs Vercel vs FLY io, where to deploy? | Stackoverflow</li> </ul> </li> </ul>"},{"location":"why-nots/course-outlines/docker-advanced/","title":"Docker - Advanced course","text":"<p>An advanced Docker course should delve into more complex concepts, advanced usage scenarios, and best practices. Here's a suggested layout for an advanced Docker course:</p>"},{"location":"why-nots/course-outlines/docker-advanced/#module-1-advanced-docker-image-techniques","title":"Module 1: Advanced Docker Image Techniques","text":"<ol> <li>Multistage Builds</li> <li>Optimizing Docker images with multistage builds</li> <li> <p>Reducing image size and improving performance</p> </li> <li> <p>Building Images with BuildKit</p> </li> <li>Introduction to BuildKit</li> <li>Advanced features for efficient image building</li> </ol>"},{"location":"why-nots/course-outlines/docker-advanced/#module-2-docker-compose-for-production","title":"Module 2: Docker Compose for Production","text":"<ol> <li>Advanced Docker Compose Features</li> <li>Environment variables and substitution</li> <li> <p>Extending and overriding Compose configurations</p> </li> <li> <p>Healthchecks and Dependencies</p> </li> <li>Defining healthchecks in Compose</li> <li>Handling service dependencies in production</li> </ol>"},{"location":"why-nots/course-outlines/docker-advanced/#module-3-docker-networking-and-security","title":"Module 3: Docker Networking and Security","text":"<ol> <li>Advanced Networking Concepts</li> <li>Overlay networks and service discovery</li> <li> <p>Network policies and segmentation</p> </li> <li> <p>Securing Docker Containers</p> </li> <li>Applying security best practices</li> <li>Using AppArmor and Seccomp profiles</li> </ol>"},{"location":"why-nots/course-outlines/docker-advanced/#module-4-advanced-container-orchestration-with-kubernetes","title":"Module 4: Advanced Container Orchestration with Kubernetes","text":"<ol> <li>Introduction to Kubernetes</li> <li>Docker Swarm vs. Kubernetes</li> <li> <p>Basic Kubernetes concepts</p> </li> <li> <p>Deploying Docker Containers in Kubernetes</p> </li> <li>Creating Kubernetes deployments</li> <li>Configuring services and ingress</li> </ol>"},{"location":"why-nots/course-outlines/docker-advanced/#module-5-docker-storage-management","title":"Module 5: Docker Storage Management","text":"<ol> <li>Managing Data Volumes</li> <li>Advanced volume usage and management</li> <li> <p>External storage solutions</p> </li> <li> <p>Storage Drivers and Backends</p> <ul> <li>Understanding different storage drivers</li> <li>Configuring storage backends</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-advanced/#module-6-docker-for-microservices","title":"Module 6: Docker for Microservices","text":"<ol> <li>Containerizing Microservices<ul> <li>Strategies for microservices architecture</li> <li>Building and deploying microservices with Docker</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-advanced/#module-7-docker-enterprise-features","title":"Module 7: Docker Enterprise Features","text":"<ol> <li> <p>Introduction to Docker Enterprise</p> <ul> <li>Features and benefits</li> <li>Docker Universal Control Plane (UCP) and Docker Trusted Registry (DTR)</li> </ul> </li> <li> <p>Docker Enterprise Security</p> <ul> <li>Role-based access control (RBAC)</li> <li>Image signing and content trust</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-advanced/#module-8-continuous-integration-and-deployment-cicd-with-docker","title":"Module 8: Continuous Integration and Deployment (CI/CD) with Docker","text":"<ol> <li>Advanced CI/CD Pipelines<ul> <li>Integrating Docker into advanced CI/CD workflows</li> <li>Automated testing and deployment strategies</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-advanced/#module-9-advanced-troubleshooting-and-performance-tuning","title":"Module 9: Advanced Troubleshooting and Performance Tuning","text":"<ol> <li> <p>Advanced Debugging Techniques</p> <ul> <li>Troubleshooting network issues</li> <li>Debugging containerized applications</li> </ul> </li> <li> <p>Performance Tuning</p> <ul> <li>Profiling and optimizing containerized applications</li> <li>Resource constraints and tuning</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-advanced/#module-10-docker-monitoring-and-logging","title":"Module 10: Docker Monitoring and Logging","text":"<ol> <li> <p>Monitoring Docker Containers</p> <ul> <li>Advanced monitoring tools and techniques</li> <li>Container orchestration and monitoring</li> </ul> </li> <li> <p>Centralized Logging Solutions</p> <ul> <li>Configuring centralized logging</li> <li>Analyzing logs for troubleshooting and insights</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-advanced/#module-11-docker-security-best-practices","title":"Module 11: Docker Security Best Practices","text":"<ol> <li> <p>Container Security Best Practices</p> <ul> <li>Advanced security configurations</li> <li>Implementing least privilege principles</li> </ul> </li> <li> <p>Securing the Docker Daemon</p> <ul> <li>Advanced daemon security configurations</li> <li>Enhancing authentication and authorization</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-advanced/#module-12-docker-tips-and-advanced-best-practices","title":"Module 12: Docker Tips and Advanced Best Practices","text":"<ol> <li> <p>Efficiency Tips and Tricks</p> <ul> <li>Optimizing Dockerfiles and images</li> <li>Advanced use of Docker CLI</li> </ul> </li> <li> <p>Advanced Networking Scenarios</p> <ul> <li>Overlay networks and advanced networking scenarios</li> <li>Custom networking solutions</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-advanced/#module-13-advanced-use-cases-and-future-trends","title":"Module 13: Advanced Use Cases and Future Trends","text":"<ol> <li> <p>Edge Computing with Docker</p> <ul> <li>Deploying containers to edge devices</li> <li>Challenges and considerations for edge computing</li> </ul> </li> <li> <p>Future Trends in Docker and Containerization</p> <ul> <li>Exploring emerging technologies and trends</li> <li>The future landscape of container orchestration</li> </ul> </li> </ol> <p>This layout provides a structured progression for an advanced Docker course, covering a wide range of topics that go beyond the basics. Practical examples and real-world scenarios should be integrated to enhance the learning experience.</p>"},{"location":"why-nots/course-outlines/docker-all/","title":"Docker course","text":"<p>Designing a Docker course involves covering a range of topics to ensure participants have a comprehensive understanding of containerization and Docker's functionalities. Here's a suggested layout for a Docker course:</p>"},{"location":"why-nots/course-outlines/docker-all/#module-1-introduction-to-docker","title":"Module 1: Introduction to Docker","text":"<ol> <li>Understanding Containerization</li> <li>Definition and benefits</li> <li> <p>Comparison with virtualization</p> </li> <li> <p>Introduction to Docker</p> </li> <li>What is Docker?</li> <li> <p>Brief history and purpose</p> </li> <li> <p>Docker Architecture</p> </li> <li>Docker Engine components</li> <li>Client-server model</li> </ol>"},{"location":"why-nots/course-outlines/docker-all/#module-2-installing-and-configuring-docker","title":"Module 2: Installing and Configuring Docker","text":"<ol> <li>Installing Docker</li> <li>Different installation methods (Windows, macOS, Linux)</li> <li> <p>Verifying the installation</p> </li> <li> <p>Configuring Docker</p> </li> <li>Docker daemon options</li> <li>Docker Compose setup</li> </ol>"},{"location":"why-nots/course-outlines/docker-all/#module-3-docker-images-and-containers","title":"Module 3: Docker Images and Containers","text":"<ol> <li>Docker Images</li> <li>What are Docker images?</li> <li> <p>Pulling and pushing images from/to Docker Hub</p> </li> <li> <p>Docker Containers</p> </li> <li>Running containers</li> <li>Container lifecycle</li> </ol>"},{"location":"why-nots/course-outlines/docker-all/#module-4-working-with-containers","title":"Module 4: Working with Containers","text":"<ol> <li>Container Management Commands</li> <li>Starting, stopping, and restarting containers</li> <li> <p>Listing and removing containers</p> </li> <li> <p>Container Networking</p> </li> <li>Understanding container networking</li> <li> <p>Exposing ports and connecting containers</p> </li> <li> <p>Volume Mounting</p> <ul> <li>Persisting data with volumes</li> <li>Sharing data between host and container</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-all/#module-5-dockerfile-basics","title":"Module 5: Dockerfile Basics","text":"<ol> <li> <p>Introduction to Dockerfile</p> <ul> <li>Building images with Dockerfile</li> <li>Basic Dockerfile instructions</li> </ul> </li> <li> <p>Dockerfile Best Practices</p> <ul> <li>Efficient layering</li> <li>Reducing image size</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-all/#module-6-docker-compose","title":"Module 6: Docker Compose","text":"<ol> <li> <p>What is Docker Compose?</p> <ul> <li>Defining multi-container applications</li> <li>Compose file structure</li> </ul> </li> <li> <p>Orchestrating Multi-Container Applications</p> <ul> <li>Defining services</li> <li>Networking with Compose</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-all/#module-7-docker-registry-and-repository","title":"Module 7: Docker Registry and Repository","text":"<ol> <li>Private Docker Registries<ul> <li>Setting up a private registry</li> <li>Pushing and pulling images from a private registry</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-all/#module-8-docker-swarm","title":"Module 8: Docker Swarm","text":"<ol> <li> <p>Introduction to Docker Swarm</p> <ul> <li>Swarm vs. Compose</li> <li>Creating a Swarm cluster</li> </ul> </li> <li> <p>Swarm Services</p> <ul> <li>Deploying and managing services</li> <li>Scaling services in a Swarm</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-all/#module-9-docker-security","title":"Module 9: Docker Security","text":"<ol> <li> <p>Container Security Best Practices</p> <ul> <li>Isolation and namespace separation</li> <li>Scanning images for vulnerabilities</li> </ul> </li> <li> <p>Securing the Docker Daemon</p> <ul> <li>TLS configuration</li> <li>User authentication and authorization</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-all/#module-10-docker-orchestration-with-kubernetes","title":"Module 10: Docker Orchestration with Kubernetes","text":"<ol> <li> <p>Introduction to Kubernetes</p> <ul> <li>Kubernetes vs. Docker Swarm</li> <li>Basic Kubernetes concepts</li> </ul> </li> <li> <p>Kubernetes Deployments</p> <ul> <li>Deploying applications with Kubernetes</li> <li>Managing pods and services</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-all/#module-11-docker-monitoring-and-logging","title":"Module 11: Docker Monitoring and Logging","text":"<ol> <li> <p>Monitoring Docker Containers</p> <ul> <li>Docker Stats</li> <li>Third-party monitoring tools</li> </ul> </li> <li> <p>Logging Best Practices</p> <ul> <li>Configuring container logs</li> <li>Centralized logging solutions</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-all/#module-12-continuous-integration-and-deployment-with-docker","title":"Module 12: Continuous Integration and Deployment with Docker","text":"<ol> <li> <p>CI/CD Pipelines with Docker</p> <ul> <li>Integrating Docker into CI/CD workflows</li> <li>Automated image builds</li> </ul> </li> <li> <p>Container Orchestration Platforms</p> <ul> <li>Integration with Jenkins, GitLab CI, etc.</li> <li>Deploying applications to Docker Swarm or Kubernetes</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-all/#module-13-docker-tips-and-best-practices","title":"Module 13: Docker Tips and Best Practices","text":"<ol> <li> <p>Efficiency Tips</p> <ul> <li>Optimizing Dockerfiles</li> <li>Image and container cleanup strategies</li> </ul> </li> <li> <p>Troubleshooting Docker Containers</p> <ul> <li>Debugging techniques</li> <li>Common issues and solutions</li> </ul> </li> </ol> <p>This layout provides a structured progression for a Docker course, covering both fundamental concepts and advanced topics. Depending on the audience's skill level and specific needs, you may adjust the depth and emphasis on certain sections. Additionally, practical hands-on exercises and projects should be integrated throughout the course to reinforce learning.</p>"},{"location":"why-nots/course-outlines/docker-beginner/","title":"Docker - Beginner course","text":"<p>When designing a Docker course for beginners, it's essential to introduce concepts gradually and provide hands-on exercises to reinforce learning. Here's a suggested layout for a Docker beginner course:</p>"},{"location":"why-nots/course-outlines/docker-beginner/#module-1-introduction-to-containerization","title":"Module 1: Introduction to Containerization","text":"<ol> <li>Understanding Containerization</li> <li>Definition and benefits</li> <li> <p>Use cases for containerization</p> </li> <li> <p>Comparison with Virtualization</p> </li> <li>Differences between containers and virtual machines</li> <li>Advantages of containers</li> </ol>"},{"location":"why-nots/course-outlines/docker-beginner/#module-2-getting-started-with-docker","title":"Module 2: Getting Started with Docker","text":"<ol> <li>Introduction to Docker</li> <li>What is Docker?</li> <li> <p>Brief history and purpose</p> </li> <li> <p>Installing Docker</p> </li> <li>Different installation methods (Windows, macOS, Linux)</li> <li>Verifying the installation</li> </ol>"},{"location":"why-nots/course-outlines/docker-beginner/#module-3-docker-basics","title":"Module 3: Docker Basics","text":"<ol> <li>Docker Images and Containers</li> <li>What are Docker images?</li> <li> <p>Running containers from images</p> </li> <li> <p>Basic Docker Commands</p> </li> <li><code>docker run</code>, <code>docker ps</code>, <code>docker exec</code>, etc.</li> <li> <p>Managing containers with basic commands</p> </li> <li> <p>Docker Hub</p> </li> <li>Introduction to Docker Hub</li> <li>Pulling images from Docker Hub</li> </ol>"},{"location":"why-nots/course-outlines/docker-beginner/#module-4-container-lifecycle","title":"Module 4: Container Lifecycle","text":"<ol> <li>Starting and Stopping Containers</li> <li>Understanding container lifecycle</li> <li> <p>Starting, stopping, and restarting containers</p> </li> <li> <p>Removing Containers</p> </li> <li>Deleting stopped containers</li> <li>Managing container resources</li> </ol>"},{"location":"why-nots/course-outlines/docker-beginner/#module-5-networking-in-docker","title":"Module 5: Networking in Docker","text":"<ol> <li> <p>Container Networking Basics</p> <ul> <li>Exposing ports</li> <li>Linking containers</li> </ul> </li> <li> <p>Docker Networks</p> <ul> <li>Overview of Docker network types</li> <li>Creating and managing custom networks</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-beginner/#module-6-persistent-data-with-volumes","title":"Module 6: Persistent Data with Volumes","text":"<ol> <li>Volume Mounting<ul> <li>Persisting data with volumes</li> <li>Sharing data between host and container</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-beginner/#module-7-introduction-to-dockerfile","title":"Module 7: Introduction to Dockerfile","text":"<ol> <li> <p>Creating Docker Images with Dockerfile</p> <ul> <li>Basics of Dockerfile syntax</li> <li>Building images with Dockerfile</li> </ul> </li> <li> <p>Dockerfile Best Practices</p> <ul> <li>Efficient layering</li> <li>Reducing image size</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-beginner/#module-8-docker-compose-basics","title":"Module 8: Docker Compose Basics","text":"<ol> <li> <p>Introduction to Docker Compose</p> <ul> <li>Defining multi-container applications</li> <li>Compose file structure</li> </ul> </li> <li> <p>Orchestrating Multi-Container Applications</p> <ul> <li>Defining services</li> <li>Networking with Compose</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-beginner/#module-9-docker-registry","title":"Module 9: Docker Registry","text":"<ol> <li>Docker Registry Basics<ul> <li>Pushing and pulling images from Docker Hub</li> <li>Overview of private registries</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-beginner/#module-10-basic-troubleshooting","title":"Module 10: Basic Troubleshooting","text":"<ol> <li> <p>Debugging Docker Containers</p> <ul> <li>Inspecting container logs</li> <li>Troubleshooting common issues</li> </ul> </li> <li> <p>Cleaning Up Resources</p> <ul> <li>Removing unused containers, images, and volumes</li> <li>Optimizing disk space</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-beginner/#module-11-practical-exercises-and-projects","title":"Module 11: Practical Exercises and Projects","text":"<ol> <li>Hands-On Exercises<ul> <li>Guided exercises to practice commands and workflows</li> <li>Mini-projects to apply learned concepts</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/docker-beginner/#module-12-next-steps-and-resources","title":"Module 12: Next Steps and Resources","text":"<ol> <li>Further Learning Paths<ul> <li>Recommendations for additional learning</li> <li>Resources for ongoing development</li> </ul> </li> </ol> <p>This layout provides a structured progression for beginners, starting with the basics and gradually moving towards more advanced topics. Including practical exercises and projects throughout the course helps solidify understanding and build confidence in using Docker.</p>"},{"location":"why-nots/course-outlines/git-advanced/","title":"Git - Advanced course","text":"<p>An advanced Git course should delve into more complex concepts, workflows, and advanced features of Git. Here's a suggested layout for an advanced Git course:</p>"},{"location":"why-nots/course-outlines/git-advanced/#module-1-advanced-git-concepts","title":"Module 1: Advanced Git Concepts","text":"<ol> <li>Git Internals</li> <li>Object model and storage</li> <li> <p>Understanding the Git database</p> </li> <li> <p>Reflog and Reset</p> </li> <li>Exploring the reflog</li> <li>Advanced usage of <code>git reset</code></li> </ol>"},{"location":"why-nots/course-outlines/git-advanced/#module-2-advanced-branching-and-merging","title":"Module 2: Advanced Branching and Merging","text":"<ol> <li>Branching Strategies</li> <li>Gitflow workflow</li> <li> <p>GitHub flow and GitLab flow</p> </li> <li> <p>Merge Strategies</p> </li> <li>Recursive, octopus, and resolve merges</li> <li>Strategies for handling complex merges</li> </ol>"},{"location":"why-nots/course-outlines/git-advanced/#module-3-rebasing","title":"Module 3: Rebasing","text":"<ol> <li>Understanding Rebasing</li> <li>Basic rebase vs. merge</li> <li> <p>Interactive rebase for rewriting history</p> </li> <li> <p>Rebase Workflows</p> </li> <li>Rebasing branches for a cleaner history</li> <li>Handling conflicts during rebase</li> </ol>"},{"location":"why-nots/course-outlines/git-advanced/#module-4-submodules-and-subtrees","title":"Module 4: Submodules and Subtrees","text":"<ol> <li>Working with Submodules</li> <li>Adding and updating submodules</li> <li> <p>Submodule workflows</p> </li> <li> <p>Git Subtree</p> </li> <li>Incorporating external repositories</li> <li>Managing subtrees in Git</li> </ol>"},{"location":"why-nots/course-outlines/git-advanced/#module-5-git-worktrees","title":"Module 5: Git Worktrees","text":"<ol> <li>Introduction to Git Worktrees</li> <li>Working with multiple branches simultaneously</li> <li>Use cases for Git worktrees</li> </ol>"},{"location":"why-nots/course-outlines/git-advanced/#module-6-git-hooks","title":"Module 6: Git Hooks","text":"<ol> <li> <p>Understanding Git Hooks</p> <ul> <li>Client-side and server-side hooks</li> <li>Customizing Git behavior with hooks</li> </ul> </li> <li> <p>Creating Custom Hooks</p> <ul> <li>Writing scripts for pre-commit, post-commit, etc.</li> <li>Examples of practical hook usage</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-advanced/#module-7-advanced-remote-repository-management","title":"Module 7: Advanced Remote Repository Management","text":"<ol> <li> <p>Advanced Remote Operations</p> <ul> <li>Fetching and pulling specific branches</li> <li>Pushing to non-default branches</li> </ul> </li> <li> <p>Shallow Cloning and Partial Cloning</p> <ul> <li>Reducing repository size with shallow clones</li> <li>Cloning only part of a repository</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-advanced/#module-8-git-workflows-and-strategies","title":"Module 8: Git Workflows and Strategies","text":"<ol> <li> <p>Release Management</p> <ul> <li>Tagging and versioning strategies</li> <li>Preparing and managing releases</li> </ul> </li> <li> <p>Cherry-Picking Commits</p> <ul> <li>Selectively applying changes</li> <li>Best practices for cherry-picking</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-advanced/#module-9-git-and-continuous-integration","title":"Module 9: Git and Continuous Integration","text":"<ol> <li>Integration with CI/CD<ul> <li>Automating builds and tests with Git</li> <li>Git in a CI/CD pipeline</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-advanced/#module-10-git-internals-and-performance-optimization","title":"Module 10: Git Internals and Performance Optimization","text":"<ol> <li> <p>Optimizing Git Performance</p> <ul> <li>Tips for faster operations</li> <li>Caching and optimization techniques</li> </ul> </li> <li> <p>Object Storage and Packfiles</p> <ul> <li>Understanding Git object storage</li> <li>Impact on performance and optimization</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-advanced/#module-11-git-security","title":"Module 11: Git Security","text":"<ol> <li> <p>GPG Signing</p> <ul> <li>Signing commits and tags</li> <li>Verifying signed commits</li> </ul> </li> <li> <p>Access Controls and Authentication</p> <ul> <li>Configuring secure access to repositories</li> <li>Multi-factor authentication</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-advanced/#module-12-advanced-troubleshooting","title":"Module 12: Advanced Troubleshooting","text":"<ol> <li> <p>Advanced Conflict Resolution</p> <ul> <li>Strategies for handling complex conflicts</li> <li>Resolving conflicts during rebase</li> </ul> </li> <li> <p>Git Bisect</p> <ul> <li>Finding the commit that introduced a bug</li> <li>Automated bisection with Git</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-advanced/#module-13-git-for-large-projects-and-teams","title":"Module 13: Git for Large Projects and Teams","text":"<ol> <li> <p>Scaling Git for Large Projects</p> <ul> <li>Performance considerations</li> <li>Strategies for handling large repositories</li> </ul> </li> <li> <p>Git in Enterprise Environments</p> <ul> <li>Git in large teams and organizations</li> <li>Managing access controls and permissions</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-advanced/#module-14-git-tips-and-tricks","title":"Module 14: Git Tips and Tricks","text":"<ol> <li> <p>Efficiency Tips</p> <ul> <li>Keyboard shortcuts and time-saving commands</li> <li>Git aliases and custom commands</li> </ul> </li> <li> <p>Advanced Log and Blame</p> <ul> <li>Customizing Git log output</li> <li>Annotating files with <code>git blame</code></li> </ul> </li> </ol> <p>This layout provides a structured progression for an advanced Git course, covering a wide range of topics that go beyond the basics. It assumes participants have a solid understanding of fundamental Git concepts and are ready to explore advanced features and workflows. Practical examples and real-world scenarios should be integrated to enhance the learning experience.</p>"},{"location":"why-nots/course-outlines/git-all/","title":"Git course","text":"<p>Designing a comprehensive Git course involves covering a range of topics to ensure that participants acquire a solid understanding of version control and Git's functionalities. Here's a suggested layout for a Git course:</p>"},{"location":"why-nots/course-outlines/git-all/#module-1-introduction-to-version-control","title":"Module 1: Introduction to Version Control","text":"<ol> <li>Understanding Version Control</li> <li>Definition and purpose</li> <li> <p>Benefits of version control</p> </li> <li> <p>Types of Version Control Systems</p> </li> <li>Centralized vs. Distributed</li> <li>Git vs. other systems (e.g., SVN, Mercurial)</li> </ol>"},{"location":"why-nots/course-outlines/git-all/#module-2-getting-started-with-git","title":"Module 2: Getting Started with Git","text":"<ol> <li>Installing Git</li> <li> <p>Different installation methods (Windows, macOS, Linux)</p> </li> <li> <p>Configuring Git</p> </li> <li>Setting up user information</li> <li> <p>Configuration settings</p> </li> <li> <p>Creating a Repository</p> </li> <li>Initializing a new repository</li> <li>Cloning an existing repository</li> </ol>"},{"location":"why-nots/course-outlines/git-all/#module-3-basic-git-concepts","title":"Module 3: Basic Git Concepts","text":"<ol> <li>Understanding Commits</li> <li>Staging and committing changes</li> <li> <p>Commit messages best practices</p> </li> <li> <p>Branching and Merging</p> </li> <li>Creating branches</li> <li>Switching between branches</li> <li> <p>Merging branches</p> </li> <li> <p>Resolving Conflicts</p> </li> <li>Identifying and handling conflicts</li> <li>Conflict resolution strategies</li> </ol>"},{"location":"why-nots/course-outlines/git-all/#module-4-remote-repositories","title":"Module 4: Remote Repositories","text":"<ol> <li> <p>Working with Remote Repositories</p> <ul> <li>Adding remotes</li> <li>Fetching and pulling changes</li> <li>Pushing changes to remote</li> </ul> </li> <li> <p>Collaboration in Git</p> <ul> <li>Forking repositories</li> <li>Pull requests and code review</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-all/#module-5-advanced-git-topics","title":"Module 5: Advanced Git Topics","text":"<ol> <li> <p>Tagging and Releases</p> <ul> <li>Creating and managing tags</li> <li>Preparing and publishing releases</li> </ul> </li> <li> <p>Git Hooks</p> <ul> <li>Introduction to hooks</li> <li>Examples of common hooks</li> </ul> </li> <li> <p>Git Workflows</p> <ul> <li>Understanding different workflows (e.g., Gitflow, GitHub flow)</li> <li>Choosing the right workflow for your project</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-all/#module-6-git-best-practices-and-tips","title":"Module 6: Git Best Practices and Tips","text":"<ol> <li> <p>Best Practices for Git Usage</p> <ul> <li>Committing best practices</li> <li>Branching strategies</li> </ul> </li> <li> <p>Performance Optimization</p> <ul> <li>Tips for faster operations</li> <li>Managing large repositories</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-all/#module-7-git-tools-and-integration","title":"Module 7: Git Tools and Integration","text":"<ol> <li> <p>Graphical Git Clients</p> <ul> <li>Overview of popular GUIs (e.g., GitKraken, Sourcetree)</li> </ul> </li> <li> <p>IDE Integration</p> <ul> <li>Integrating Git with IDEs (e.g., Visual Studio Code, IntelliJ)</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-all/#module-8-git-security","title":"Module 8: Git Security","text":"<ol> <li> <p>Securing Repositories</p> <ul> <li>Access control and permissions</li> <li>Authentication methods</li> </ul> </li> <li> <p>Handling Sensitive Information</p> <ul> <li>Gitignore</li> <li>Git-crypt and other encryption tools</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-all/#module-9-troubleshooting-and-debugging","title":"Module 9: Troubleshooting and Debugging","text":"<ol> <li> <p>Common Git Issues</p> <ul> <li>Diagnosing and fixing common problems</li> <li>Git log and other debugging tools</li> </ul> </li> <li> <p>Git Maintenance</p> <ul> <li>Housekeeping tasks</li> <li>Garbage collection and repository optimization</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-all/#module-10-git-and-continuous-integration","title":"Module 10: Git and Continuous Integration","text":"<ol> <li> <p>Integration with CI/CD</p> <ul> <li>Using Git with Jenkins, GitLab CI, GitHub Actions, etc.</li> </ul> </li> <li> <p>Automated Testing</p> <ul> <li>Leveraging version control for testing workflows</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-all/#module-11-git-for-specific-scenarios","title":"Module 11: Git for Specific Scenarios","text":"<ol> <li> <p>Git for Open Source Projects</p> <ul> <li>Best practices for open-source collaboration</li> </ul> </li> <li> <p>Git in Enterprise Environments</p> <ul> <li>Scaling Git for large projects</li> <li>Git in enterprise workflows</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-all/#module-12-future-trends-in-git","title":"Module 12: Future Trends in Git","text":"<ol> <li>Git Trends and Innovations<ul> <li>Current and emerging trends in version control</li> <li>The future of Git</li> </ul> </li> </ol> <p>This layout provides a comprehensive structure for a Git course, covering both fundamental concepts and advanced topics. Depending on the audience's skill level and specific needs, you may adjust the depth and emphasis on certain sections. Additionally, practical hands-on exercises and projects should be integrated throughout the course to reinforce learning.</p>"},{"location":"why-nots/course-outlines/git-beginner/","title":"Git - Beginner course","text":"<p>When designing a Git course for beginners, it's crucial to introduce concepts gradually and provide hands-on exercises to reinforce learning. Here's a suggested layout for a Git beginner course:</p>"},{"location":"why-nots/course-outlines/git-beginner/#module-1-introduction-to-version-control-and-git","title":"Module 1: Introduction to Version Control and Git","text":"<ol> <li>Understanding Version Control</li> <li>Definition and benefits</li> <li> <p>Importance in collaborative development</p> </li> <li> <p>Introduction to Git</p> </li> <li>What is Git?</li> <li>Brief history and purpose</li> </ol>"},{"location":"why-nots/course-outlines/git-beginner/#module-2-setting-up-git","title":"Module 2: Setting Up Git","text":"<ol> <li>Installing Git</li> <li>Download and installation process</li> <li> <p>Configuring user information</p> </li> <li> <p>Creating Your First Repository</p> </li> <li>Initializing a new repository</li> <li>Adding and committing files</li> </ol>"},{"location":"why-nots/course-outlines/git-beginner/#module-3-basic-git-commands","title":"Module 3: Basic Git Commands","text":"<ol> <li>Git Workflow</li> <li>Working directory, staging area, and repository</li> <li> <p>Basic lifecycle of a file</p> </li> <li> <p>Committing Changes</p> </li> <li>Staging changes</li> <li> <p>Making commits</p> </li> <li> <p>Viewing Changes</p> </li> <li>Checking the status of your repository</li> <li>Viewing commit history</li> </ol>"},{"location":"why-nots/course-outlines/git-beginner/#module-4-branching-and-merging","title":"Module 4: Branching and Merging","text":"<ol> <li>Understanding Branches</li> <li>Creating branches</li> <li> <p>Switching between branches</p> </li> <li> <p>Merging Changes</p> </li> <li>Integrating changes from one branch into another</li> <li>Handling merge conflicts</li> </ol>"},{"location":"why-nots/course-outlines/git-beginner/#module-5-remote-repositories","title":"Module 5: Remote Repositories","text":"<ol> <li> <p>Introduction to Remote Repositories</p> <ul> <li>Connecting to remote repositories</li> <li>Cloning repositories</li> </ul> </li> <li> <p>Pushing and Pulling Changes</p> <ul> <li>Sending changes to a remote repository</li> <li>Updating your local repository with remote changes</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-beginner/#module-6-collaboration-basics","title":"Module 6: Collaboration Basics","text":"<ol> <li> <p>Forking and Cloning</p> <ul> <li>Forking a repository</li> <li>Cloning a forked repository</li> </ul> </li> <li> <p>Making Pull Requests</p> <ul> <li>Creating and submitting pull requests</li> <li>Code review basics</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-beginner/#module-7-undoing-changes","title":"Module 7: Undoing Changes","text":"<ol> <li>Undoing Changes in Git<ul> <li>Discarding changes in the working directory</li> <li>Reverting commits</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-beginner/#module-8-git-best-practices-for-beginners","title":"Module 8: Git Best Practices for Beginners","text":"<ol> <li> <p>Committing Best Practices</p> <ul> <li>Writing meaningful commit messages</li> <li>Committing small and focused changes</li> </ul> </li> <li> <p>Branching Best Practices</p> <ul> <li>Naming conventions for branches</li> <li>Keeping branches clean</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-beginner/#module-9-git-tools-for-beginners","title":"Module 9: Git Tools for Beginners","text":"<ol> <li> <p>Basic Git GUIs</p> <ul> <li>Introduction to graphical interfaces (e.g., GitKraken, Sourcetree)</li> </ul> </li> <li> <p>Integrated Development Environment (IDE) Integration</p> <ul> <li>Using Git within popular IDEs (e.g., Visual Studio Code, GitHub Desktop)</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-beginner/#module-10-troubleshooting-basics","title":"Module 10: Troubleshooting Basics","text":"<ol> <li>Common Git Issues and Solutions<ul> <li>Handling merge conflicts</li> <li>Recovering from mistakes</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-beginner/#module-11-practical-exercises-and-projects","title":"Module 11: Practical Exercises and Projects","text":"<ol> <li>Hands-On Exercises<ul> <li>Guided exercises to practice commands and workflows</li> <li>Mini-projects to apply learned concepts</li> </ul> </li> </ol>"},{"location":"why-nots/course-outlines/git-beginner/#module-12-next-steps-and-resources","title":"Module 12: Next Steps and Resources","text":"<ol> <li>Further Learning Paths<ul> <li>Recommendations for additional learning</li> <li>Resources for ongoing development</li> </ul> </li> </ol> <p>This layout provides a structured progression for beginners, starting with the basics and gradually moving towards more advanced topics. Including practical exercises and projects throughout the course helps solidify understanding and build confidence in using Git.</p>"}]}